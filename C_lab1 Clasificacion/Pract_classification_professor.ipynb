{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Classification**\n",
    "\n",
    "    Notebook version: 1.1 (Oct 25, 2017)\n",
    "\n",
    "    Authors: Jes√∫s Cid Sueiro (jcid@tsc.uc3m.es)\n",
    "\n",
    "    Changes: v.1.0 - First version. Python version\n",
    "             v.1.1 - Updated to sklearn.model_selection. Python 3 compatibility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import some libraries that will be necessary for working with data and displaying plots\n",
    "\n",
    "# To visualize plots in the notebook\n",
    "%matplotlib inline\n",
    "\n",
    "#import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import scipy.io       # To read matlab files\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn import svm\n",
    "\n",
    "from sklearn import model_selection\n",
    "\n",
    "import pylab\n",
    "pylab.rcParams['figure.figsize'] = 9, 7"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Introduction\n",
    "\n",
    "In this notebook we will analyze the behavior of logistic regression and support vector machines on the dataset in file `Dataset2D.mat`. We first load the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "matvar = scipy.io.loadmat('Dataset2D.mat')\n",
    "Xtrain = matvar['xTrain']\n",
    "Xtest = matvar['xTest']\n",
    "Xval = matvar['xVal']\n",
    "\n",
    "# We must use astype(int) to convert the original target values (which are unsigned integers) to int.\n",
    "Ytrain = matvar['yTrain'].astype(int)\n",
    "Ytest = matvar['yTest'].astype(int)\n",
    "Yval = matvar['yVal'].astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Data Preparation.\n",
    "\n",
    "Normalize the dataset. Remind that the same transformation must be applied to training, validation and test data. Store train, validation and test input data in variables `Xtrain`, `Xval` and `Xtest`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# <SOL>\n",
    "# Data normalization\n",
    "def normalize(X, mx=None, sx=None):\n",
    "    \n",
    "    # Compute means and standard deviations\n",
    "    if mx is None:\n",
    "        mx = np.mean(X, axis=0)\n",
    "    if sx is None:\n",
    "        sx = np.std(X, axis=0)\n",
    "\n",
    "    # Normalize\n",
    "    X0 = (X-mx)/sx\n",
    "\n",
    "    return X0, mx, sx\n",
    "\n",
    "# Normalize data\n",
    "Xtrain, mx, sx = normalize(Xtrain)\n",
    "Xval, mx, sx = normalize(Xval, mx, sx)\n",
    "Xtest, mx, sx = normalize(Xtest, mx, sx)\n",
    "\n",
    "n_tr = Xtrain.shape[0]\n",
    "n_val = Xval.shape[0]\n",
    "n_tst = Xtest.shape[0]\n",
    "\n",
    "print('The number of training samples is ' + str(n_tr))\n",
    "print('The number of validation samples is ' + str(n_val))\n",
    "print('The number of test samples is ' + str(n_tst))\n",
    "print('The data dimension is ' + str(Xtrain.shape[1]))\n",
    "# </SOL>\n",
    "\n",
    "# Check normalization\n",
    "print(np.mean(Xtrain, axis=0))\n",
    "print(np.mean(Xval, axis=0))\n",
    "print(np.mean(Xtest, axis=0))\n",
    "print(np.std(Xtrain, axis=0))\n",
    "print(np.std(Xval, axis=0))\n",
    "print(np.std(Xtest, axis=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualize the input variables from the training set in a 2-dimensional plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Data visualization. This works for dimension 2 only.\n",
    "if Xtrain.shape[1]==2:\n",
    "    plt.scatter(Xtrain[:, 0], Xtrain[:, 1], c=Ytrain.flatten(), s=50, cmap='copper')\n",
    "    plt.xlabel(\"$x_0$\", fontsize=14)\n",
    "    plt.ylabel(\"$x_1$\", fontsize=14)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Linear Classification with Logistic Regression.\n",
    "\n",
    "First we will analyze the behavior of logistic regression for this dataset. \n",
    "\n",
    "### 2.1. MAP estimator.\n",
    "\n",
    "Implement a function to compute the MAP estimate of the parameters of a linear logistic regression model with Gaussian prior and a given value of the inverse regularization parameter $C$. The method should return the estimated parameter and the negative log-likelihood, $\\text{NLL}({\\bf w})$. The sintaxis must be\n",
    "    **`w, NLL = logregFitR(Z_tr, Y_tr, rho, C, n_it)`**\n",
    "where\n",
    "\n",
    "  - `Z_tr` is the input training data matrix (one instance per row)\n",
    "  - `Y_tr` contains the labels of corresponding to each row in the data matrix\n",
    "  - `rho` is the learning step\n",
    "  - `C` is the inverse regularizer\n",
    "  - `n_it` is the number of iterations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# <SOL>\n",
    "# Define the logistic function\n",
    "def logistic(x):                                        \n",
    "   p = 1.0 / (1 + np.exp(-x))\n",
    "   return p\n",
    "\n",
    "# MAP trainer.\n",
    "def logregFitR(Z_tr, Y_tr, rho, C, n_it):\n",
    "\n",
    "    # Data dimension\n",
    "    n_dim = Z_tr.shape[1]\n",
    "\n",
    "    # Initialize variables\n",
    "    nll_tr = np.zeros(n_it)\n",
    "    pe_tr = np.zeros(n_it)\n",
    "    w = np.random.randn(n_dim,1)\n",
    "\n",
    "    # Running the gradient descent algorithm\n",
    "    for n in range(n_it):\n",
    "        \n",
    "        # Compute posterior probabilities for weight w\n",
    "        p1_tr = logistic(np.dot(Z_tr, w))\n",
    "        p0_tr = logistic(-np.dot(Z_tr, w))\n",
    "        \n",
    "        # Compute negative log-likelihood\n",
    "        nll_tr[n] = - np.dot(Y_tr.T, np.log(p1_tr)) - np.dot((1-Y_tr).T, np.log(p0_tr))\n",
    "\n",
    "        # Update weights\n",
    "        w = (1-2*rho/C)*w + rho*np.dot(Z_tr.T, Y_tr - p1_tr)\n",
    "\n",
    "    return w, nll_tr\n",
    "\n",
    "# Compute predictions for a given model\n",
    "def logregPredict(Z, w):\n",
    "\n",
    "    # Compute posterior probability of class 1 for weights w.\n",
    "    p = logistic(np.dot(Z, w))\n",
    "    \n",
    "    # Classify\n",
    "    D = [int(np.round(pn)) for pn in p]\n",
    "    \n",
    "    return p, D\n",
    "\n",
    "#</SOL>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2..2 Log-likelihood\n",
    "\n",
    "Compute the MAP estimate for a polynomial regression with degree 5, for $C$ ranging from -0.01 to 100. Sample $C$ uniformly in a log scale, an plot using `plt.semilogx`. \n",
    "\n",
    "Plot the final value of $\\text{NLL}$ as a function of $C$. Can you explain the qualitative behavior of $\\text{NLL}$ as $C$ grows?\n",
    "\n",
    "The plot may show some oscillation because of the random noise introduced by random initializations of the learning algoritm. In order to smooth the results, you can initialize the random seed right before calling the `logregFitR` method, using\n",
    "\n",
    "    np.random.seed(3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# <SOL>\n",
    "# Set parameters\n",
    "nC = 50\n",
    "\n",
    "logC = np.linspace(-3.5, 2, num=nC)\n",
    "C_all = np.exp(logC)\n",
    "n_it = 2000\n",
    "rho = 0.001\n",
    "\n",
    "# Compute Z_tr\n",
    "poly = PolynomialFeatures(degree=5)\n",
    "Z_tr = poly.fit_transform(Xtrain)\n",
    "\n",
    "# Normalize columns (this is useful to make algorithms more stable).)\n",
    "Zn, mz, sz = normalize(Z_tr[:,1:])\n",
    "Z_tr = np.concatenate((np.ones((n_tr, 1)), Zn), axis=1)\n",
    "\n",
    "# Compute Z_val\n",
    "Z_val = poly.fit_transform(Xval)\n",
    "Zn, mz, sz = normalize(Z_val[:,1:], mz, sz)\n",
    "Z_val = np.concatenate((np.ones((n_val,1)), Zn), axis=1)\n",
    "\n",
    "dim = Z_tr.shape[1]\n",
    "L = np.zeros((nC, 1))\n",
    "w_all = np.zeros((nC, dim))\n",
    "\n",
    "# Train models\n",
    "for k, C in enumerate(C_all):\n",
    "    \n",
    "    np.random.seed(3)\n",
    "    w, L_all = logregFitR(Z_tr, Ytrain, rho, C, n_it)\n",
    "\n",
    "    L[k] = L_all[-1]\n",
    "    w_all[k] = w.T\n",
    "\n",
    "plt.figure()\n",
    "plt.semilogx(C_all, L, '.')\n",
    "plt.xlabel('C')\n",
    "plt.ylabel('Negative log-likelihood')\n",
    "\n",
    "print(\"As C grows, the regularization effect dissapears, and the fit method minimizes NLL.\")\n",
    "# </SOL>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is a plot for the last value of C used in the code above.\n",
    "\n",
    "if Xtrain.shape[1]==2:\n",
    "\n",
    "    # Create a regtangular grid.\n",
    "    x_min, x_max = Xtrain[:, 0].min(), Xtrain[:, 0].max() \n",
    "    y_min, y_max = Xtrain[:, 1].min(), Xtrain[:, 1].max()\n",
    "    dx = x_max - x_min\n",
    "    dy = y_max - y_min\n",
    "    h = dy /400\n",
    "    xx, yy = np.meshgrid(np.arange(x_min - 0.1 * dx, x_max + 0.1 * dx, h),\n",
    "                         np.arange(y_min - 0.1 * dx, y_max + 0.1 * dy, h))\n",
    "    X_grid = np.array([xx.ravel(), yy.ravel()]).T\n",
    "\n",
    "    # Compute Z_grid\n",
    "    Z_grid = poly.fit_transform(X_grid)\n",
    "    n_grid = Z_grid.shape[0]\n",
    "    Zn, mz, sz = normalize(Z_grid[:,1:], mz, sz)\n",
    "    Z_grid = np.concatenate((np.ones((n_grid,1)), Zn), axis=1)\n",
    "\n",
    "    # Compute the classifier output for all samples in the grid.\n",
    "    pp, dd = logregPredict(Z_grid, w)\n",
    "    pp = pp.reshape(xx.shape)\n",
    "\n",
    "    # Paint output maps\n",
    "    plt.figure()\n",
    "    pylab.rcParams['figure.figsize'] = 8, 4  # Set figure size\n",
    "    for i in [1, 2]:\n",
    "        ax = plt.subplot(1,2,i)\n",
    "        ax.set_xlabel('$x_0$')\n",
    "        ax.set_ylabel('$x_1$')\n",
    "        ax.axis('equal')\n",
    "        if i==1:\n",
    "            ax.contourf(xx, yy, pp, cmap=plt.cm.copper)\n",
    "        else:\n",
    "            ax.contourf(xx, yy, np.round(pp), cmap=plt.cm.copper)\n",
    "        ax.scatter(Xtrain[:, 0], Xtrain[:, 1], c=Ytrain.flatten(), s=4, cmap='summer')\n",
    "        \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  2.3. Training and test errors.\n",
    "\n",
    "Plot the training and validation error rates as a function of $C$. Compute the value of $C$ minimizing the validation error rate.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# <SOL>\n",
    "# Train models\n",
    "pe_tr = np.zeros((nC, 1))\n",
    "pe_val = np.zeros((nC, 1))\n",
    "\n",
    "for k, C in enumerate(C_all):\n",
    "\n",
    "    p_tr, D_tr = logregPredict(Z_tr, w_all[k])\n",
    "    p_val, D_val = logregPredict(Z_val, w_all[k])\n",
    "\n",
    "    # Compute error rates\n",
    "    E_tr = D_tr!=Ytrain.T\n",
    "    E_val = D_val!=Yval.T\n",
    "\n",
    "    # Error rates\n",
    "    pe_tr[k] = np.mean(E_tr)\n",
    "    pe_val[k] = np.mean(E_val)\n",
    "\n",
    "plt.figure()\n",
    "plt.semilogx(C_all, pe_tr, '.-', label='Train')\n",
    "plt.semilogx(C_all, pe_val, '.-', label='Validation')\n",
    "plt.legend(loc='best')\n",
    "\n",
    "print(\"The optimal value of C is {0}\".format(C_all[np.argmin(pe_val)]))\n",
    "\n",
    "# </SOL>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Non-linear classification with Support Vector Machines\n",
    "\n",
    "In this section we will train an SVM with Gaussian kernels. In this case, we will select parameter $C$ of the SVM by cross-validation.\n",
    "\n",
    "### 3.1. Dataset preparation.\n",
    "\n",
    "Join the training and validation datasets in a single input matrix `X_tr2` and a single label vector `Y_tr2`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# <SOL>\n",
    "X_tr2 = np.concatenate((Xtrain, Xval), axis = 0)\n",
    "Y_tr2 = np.concatenate((Ytrain, Yval), axis = 0)\n",
    "# </SOL>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2. Cross validated error estimate\n",
    "\n",
    "Apply a 10-fold cross validation procedure to estimate the average error rate of the SVM for $C=1$ and $\\gamma$ (which is the kernel width) equal to 5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# <SOL>\n",
    "n_folds = 10\n",
    "C = 1\n",
    "gamma = 5\n",
    "n_samples = X_tr2.shape[0]\n",
    "\n",
    "kf = model_selection.KFold(n_splits=n_folds)\n",
    "pe_val = 0\n",
    "clf = svm.SVC(kernel='rbf', C=C, gamma=1)\n",
    "for tr_index, val_index in kf.split(X_tr2):\n",
    "    Xcv_tr, Xcv_val = X_tr2[tr_index], X_tr2[val_index]\n",
    "    Ycv_tr, Ycv_val = Y_tr2[tr_index], Y_tr2[val_index]\n",
    "\n",
    "    clf.fit(Xcv_tr, np.ravel(Ycv_tr))\n",
    "\n",
    "    pe_val += 1.0 - clf.score(Xval, Yval)\n",
    "    \n",
    "pe_val = pe_val/n_folds\n",
    "print(\"The average error rate is {0}\".format(pe_val))\n",
    "# </SOL>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3. Influence of $C$.\n",
    "\n",
    "Repeate exercise 3.2 for $\\gamma=5$ and different values of $C$, ranging from $10^{-3}$ to $10^{4}$, obtained by uniform sampling in a logarithmic scale. Plot the average number of errors as function of $C$.\n",
    "\n",
    "Note that fitting the SVM may take some time, specially for the largest values of $C$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# <SOL>\n",
    "C_all = np.logspace(-3, 4, 10)\n",
    "n_folds = 10\n",
    "\n",
    "n_samples = X_tr2.shape[0]\n",
    "kf = model_selection.KFold(n_splits=n_folds)\n",
    "\n",
    "gamma = 5\n",
    "pe_val = np.zeros((len(C_all),1))\n",
    "\n",
    "for k, C in enumerate(C_all):\n",
    "    print(\"C = {0}\".format(C))\n",
    "\n",
    "    clf = svm.SVC(kernel='rbf', C=C, gamma=gamma)\n",
    "\n",
    "    for tr_index, val_index in kf.split(X_tr2):\n",
    "        Xcv_tr, Xcv_val = X_tr2[tr_index], X_tr2[val_index]\n",
    "        Ycv_tr, Ycv_val = Y_tr2[tr_index], Y_tr2[val_index]\n",
    "\n",
    "        clf.fit(Xcv_tr, np.ravel(Ycv_tr))\n",
    "\n",
    "        pe_val[k] += 1.0 - clf.score(Xcv_val, Ycv_val)\n",
    "\n",
    "    pe_val[k] = pe_val[k]/n_folds\n",
    "\n",
    "# Put the result into a color plot\n",
    "plt.figure()\n",
    "plt.semilogx(C_all, pe_val,'.-')\n",
    "plt.show()\n",
    "# </SOL>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3. Hyperparameter optimization.\n",
    "\n",
    "Compute the value of $C$ minimizing the validation error rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# <SOL>\n",
    "C_opt = C_all[np.argmin(pe_val)]\n",
    "print(\"The optimal value of C in the explored range is {0}\".format(C_opt))\n",
    "# </SOL>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4. Test error\n",
    "\n",
    "Evaluate the classifier performance using the test data, for the selected hyperparameter values.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# <SOL>\n",
    "clf = svm.SVC(kernel='rbf', C=C_opt, gamma=gamma)\n",
    "clf.fit(X_tr2, np.ravel(Y_tr2))\n",
    "pe_tst = 1.0 - clf.score(Xtest, Ytest)\n",
    "print(\"The test error for the selected model is {0}\".format(pe_tst))\n",
    "# </SOL>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
