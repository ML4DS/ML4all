{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "# **The *k*-nearest neighbors (*k*NN) regression algorithm**\n",
    "\n",
    "    Author: Jerónimo Arenas García (jarenas@tsc.uc3m.es)\n",
    "            Jesús Cid Sueiro (jcid@tsc.uc3m.es)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "    Notebook version: 2.2 (Sep 08, 2017)\n",
    "\n",
    "    Changes: v.1.0 - First version\n",
    "    Changes: v.1.1 - Stock dataset included.\n",
    "    Changes: v.2.0 - Notebook for UTAD course. Advertising data incorporated\n",
    "    Changes: v.2.1 - Text and code revisited. General introduction removed.\n",
    "    Changes: v.2.2 - Compatibility with python 2 and 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Import some libraries that will be necessary for working with data and displaying plots\n",
    "\n",
    "# To visualize plots in the notebook\n",
    "%matplotlib inline \n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pylab\n",
    "\n",
    "# Packages used to read datasets\n",
    "import scipy.io       # To read matlab files\n",
    "import pandas as pd   # To read datasets in csv format\n",
    "\n",
    "# For the student tests (only for python 2)\n",
    "import sys\n",
    "if sys.version_info.major==2:\n",
    "    from test_helper import Test\n",
    "\n",
    "# That's default image size for this interactive session\n",
    "pylab.rcParams['figure.figsize'] = 9, 6  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 1. The dataset\n",
    "\n",
    "We describe next the regression task that we will use in the session. The dataset is an adaptation of the <a href=http://www.dcc.fc.up.pt/~ltorgo/Regression/DataSets.html> `STOCK` dataset</a>, taken originally from the <a href=http://lib.stat.cmu.edu/> StatLib Repository</a>. The goal of this problem is to predict the values of the stocks of a given airplane company, given the values of another 9 companies in the same day. \n",
    "\n",
    "<small> If you are reading this text from the python notebook with its full functionality, you can explore the results of the regression experiments using two alternative datasets:\n",
    "\n",
    "* The \n",
    "<a href=https://archive.ics.uci.edu/ml/datasets/Concrete+Compressive+Strength>`CONCRETE` dataset</a>, taken from the <a href=https://archive.ics.uci.edu/ml/index.html>Machine Learning Repository at the University of California Irvine</a>. The goal of the `CONCRETE` dataset tas is to predict the compressive strength of cement mixtures based on eight observed variables related to the composition of the mixture and the age of the material). \n",
    "\n",
    "* The `Advertising` dataset, taken from the book <a href= http://www-bcf.usc.edu/~gareth/ISL/data.html> An Introduction to Statistical Learning with applications in R</a>, with permission from the authors: G. James, D. Witten, T. Hastie and R. Tibshirani. The goal of this problem is to predict the sales of a given product, knowing the investment in different advertising sectors. More specifically, the input and output variables can be described as follows:\n",
    "\n",
    "  - *Input features:*\n",
    "     * TV: advertising dollars spent on TV for a single product in a given market (in thousands of dollars)\n",
    "     * Radio: advertising dollars spent on Radio\n",
    "     * Newspaper: advertising dollars spent on Newspaper\n",
    "     \n",
    "  - *Response variable:*\n",
    "     * Sales: sales of a single product in a given market (in thousands of widgets)\n",
    "\n",
    "To do so, just replace `stock` by `concrete` or `advertising` in the next cell. Remind that you must run the cells again to see the changes. \n",
    "</small>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# SELECT dataset\n",
    "# Available options are 'stock', 'concrete' or 'advertising'\n",
    "ds_name = 'stock'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# Let us start by loading the data into the workspace, and visualizing the dimensions of all matrices\n",
    "if ds_name == 'stock':\n",
    "    # STOCK DATASET\n",
    "    data = scipy.io.loadmat('datasets/stock.mat')\n",
    "    X_tr = data['xTrain']\n",
    "    S_tr = data['sTrain']\n",
    "    X_tst = data['xTest']\n",
    "    S_tst = data['sTest']\n",
    "\n",
    "elif ds_name == 'concrete':\n",
    "    # CONCRETE DATASET. \n",
    "    data = scipy.io.loadmat('datasets/concrete.mat')\n",
    "    X_tr = data['X_tr']\n",
    "    S_tr = data['S_tr']\n",
    "    X_tst = data['X_tst']\n",
    "    S_tst = data['S_tst']\n",
    "\n",
    "elif ds_name == 'advertising':    \n",
    "    # ADVERTISING DATASET\n",
    "    df = pd.read_csv('datasets/Advertising.csv', header=0)\n",
    "    X_tr = df.values[:150, 1:4]\n",
    "    S_tr = df.values[:150, [-1]]   # The brackets around -1 is to make sure S_tr is a column vector, as in the other datasets\n",
    "    X_tst = df.values[150:, 1:4]\n",
    "    S_tst = df.values[150:, [-1]]\n",
    "\n",
    "else:\n",
    "    print('Unknown dataset')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# Print the data dimension and the dataset sizes\n",
    "print(\"SELECTED DATASET: \" + ds_name)\n",
    "print(\"---- The size of the training set is {0}, that is: {1} samples with dimension {2}.\".format(\n",
    "    X_tr.shape, X_tr.shape[0], X_tr.shape[1]))\n",
    "print(\"---- The target variable of the training set contains {0} samples with dimension {1}\".format(\n",
    "    S_tr.shape[0], S_tr.shape[1]))\n",
    "print(\"---- The size of the test set is {0}, that is: {1} samples with dimension {2}.\".format(\n",
    "    X_tst.shape, X_tst.shape[0], X_tst.shape[1]))\n",
    "print(\"---- The target variable of the test set contains {0} samples with dimension {1}\".format(\n",
    "    S_tst.shape[0], S_tst.shape[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 1.1. Scatter plots\n",
    "\n",
    "We can get a first rough idea about the regression task representing the *scatter plot* of each of the one-dimensional variables against the target data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "pylab.subplots_adjust(hspace=0.2)\n",
    "for idx in range(X_tr.shape[1]):\n",
    "    ax1 = plt.subplot(3,3,idx+1)\n",
    "    ax1.plot(X_tr[:,idx],S_tr,'.')\n",
    "    ax1.get_xaxis().set_ticks([])\n",
    "    ax1.get_yaxis().set_ticks([])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 2. Baseline estimation. Using the average of the training set labels\n",
    "\n",
    "A first very simple method to build the regression model is to use the average of all the target values in the training set as the output of the model, discarding the value of the observation input vector.\n",
    "\n",
    "This approach can be considered as a baseline, given that any other method making an effective use of the observation variables, statistically related to $s$, should improve the performance of this method.\n",
    "\n",
    "The prediction is thus given by"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Mean of all target values in the training set\n",
    "s_hat = np.mean(S_tr)\n",
    "print(s_hat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "for any input ${\\bf x}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "##### Exercise 1\n",
    "\n",
    "Compute the mean square error over training and test sets, for the baseline estimation  method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# We start by defining a function that calculates the average square error\n",
    "def square_error(s, s_est):\n",
    "    # Squeeze is used to make sure that s and s_est have the appropriate dimensions.\n",
    "    y = np.mean(np.power((s - s_est), 2))\n",
    "    # y = np.mean(np.power((np.squeeze(s) - np.squeeze(s_est)), 2))\n",
    "    return y\n",
    "\n",
    "# Mean square error of the baseline prediction over the training data\n",
    "# MSE_tr = <FILL IN>\n",
    "\n",
    "# Mean square error of the baseline prediction over the test data\n",
    "# MSE_tst = <FILL IN>\n",
    "\n",
    "print('Average square error in the training set (baseline method): {0}'.format(MSE_tr))\n",
    "print('Average square error in the test set (baseline method): {0}'.format(MSE_tst))    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Note that in the previous piece of code, function 'square_error' can be used when the second argument is a number instead of a vector with the same length as the first argument. The value will be subtracted from each of the components of the vector provided as the first argument."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "if sys.version_info.major == 2:\n",
    "    Test.assertTrue(np.isclose(MSE_tr, square_error(S_tr, s_hat)),'Incorrect value for MSE_tr')\n",
    "    Test.assertTrue(np.isclose(MSE_tst, square_error(S_tst, s_hat)),'Incorrect value for MSE_tst')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 3. Unidimensional regression with the $k$-nn method\n",
    "\n",
    "The principles of the $k$-nn method are the following:\n",
    "\n",
    "   - For each point where a prediction is to be made, find the $k$ closest neighbors to that point (in the training set)\n",
    "   - Obtain the estimation averaging the labels corresponding to the selected neighbors\n",
    "   \n",
    "The number of neighbors is a hyperparameter that plays an important role in the performance of the method. You can test its influence by changing $k$ in the following piece of code. In particular, you can sart with $k=1$ and observe the efect of increasing the value of $k$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# We implement unidimensional regression using the k-nn method\n",
    "# In other words, the estimations are to be made using only one variable at a time\n",
    "\n",
    "from scipy import spatial\n",
    "\n",
    "var = 0 # pick a variable (e.g., any value from 0 to 8 for the STOCK dataset)\n",
    "k = 1  # Number of neighbors\n",
    "n_points = 1000 # Number of points in the 'x' axis (for representational purposes)\n",
    "\n",
    "# For representational purposes, we will compute the output of the regression model\n",
    "# in a series of equally spaced-points along the x-axis\n",
    "grid_min = np.min([np.min(X_tr[:,var]), np.min(X_tst[:,var])])\n",
    "grid_max = np.max([np.max(X_tr[:,var]), np.max(X_tst[:,var])])\n",
    "X_grid = np.linspace(grid_min,grid_max,num=n_points)\n",
    "\n",
    "def knn_regression(X1, S1, X2, k):\n",
    "    \"\"\" Compute the k-NN regression estimate for the observations contained in\n",
    "        the rows of X2, for the training set given by the rows in X1 and the\n",
    "        components of S1. k is the number of neighbours of the k-NN algorithm\n",
    "    \"\"\"\n",
    "    if X1.ndim == 1:\n",
    "        X1 = np.asmatrix(X1).T\n",
    "    if X2.ndim == 1:\n",
    "        X2 = np.asmatrix(X2).T\n",
    "    distances = spatial.distance.cdist(X1,X2,'euclidean')\n",
    "    neighbors = np.argsort(distances, axis=0, kind='quicksort', order=None)\n",
    "    closest = neighbors[range(k),:]\n",
    "    \n",
    "    est_values = np.zeros([X2.shape[0],1])\n",
    "    for idx in range(X2.shape[0]):\n",
    "        est_values[idx] = np.mean(S1[closest[:,idx]])\n",
    "        \n",
    "    return est_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "est_tst = knn_regression(X_tr[:,var], S_tr, X_tst[:,var], k)\n",
    "est_grid = knn_regression(X_tr[:,var], S_tr, X_grid, k)\n",
    "\n",
    "plt.plot(X_tr[:,var], S_tr,'b.',label='Training points')\n",
    "plt.plot(X_tst[:,var], S_tst,'rx',label='Test points')\n",
    "plt.plot(X_grid, est_grid,'g-',label='Regression model')\n",
    "plt.axis('tight')\n",
    "plt.legend(loc='best')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 3.1. Evolution of the error with the number of neighbors ($k$)\n",
    "\n",
    "We see that a small $k$ results in a regression curve that exhibits many and large oscillations.  The curve is capturing any noise that may be present in the training data, and <i>overfits</i> the training set. On the other hand, picking a too large $k$ (e.g., 200) the regression curve becomes too smooth, averaging out the values of the labels in the training set over large intervals of the observation variable.\n",
    "\n",
    "The next code illustrates this effect by plotting the average training and test square errors as a function of $k$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "var = 0\n",
    "k_max = 60\n",
    "\n",
    "k_max = np.minimum(k_max, X_tr.shape[0])  # k_max cannot be larger than the number of samples\n",
    "\n",
    "#Be careful with the use of range, e.g., range(3) = [0,1,2] and range(1,3) = [1,2]\n",
    "MSEk_tr = [square_error(S_tr, knn_regression(X_tr[:,var], S_tr, X_tr[:,var],k)) \n",
    "           for k in range(1, k_max+1)]\n",
    "MSEk_tst = [square_error(S_tst,knn_regression(X_tr[:,var], S_tr, X_tst[:,var],k)) \n",
    "            for k in range(1, k_max+1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "kgrid = np.arange(1, k_max+1)\n",
    "plt.plot(kgrid, MSEk_tr,'bo', label='Training square error')\n",
    "plt.plot(kgrid, MSEk_tst,'ro', label='Test square error')\n",
    "plt.xlabel('$k$')\n",
    "plt.ylabel('Square Error')\n",
    "plt.axis('tight')\n",
    "\n",
    "plt.legend(loc='best')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "As we can see, the error initially decreases achiving a minimum (in the test set) for some finite value of $k$ ($k\\approx 10$ for the `STOCK` dataset). Increasing the value of $k$ beyond that value results in poorer performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "#### Exercise 2\n",
    "\n",
    "Analize the training MSE for $k=1$. Why is it smaller than for any other $k$? Under which conditions will it be exactly zero?\n",
    "\n",
    "#### Exercise 3\n",
    "\n",
    "Modify the code above to visualize the square error from $k=1$ up to $k$ equal to the number of training instances. Can you relate the square error of the $k$-NN method with that of the baseline method for certain value of $k$? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 3.1. Influence of the input variable\n",
    "\n",
    "Having a look at the scatter plots, we can observe that some observation variables seem to have a more clear relationship with the target value. Thus, we can expect that not all variables are equally useful for the regression task. In the following plot, we carry out a study of the performance that can be achieved with each variable. \n",
    "\n",
    "Note that, in practice, the test labels are not available for the selection of hyperparameter\n",
    "$k$, so we should be careful about the conclusions of this experiment. A more realistic approach will be studied later when we introduce the concept of model validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "k_max = 20\n",
    "\n",
    "var_performance = []\n",
    "k_values = []\n",
    "\n",
    "for var in range(X_tr.shape[1]):\n",
    "    \n",
    "    MSE_tr = [square_error(S_tr, knn_regression(X_tr[:,var], S_tr, X_tr[:, var], k)) \n",
    "              for k in range(1, k_max+1)]\n",
    "    MSE_tst = [square_error(S_tst, knn_regression(X_tr[:,var], S_tr, X_tst[:, var], k)) \n",
    "               for k in range(1, k_max+1)]\n",
    "    MSE_tr = np.asarray(MSE_tr)\n",
    "    MSE_tst = np.asarray(MSE_tst)\n",
    "\n",
    "    # We select the variable associated to the value of k for which the training error is minimum\n",
    "    pos = np.argmin(MSE_tr)\n",
    "    k_values.append(pos + 1)\n",
    "    var_performance.append(MSE_tst[pos])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "plt.stem(range(X_tr.shape[1]), var_performance, use_line_collection=True)\n",
    "plt.title('Results of unidimensional regression ($k$NN)')\n",
    "plt.xlabel('Variable')\n",
    "plt.ylabel('Test MSE')\n",
    "\n",
    "plt.figure(2)\n",
    "plt.stem(range(X_tr.shape[1]), k_values, use_line_collection=True)\n",
    "plt.xlabel('Variable')\n",
    "plt.ylabel('$k$')\n",
    "plt.title('Selection of the hyperparameter')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 4. Multidimensional regression with the $k$-nn method\n",
    "\n",
    "In the previous subsection, we have studied the performance of the $k$-nn method when using only one variable. Doing so was convenient, because it allowed us to plot the regression curves in a 2-D plot, and to get some insight about the consequences of modifying the number of neighbors.\n",
    "\n",
    "For completeness, we evaluate now the performance of the $k$-nn method in this dataset when using all variables together. In fact, when designing a regression model, we should proceed in this manner, using all available information to make as accurate an estimation as possible. In this way, we can also account for correlations that might be present among the different observation variables, and that may carry very relevant information for the regression task.\n",
    "\n",
    "For instance, in the `STOCK` dataset, it may be that the combination of the stock values of two airplane companies is more informative about the price of the target company, while the value for a single company is not enough.\n",
    "\n",
    "<small> Also, in the `CONCRETE` dataset, it may be that for the particular problem at hand the combination of a large proportion of water and a small proportion of coarse grain is a clear indication of certain compressive strength of the material, while the proportion of water or coarse grain alone are not enough to get to that result.</small>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "k_max = 20\n",
    "\n",
    "MSE_tr = [square_error(S_tr, knn_regression(X_tr, S_tr, X_tr, k)) for k in range(1, k_max+1)]\n",
    "MSE_tst = [square_error(S_tst, knn_regression(X_tr, S_tr, X_tst, k)) for k in range(1, k_max+1)]\n",
    "\n",
    "plt.plot(np.arange(k_max)+1, MSE_tr,'bo',label='Training square error')\n",
    "plt.plot(np.arange(k_max)+1, MSE_tst,'ro',label='Test square error')\n",
    "plt.xlabel('k')\n",
    "plt.ylabel('Square error')\n",
    "\n",
    "plt.legend(loc='best')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "In this case, we can check that the average test square error is much lower than the error that was achieved when using only one variable, and also far better than the baseline method. It is also interesting to note that in this particular case the best performance is achieved for a small value of $k$, with the error increasing for larger values of the hyperparameter.\n",
    "\n",
    "Nevertheless, as we discussed previously, these results should be taken carefully. How would we select the value of $k$, if test labels are (obvioulsy) not available for model validation?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 5. Hyperparameter selection via cross-validation\n",
    "\n",
    "### 5.1. Generalization\n",
    "\n",
    "An inconvenient of the application of the $k$-nn method is that the selection of $k$ influences the final error of the algorithm. In the previous experiments, we kept the value of $k$ that minimized the square error on the training set. However, we also noticed that the location of the minimum is not necessarily the same from the perspective of the test data. Ideally, we would like that the designed regression model works as well as possible on future unlabeled patterns that are not available during the training phase. This property is known as <b>generalization</b>. \n",
    "\n",
    "Fitting the training data is only pursued in the hope that we are also indirectly obtaining a model that generalizes well. In order to achieve this goal, there are some strategies that try to guarantee a correct generalization of the model. One of such approaches is known as <b>cross-validation</b> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 5.2. Cross-validation\n",
    "\n",
    "Since using the test labels during the training phase is not allowed (they should be kept aside to simultate the future application of the regression model on unseen patterns), we need to figure out some way to improve our estimation of the hyperparameter that requires only training data. Cross-validation allows us to do so by following the following steps:\n",
    "\n",
    "   - **Split** the training data into several (generally non-overlapping) subsets. If we use $M$ subsets, the method is referred to as $M$-fold cross-validation. If we consider each pattern a different subset, the method is usually referred to as leave-one-out (LOO) cross-validation.\n",
    "   - Carry out the **training** of the system $M$ times. For each run, use a different partition as a <i>validation</i> set, and use the restating partitions as the training set. Evaluate the performance for different choices of the hyperparameter (i.e., for different values of $k$ for the $k$-NN method).\n",
    "   - **Average** the validation error over all partitions, and pick the hyperparameter that provided the minimum validation error.\n",
    "   - **Rerun** the algorithm using all the training data, keeping the value of the parameter that came out of the cross-validation process.\n",
    "   \n",
    "<img src=\"https://chrisjmccormick.files.wordpress.com/2013/07/10_fold_cv.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "### This fragment of code runs k-nn with M-fold cross validation\n",
    "\n",
    "# Parameters:\n",
    "M = 5       # Number of folds for M-cv\n",
    "k_max = 40  # Maximum value of the k-nn hyperparameter to explore\n",
    "\n",
    "# First we compute the train error curve, that will be useful for comparative visualization.\n",
    "MSE_tr = [square_error(S_tr, knn_regression(X_tr, S_tr, X_tr, k)) for k in range(1, k_max+1)]\n",
    "\n",
    "## M-CV\n",
    "# Obtain the indices for the different folds\n",
    "n_tr = X_tr.shape[0]\n",
    "permutation = np.random.permutation(n_tr)\n",
    "\n",
    "# Split the indices in M subsets with (almost) the same size. \n",
    "set_indices = {i: [] for i in range(M)}\n",
    "i = 0\n",
    "for pos in range(n_tr):\n",
    "    set_indices[i].append(permutation[pos])\n",
    "    i = (i+1) % M\n",
    "    \n",
    "# Obtain the validation errors\n",
    "MSE_val = np.zeros((1,k_max))\n",
    "for i in range(M):\n",
    "    val_indices = set_indices[i]\n",
    "    \n",
    "    # Take out the val_indices from the set of indices.\n",
    "    tr_indices = list(set(permutation) - set(val_indices))\n",
    "    \n",
    "    MSE_val_iter = [square_error(S_tr[val_indices], \n",
    "                                 knn_regression(X_tr[tr_indices, :], S_tr[tr_indices], \n",
    "                                                X_tr[val_indices, :], k)) \n",
    "                    for k in range(1, k_max+1)]\n",
    "\n",
    "    MSE_val = MSE_val + np.asarray(MSE_val_iter).T\n",
    "    \n",
    "MSE_val = MSE_val/M\n",
    "\n",
    "# Select the best k based on the validation error\n",
    "k_best = np.argmin(MSE_val) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# Compute the final test MSE for the selecte k\n",
    "MSE_tst = square_error(S_tst, knn_regression(X_tr, S_tr, X_tst, k_best))\n",
    "\n",
    "plt.plot(np.arange(k_max)+1, MSE_tr, 'bo', label='Training square error')\n",
    "plt.plot(np.arange(k_max)+1, MSE_val.T, 'go', label='Validation square error')\n",
    "plt.plot([k_best, k_best], [0, MSE_tst],'r-')\n",
    "plt.plot(k_best, MSE_tst,'ro',label='Test error')\n",
    "plt.legend(loc='best')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "##### Exercise 4\n",
    "\n",
    "Modify the previous code to use only one of the variables in the input dataset\n",
    "  - Following a cross-validation approach, select the best value of $k$ for the $k$-nn based in variable 0 only.\n",
    "  - Compute the test error for the selected valua of $k$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 6. Scikit-learn implementation\n",
    "\n",
    "In practice, most well-known machine learning methods are implemented and available for python. Probably, the most complete module for machine learning tools is <a href=http://scikit-learn.org/stable/>Scikit-learn</a>. The following piece of code uses the method\n",
    "\n",
    "    KNeighborsRegressor\n",
    "   \n",
    "available in Scikit-learn. The example has been taken from <a href=http://scikit-learn.org/stable/auto_examples/neighbors/plot_regression.html>here</a>. As you can check, this routine allows us to build the estimation for a particular point using a weighted average of the targets of the neighbors:\n",
    "\n",
    "   To obtain the estimation at a point ${\\bf x}$:\n",
    "   \n",
    "   - Find $k$ closest points to ${\\bf x}$ in the training set\n",
    "   - Average the corresponding targets, weighting each value according to the distance of each point to ${\\bf x}$, so that closer points have a larger influence in the estimation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# Author: Alexandre Gramfort <alexandre.gramfort@inria.fr>\n",
    "#         Fabian Pedregosa <fabian.pedregosa@inria.fr>\n",
    "#\n",
    "# License: BSD 3 clause (C) INRIA\n",
    "\n",
    "###############################################################################\n",
    "# Generate sample data\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import neighbors\n",
    "\n",
    "np.random.seed(0)\n",
    "X = np.sort(5 * np.random.rand(40, 1), axis=0)\n",
    "T = np.linspace(0, 5, 500)[:, np.newaxis]\n",
    "y = np.sin(X).ravel()\n",
    "\n",
    "# Add noise to targets\n",
    "y[::5] += 1 * (0.5 - np.random.rand(8))\n",
    "\n",
    "###############################################################################\n",
    "# Fit regression model\n",
    "n_neighbors = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "for i, weights in enumerate(['uniform', 'distance']):\n",
    "    knn = neighbors.KNeighborsRegressor(n_neighbors, weights=weights)\n",
    "    y_ = knn.fit(X, y).predict(T)\n",
    "\n",
    "    plt.subplot(2, 1, i + 1)\n",
    "    plt.scatter(X, y, c='k', label='data')\n",
    "    plt.plot(T, y_, c='g', label='prediction')\n",
    "    plt.axis('tight')\n",
    "    plt.legend()\n",
    "    plt.title(\"KNeighborsRegressor (k = %i, weights = '%s')\" % (n_neighbors,\n",
    "                                                                weights))\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Exercise 5\n",
    "\n",
    "Use scikit-learn implementation of the $k$-nn method to compute the generalization error on the `CONCRETE` dataset. Compare the perfomance when using uniform and distance-based weights in the computation the estimates. Visualize the regression curves and error for different values of $k$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
