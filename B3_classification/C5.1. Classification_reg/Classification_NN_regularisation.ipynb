{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color='teal'> Regularisation in Neural Networks </font>\n",
    "\n",
    "    Notebook version: 0.1. (Nov 14, 2024)\n",
    "\n",
    "    Authors: Carlos Sevilla Salcedo (casevill@ing.uc3m.es)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    Changes: v.0.1. (Nov 14, 2024) - First version\n",
    "    \n",
    "    Pending changes:\n",
    "        Examples on transfer learning\n",
    "        Explore different datasets\n",
    "        Analyse combinations of regularisation techniques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Critical plotting imports\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# Set seed\n",
    "torch.manual_seed(1337)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(1337)\n",
    "    \n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "size = 14\n",
    "params = {'legend.fontsize': 'Large',\n",
    "          'axes.labelsize': size,\n",
    "          'axes.titlesize': size,\n",
    "          'xtick.labelsize': size*0.75,\n",
    "          'ytick.labelsize': size*0.75}\n",
    "plt.rcParams.update(params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color='teal'> 1. Introduction </font>\n",
    "\n",
    "This notebook will be dedicated to exploring the regularisation on neural networks.\n",
    "\n",
    "## <font color='teal'> Why Regularisation in Neural Networks is Crucial </font>\n",
    "\n",
    "Neural networks are incredibly powerful tools for modeling complex data relationships. Their ability to approximate non-linear functions has made them a cornerstone of modern machine learning applications, from image recognition to natural language processing.\n",
    "\n",
    "However, this power comes with a risk: **overfitting**. Neural networks are models that inherently tend to be overparametrised. Wht this means is that by including more layers and make the model deeper the model has enough expressiveness to almost perfectly fit the patterns in the training data, leading to poor generalisation to the test set.  This is mostly observed when a neural network is trained on limited or noisy data. Overfitting results in poor performance on test data, undermining the model's ability to make accurate predictions in real-world scenarios.\n",
    "\n",
    "Keeping this in mind, how can we ensure that neural networks learn meaningful patterns while avoiding overfitting? The answer lies in **regularisation**—a set of techniques designed to constrain the learning process, making models more robust and generalizable.\n",
    "\n",
    "In this notebook, we will explore various regularization methods, both theoretical and practical. By the end, you’ll understand how to implement these techniques in PyTorch and evaluate their impact on neural network performance.\n",
    "\n",
    "## <font color='teal'> Regularisation techniques </font>\n",
    "There are different approaches that help mitigating the overfitting to the training set. We could group them into:\n",
    "* __Parametric__: adding prior knowledge to model\n",
    "  * Initializing weights with specific distributions.\n",
    "* __Constraining__: restricting the model to simplify it by giving it less degrees of freedom.\n",
    "  * L1 and L2 (weight decay) regularization, restricting the learnable loss function\n",
    "  * Early Stopping, restricting the number of iterations\n",
    "  * Dropout, restricting the number of neurons\n",
    "* __Indirect__: not regularization but has same effect\n",
    "  * Batch Normalization\n",
    "  * Data Augmentation\n",
    "  * Transfer Learning / Fine Tuning\n",
    "\n",
    "This notebook provides an introduction to the main concepts necessary to regularise NNs with PyTorch taking materials from:\n",
    "* <a href=\"https://gist.github.com/sujitpal/2bffa9d5d93510a201f4064f25c7abad\">Regularisation in NNs</a>, by Github user sujitpal\n",
    "* <a href=\"https://pytorch.org/tutorials/beginner/pytorch_with_examples.html\">Learning PyTorch with Examples</a>, by Justin Johnson\n",
    "* <a href=\"https://pytorch.org/tutorials/beginner/nn_tutorial.html\">What is *torch.nn* really?</a>, by Jeremy Howard\n",
    "* <a href=\"https://www.kaggle.com/kanncaa1/pytorch-tutorial-for-deep-learning-lovers\">Pytorch Tutorial for Deep Learning Lovers</a>, by Kaggle user kanncaa1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color='teal'> 2. Datasets </font>\n",
    "\n",
    "Along this notebook, we will use the `digits` dataset from the previous notebook to run some experiments and explore the different regularisation techniques."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def get_dataset(dataset_name, forze_binary=False):\n",
    "    \"\"\"\n",
    "    Loads the selected dataset, among two options: DogsCats or digits.\n",
    "\n",
    "    If dataset_name == 'digits', you can take a dataset with two classes only,\n",
    "    using forze_binary == True\n",
    "    \"\"\"\n",
    "\n",
    "    if dataset_name == 'DogsCats':\n",
    "        DogsCatsX = np.load('./data/DogsCats/X.npy')\n",
    "        DogsCatsY = np.load('./data/DogsCats/Y.npy')\n",
    "\n",
    "        # Flatten images to get 1D inputs\n",
    "        K = DogsCatsX.shape[0]\n",
    "        img_size = DogsCatsX.shape[1]\n",
    "        DogsCatsX_flatten = DogsCatsX.reshape(K,img_size*img_size)\n",
    "        X = DogsCatsX_flatten\n",
    "        y = DogsCatsY\n",
    "    elif dataset_name == 'digits':\n",
    "        digitsX = np.load('./data/Sign-language-digits-dataset/X.npy')\n",
    "        digitsY = np.load('./data/Sign-language-digits-dataset/Y.npy')\n",
    "\n",
    "        # Flatten images (to get 1-dimensional inputs\n",
    "        K = digitsX.shape[0]\n",
    "        img_size = digitsX.shape[1]\n",
    "        digitsX_flatten = digitsX.reshape(K,img_size*img_size)\n",
    "        if forze_binary:\n",
    "            # Zero and Ones are one hot encoded in columns 1 and 4\n",
    "            X0 = digitsX_flatten[np.argmax(digitsY, axis=1)==1,]\n",
    "            X1 = digitsX_flatten[np.argmax(digitsY, axis=1)==4,]\n",
    "            X = np.vstack((X0, X1))\n",
    "            y = np.zeros(X.shape[0])\n",
    "            y[X0.shape[0]:] = 1\n",
    "        else:\n",
    "            X = digitsX_flatten\n",
    "            y = digitsY\n",
    "    else:\n",
    "        print(\"-- ERROR: Unknown dataset\")\n",
    "        return\n",
    "\n",
    "    # Joint normalization of all data. For images [-.5, .5] scaling is frequent\n",
    "    min_max_scaler = MinMaxScaler(feature_range=(-.5, .5))\n",
    "    X = min_max_scaler.fit_transform(X)\n",
    "\n",
    "    # Generate train and validation data, shuffle\n",
    "    X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42, shuffle=True)\n",
    "\n",
    "    return X_train, X_val, y_train, y_val"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we will define the dataset we will utilise for the following sections. Depending on how fast you want to run the experiments or the device in which you are doing it you might want to tweak the number of samples. However, to fully observe the overfitting we are exploring is recommended to use the whole dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "dataset = 'digits' # 'digits' or 'DogsCats'\n",
    "N = 5000 # Number of samples to use for training, -1 for all, 5000 for testing\n",
    "\n",
    "X_train, X_val, y_train, y_val = get_dataset(dataset)\n",
    "\n",
    "min_max_scaler = MinMaxScaler(feature_range=(-.5, .5))\n",
    "X_train = min_max_scaler.fit_transform(X_train)\n",
    "X_val = min_max_scaler.fit_transform(X_val)\n",
    "\n",
    "# Convert to Torch tensors\n",
    "X_train_torch = torch.from_numpy(X_train[:N]) if N != -1 else torch.from_numpy(X_train)\n",
    "X_val_torch = torch.from_numpy(X_val)\n",
    "y_train_torch = torch.from_numpy(y_train[:N]) if N != -1 else torch.from_numpy(y_train)\n",
    "y_val_torch = torch.from_numpy(y_val)\n",
    "\n",
    "batch_size = 64\n",
    "\n",
    "ds_train = TensorDataset(X_train_torch, y_train_torch)\n",
    "ds_valid = TensorDataset(X_val_torch, y_val_torch)\n",
    "train_loader = DataLoader(ds_train, batch_size=batch_size)\n",
    "valid_loader = DataLoader(ds_valid, batch_size=batch_size)\n",
    "\n",
    "# Now we've training, validation and test set\n",
    "print('train : ', len(ds_train))\n",
    "print('valid : ', len(ds_valid))\n",
    "\n",
    "test_batch = next(train_loader.__iter__())\n",
    "print('train batch shape:', test_batch[0].shape)\n",
    "\n",
    "# Show sample images\n",
    "selected = [260, 340]\n",
    "img_size = int(np.sqrt(X_train.shape[1]))\n",
    "plt.subplot(1, 2, 1), plt.imshow(X_train[selected[0]].reshape(img_size, img_size)), plt.axis('off')\n",
    "plt.subplot(1, 2, 2), plt.imshow(X_train[selected[1]].reshape(img_size, img_size)), plt.axis('off')\n",
    "plt.show()\n",
    "print('Labels corresponding to figures:', y_train[selected,])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color='teal'> 3. Base network </font>\n",
    "\n",
    "This section presents the basic structure of the neural network we will start from. Thi is composed solely of linear layers followed by their corresponding activation functions and one last layer for classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedforwardNeuralNetModel(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim, hidden_dim= (200, 50, 20)):\n",
    "        super(FeedforwardNeuralNetModel, self).__init__()\n",
    "\n",
    "        sizes = [input_dim] + list(map(int, hidden_dim)) + [output_dim]\n",
    "        layers = []\n",
    "        for i in range(len(sizes) - 2):\n",
    "            layers.append(nn.Linear(sizes[i], sizes[i + 1]))\n",
    "            layers.append(nn.ReLU())\n",
    "        layers.append(nn.Linear(sizes[-2], sizes[-1]))\n",
    "        self.net = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):        \n",
    "        out = self.net(x)    \n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can explore different configurations of the neural network just by increasing the number of hidden dimensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dim = train_loader.dataset.tensors[0].shape[1]\n",
    "hidden_dim = (200, 50, 20)\n",
    "output_dim = train_loader.dataset.tensors[1].shape[1] if dataset == 'digits' else 1\n",
    "\n",
    "# Instantiate model class and assign to object\n",
    "model = FeedforwardNeuralNetModel(input_dim, output_dim, hidden_dim)\n",
    "\n",
    "# Push model to CUDA device if available\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "# Loss function (binary if DogsCats, otherwise multi-class)\n",
    "loss_func = nn.BCEWithLogitsLoss() if dataset == 'DogsCats' else nn.CrossEntropyLoss()\n",
    "\n",
    "# Optimizer\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=1e-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice how we have used the `BCEWithLogitsLoss` loss function. This is because we are using a sigmoid activation function in the output layer. This function combines a sigmoid activation function with the binary cross-entropy loss. This is useful when we are dealing with binary classification problems.\n",
    "\n",
    "Let's take a look now at the number of parameters in our network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of groups of parameters\n",
    "print('Number of groups of parameters {}'.format(len(list(model.parameters()))))\n",
    "print('-'*50)\n",
    "# Print parameters\n",
    "for i in range(len(list(model.parameters()))):\n",
    "    print(list(model.parameters())[i].size())\n",
    "print('-'*50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can observe how the number of parameters in the model is increasing as we add more layers. The number of parameters in the model is given by the number of weights and biases in the model. The number of weights in a layer is given by the number of neurons in the current layer multiplied by the number of neurons in the previous layer. The number of biases in a layer is given by the number of neurons in the current layer. The number of parameters in a layer is given by the sum of the number of weights and biases in the layer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's train our network to see how it performs in our classification task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(q, y):\n",
    "    return (y.argmax(axis=-1) == q.argmax(axis=-1)).float().mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "num_epochs = 100\n",
    "best_accuracy = 0\n",
    "loss_train = np.zeros(num_epochs)\n",
    "loss_val = np.zeros(num_epochs)\n",
    "acc_train = np.zeros(num_epochs)\n",
    "acc_val = np.zeros(num_epochs)\n",
    "\n",
    "# Check if the best model exists\n",
    "pretrained = False\n",
    "hidden_dim_str = '_'.join(map(str, hidden_dim))\n",
    "model_path = f'models/best_model_{dataset}_N{int(X_train_torch.shape[0])}_hidden{hidden_dim_str}.pth'\n",
    "if os.path.exists(model_path) and pretrained:\n",
    "    print(\"Loading best model and training history...\")\n",
    "    checkpoint = torch.load(model_path)\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    loss_train = checkpoint['loss_train']\n",
    "    loss_val = checkpoint['loss_val']\n",
    "    acc_train = checkpoint['acc_train']\n",
    "    acc_val = checkpoint['acc_val']\n",
    "    best_accuracy = checkpoint['best_accuracy']\n",
    "else:\n",
    "    print(\"Best model not found. Running optimization...\")\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "\n",
    "        for xb, yb in train_loader:\n",
    "            # Ensure the input tensors are of type float32\n",
    "            xb = xb.float()\n",
    "            yb = yb.reshape(-1, 1).float() if dataset == 'DogsCats' else yb.argmax(axis=-1)\n",
    "\n",
    "            #Compute network output and cross-entropy loss for current minibatch\n",
    "            pred = model(xb)\n",
    "            loss = loss_func(pred, yb)\n",
    "\n",
    "            #Compute gradients and optimize parameters\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "                \n",
    "        #At the end of each epoch, evaluate overall network performance\n",
    "        with torch.no_grad():\n",
    "            #Computing network performance after iteration\n",
    "            pred = model(X_train_torch.float())\n",
    "            loss_train[epoch] = loss_func(pred, y_train_torch.reshape(-1, 1).float()).item() if dataset == 'DogsCats' else loss_func(pred, y_train_torch.argmax(axis=-1)).item()\n",
    "            acc_train[epoch] = accuracy(pred, y_train_torch.reshape(-1, 1).float()).item() if dataset == 'DogsCats' else accuracy(pred, y_train_torch).item()\n",
    "            pred_val = model(X_val_torch.float())\n",
    "            loss_val[epoch] = loss_func(pred_val, y_val_torch.reshape(-1, 1).float()).item() if dataset == 'DogsCats' else loss_func(pred_val, y_val_torch.argmax(axis=-1)).item()\n",
    "            acc_val[epoch] = accuracy(pred_val, y_val_torch.reshape(-1, 1).float()).item() if dataset == 'DogsCats' else accuracy(pred_val, y_val_torch).item()\n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss_train[epoch]:.4f}, Accuracy: {acc_train[epoch]:.4f}, Val Loss: {loss_val[epoch]:.4f}, Val Accuracy: {acc_val[epoch]:.4f}')\n",
    "        \n",
    "        # Save model when accuracy beats best accuracy\n",
    "        if acc_val[epoch] > best_accuracy:\n",
    "            best_accuracy = acc_val[epoch]\n",
    "            # Save the model state and training history\n",
    "            torch.save({\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'loss_train': loss_train,\n",
    "                'loss_val': loss_val,\n",
    "                'acc_train': acc_train,\n",
    "                'acc_val': acc_val,\n",
    "                'best_accuracy': best_accuracy\n",
    "            }, model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(14,5))\n",
    "plt.subplot(1, 2, 1), plt.plot(loss_train, 'b'), plt.plot(loss_val, 'r'), plt.legend(['train', 'val']), plt.title('Cross-entropy loss')\n",
    "plt.subplot(1, 2, 2), plt.plot(acc_train, 'b'), plt.plot(acc_val, 'r'), plt.legend(['train', 'val']), plt.title('Accuracy')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color='teal'> 4. Parametric regularisation </font>\n",
    "\n",
    "These techniques introduce prior knowledge into the model through constraints on the parameters.\n",
    "\n",
    "**Weight Initialization:**  \n",
    "By initializing weights using specific distributions, we can guide the model toward better convergence and avoid poor local minima. Common initialization methods include:\n",
    "- **Xavier Initialization:** Scales weights based on the number of neurons in the layer, promoting stable gradients.\n",
    "- **He Initialization:** Tailored for activation functions like ReLU, it initializes weights to prevent vanishing/exploding gradients.\n",
    "\n",
    "Let's modify our network to test this initialisation technique."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedforwardNeuralNetModel(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim, hidden_dim= (200, 50, 20), initialize_weights=False):\n",
    "        super(FeedforwardNeuralNetModel, self).__init__()\n",
    "\n",
    "        sizes = [input_dim] + list(map(int, hidden_dim)) + [output_dim]\n",
    "        layers = []\n",
    "        for i in range(len(sizes) - 2):\n",
    "            layers.append(nn.Linear(sizes[i], sizes[i + 1]))\n",
    "            if initialize_weights:\n",
    "                torch.nn.init.xavier_uniform_(layers[-1].weight)\n",
    "            layers.append(nn.ReLU())\n",
    "        layers.append(nn.Linear(sizes[-2], sizes[-1]))\n",
    "        self.net = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):        \n",
    "        out = self.net(x)    \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dim = train_loader.dataset.tensors[0].shape[1]\n",
    "hidden_dim = (200, 50, 20)\n",
    "output_dim = train_loader.dataset.tensors[1].shape[1] if dataset == 'digits' else 1\n",
    "\n",
    "# Instantiate model class and assign to object\n",
    "model = FeedforwardNeuralNetModel(input_dim, output_dim, hidden_dim, initialize_weights=True)\n",
    "\n",
    "# Push model to CUDA device if available\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "# Loss function\n",
    "loss_func = nn.BCEWithLogitsLoss() if dataset == 'DogsCats' else nn.CrossEntropyLoss()\n",
    "\n",
    "# Optimizer\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 100\n",
    "best_accuracy = 0\n",
    "loss_train = np.zeros(num_epochs)\n",
    "loss_val = np.zeros(num_epochs)\n",
    "acc_train = np.zeros(num_epochs)\n",
    "acc_val = np.zeros(num_epochs)\n",
    "\n",
    "# Check if the best model exists\n",
    "reg_technique = 'He'\n",
    "pretrained = False\n",
    "hidden_dim_str = '_'.join(map(str, hidden_dim))\n",
    "model_path = f'models/best_model_{dataset}_{reg_technique}_N{int(X_train_torch.shape[0])}_hidden{hidden_dim_str}.pth'\n",
    "if os.path.exists(model_path) and pretrained:\n",
    "    print(\"Loading best model and training history...\")\n",
    "    checkpoint = torch.load(model_path)\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    loss_train = checkpoint['loss_train']\n",
    "    loss_val = checkpoint['loss_val']\n",
    "    acc_train = checkpoint['acc_train']\n",
    "    acc_val = checkpoint['acc_val']\n",
    "    best_accuracy = checkpoint['best_accuracy']\n",
    "else:\n",
    "    print(\"Best model not found. Running optimization...\")\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "\n",
    "        for xb, yb in train_loader:\n",
    "            # Ensure the input tensors are of type float32\n",
    "            xb = xb.float()\n",
    "            yb = yb.reshape(-1, 1).float() if dataset == 'DogsCats' else yb.argmax(axis=-1)\n",
    "\n",
    "            #Compute network output and cross-entropy loss for current minibatch\n",
    "            pred = model(xb)\n",
    "            loss = loss_func(pred, yb)\n",
    "\n",
    "            #Compute gradients and optimize parameters\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "                \n",
    "        #At the end of each epoch, evaluate overall network performance\n",
    "        with torch.no_grad():\n",
    "            #Computing network performance after iteration\n",
    "            pred = model(X_train_torch.float())\n",
    "            loss_train[epoch] = loss_func(pred, y_train_torch.reshape(-1, 1).float()).item() if dataset == 'DogsCats' else loss_func(pred, y_train_torch.argmax(axis=-1)).item()\n",
    "            acc_train[epoch] = accuracy(pred, y_train_torch.reshape(-1, 1).float()).item() if dataset == 'DogsCats' else accuracy(pred, y_train_torch).item()\n",
    "            pred_val = model(X_val_torch.float())\n",
    "            loss_val[epoch] = loss_func(pred_val, y_val_torch.reshape(-1, 1).float()).item() if dataset == 'DogsCats' else loss_func(pred_val, y_val_torch.argmax(axis=-1)).item()\n",
    "            acc_val[epoch] = accuracy(pred_val, y_val_torch.reshape(-1, 1).float()).item() if dataset == 'DogsCats' else accuracy(pred_val, y_val_torch).item()\n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss_train[epoch]:.4f}, Accuracy: {acc_train[epoch]:.4f}, Val Loss: {loss_val[epoch]:.4f}, Val Accuracy: {acc_val[epoch]:.4f}')\n",
    "        \n",
    "        # Save model when accuracy beats best accuracy\n",
    "        if acc_val[epoch] > best_accuracy:\n",
    "            best_accuracy = acc_val[epoch]\n",
    "            # Save the model state and training history\n",
    "            torch.save({\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'loss_train': loss_train,\n",
    "                'loss_val': loss_val,\n",
    "                'acc_train': acc_train,\n",
    "                'acc_val': acc_val,\n",
    "                'best_accuracy': best_accuracy\n",
    "            }, model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(14,5))\n",
    "plt.subplot(1, 2, 1), plt.plot(loss_train, 'b'), plt.plot(loss_val, 'r'), plt.legend(['train', 'val']), plt.title('Cross-entropy loss')\n",
    "plt.subplot(1, 2, 2), plt.plot(acc_train, 'b'), plt.plot(acc_val, 'r'), plt.legend(['train', 'val']), plt.title('Accuracy')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems like using He initialisation slightly improves the performance of the model in validation. However, he improvement is not substantial."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color='teal'> 5. Constraining regularisation </font>\n",
    "Techniques that directly constrain the function being learned by adding penalties to different parts of the network.\n",
    "\n",
    "### <font color='teal'> 5.1. Lasso regularisation </font>\n",
    "Adds a l1 penalty proportional to the sum of the absolute values of the weights. Encourages sparsity (i.e., some weights become zero).\n",
    "  $$\n",
    "  L = L_{\\text{original}} + \\lambda \\sum |w_i|\n",
    "  $$\n",
    "Let's modify our optimisation to restrict the weights of the network towards 0. This is done by adding a regularizer term to the loss function containing the 1-norm of the parameters of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dim = train_loader.dataset.tensors[0].shape[1]\n",
    "hidden_dim = (200, 50, 20)\n",
    "output_dim = train_loader.dataset.tensors[1].shape[1] if dataset == 'digits' else 1\n",
    "\n",
    "# Instantiate model class and assign to object\n",
    "model = FeedforwardNeuralNetModel(input_dim, output_dim, hidden_dim)\n",
    "\n",
    "# Push model to CUDA device if available\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "# Loss function\n",
    "loss_func = nn.BCEWithLogitsLoss() if dataset == 'DogsCats' else nn.CrossEntropyLoss()\n",
    "\n",
    "# Optimizer\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 100\n",
    "best_accuracy = 0\n",
    "loss_train = np.zeros(num_epochs)\n",
    "loss_val = np.zeros(num_epochs)\n",
    "acc_train = np.zeros(num_epochs)\n",
    "acc_val = np.zeros(num_epochs)\n",
    "\n",
    "# Check if the best model exists\n",
    "l1_lambda = 1e-3\n",
    "reg_technique = 'l1'\n",
    "pretrained = False\n",
    "hidden_dim_str = '_'.join(map(str, hidden_dim))\n",
    "model_path = f'models/best_model_{dataset}_{reg_technique}_{l1_lambda}_N{int(X_train_torch.shape[0])}_hidden{hidden_dim_str}.pth'\n",
    "if os.path.exists(model_path) and pretrained:\n",
    "    print(\"Loading best model and training history...\")\n",
    "    checkpoint = torch.load(model_path)\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    loss_train = checkpoint['loss_train']\n",
    "    loss_val = checkpoint['loss_val']\n",
    "    acc_train = checkpoint['acc_train']\n",
    "    acc_val = checkpoint['acc_val']\n",
    "    best_accuracy = checkpoint['best_accuracy']\n",
    "else:\n",
    "    print(\"Best model not found. Running optimization...\")\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "\n",
    "        for xb, yb in train_loader:\n",
    "            # Ensure the input tensors are of type float32\n",
    "            xb = xb.float()\n",
    "            yb = yb.reshape(-1, 1).float() if dataset == 'DogsCats' else yb.argmax(axis=-1)\n",
    "\n",
    "            #Compute network output and cross-entropy loss for current minibatch\n",
    "            pred = model(xb)\n",
    "            loss = loss_func(pred, yb)\n",
    "            # ####################################################################\n",
    "            # Apply L1 regularization to all layers\n",
    "            l1_norm = 0\n",
    "\n",
    "            for name, param in model.named_parameters():\n",
    "                if 'weight' in name:\n",
    "                    l1_norm += torch.norm(param, 1)\n",
    "\n",
    "            loss += l1_lambda * l1_norm\n",
    "            # ####################################################################\n",
    "\n",
    "            #Compute gradients and optimize parameters\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "                \n",
    "        #At the end of each epoch, evaluate overall network performance\n",
    "        with torch.no_grad():\n",
    "            #Computing network performance after iteration\n",
    "            pred = model(X_train_torch.float())\n",
    "            loss_train[epoch] = loss_func(pred, y_train_torch.reshape(-1, 1).float()).item() if dataset == 'DogsCats' else loss_func(pred, y_train_torch.argmax(axis=-1)).item()\n",
    "            acc_train[epoch] = accuracy(pred, y_train_torch.reshape(-1, 1).float()).item() if dataset == 'DogsCats' else accuracy(pred, y_train_torch).item()\n",
    "            pred_val = model(X_val_torch.float())\n",
    "            loss_val[epoch] = loss_func(pred_val, y_val_torch.reshape(-1, 1).float()).item() if dataset == 'DogsCats' else loss_func(pred_val, y_val_torch.argmax(axis=-1)).item()\n",
    "            acc_val[epoch] = accuracy(pred_val, y_val_torch.reshape(-1, 1).float()).item() if dataset == 'DogsCats' else accuracy(pred_val, y_val_torch).item()\n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss_train[epoch]:.4f}, Accuracy: {acc_train[epoch]:.4f}, Val Loss: {loss_val[epoch]:.4f}, Val Accuracy: {acc_val[epoch]:.4f}')\n",
    "        \n",
    "        # Save model when accuracy beats best accuracy\n",
    "        if acc_val[epoch] > best_accuracy:\n",
    "            best_accuracy = acc_val[epoch]\n",
    "            # Save the model state and training history\n",
    "            torch.save({\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'loss_train': loss_train,\n",
    "                'loss_val': loss_val,\n",
    "                'acc_train': acc_train,\n",
    "                'acc_val': acc_val,\n",
    "                'best_accuracy': best_accuracy\n",
    "            }, model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(14,5))\n",
    "plt.subplot(1, 2, 1), plt.plot(loss_train, 'b'), plt.plot(loss_val, 'r'), plt.legend(['train', 'val']), plt.title('Cross-entropy loss')\n",
    "plt.subplot(1, 2, 2), plt.plot(acc_train, 'b'), plt.plot(acc_val, 'r'), plt.legend(['train', 'val']), plt.title('Accuracy')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color='teal'> 5.2. Ridge regularisation </font>\n",
    "Adds a l2 penalty proportional to the sum of the squared values of the weights. \n",
    "  $$\n",
    "  L = L_{\\text{original}} + \\lambda \\sum (w_i)^2\n",
    "  $$\n",
    "This can be done in two different ways: (1) modify the penalisation from l1 to be squared instead of absloute value, or (2) adding a `weight_decay` parameter to the optimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dim = train_loader.dataset.tensors[0].shape[1]\n",
    "hidden_dim = (200, 50, 20)\n",
    "output_dim = train_loader.dataset.tensors[1].shape[1] if dataset == 'digits' else 1\n",
    "\n",
    "# Instantiate model class and assign to object\n",
    "model = FeedforwardNeuralNetModel(input_dim, output_dim, hidden_dim)\n",
    "\n",
    "# Push model to CUDA device if available\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "# Loss function\n",
    "loss_func = nn.BCEWithLogitsLoss() if dataset == 'DogsCats' else nn.CrossEntropyLoss()\n",
    "\n",
    "# Optimizer\n",
    "l2_lambda = 1e-2\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3, weight_decay=l2_lambda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 100\n",
    "best_accuracy = 0\n",
    "loss_train = np.zeros(num_epochs)\n",
    "loss_val = np.zeros(num_epochs)\n",
    "acc_train = np.zeros(num_epochs)\n",
    "acc_val = np.zeros(num_epochs)\n",
    "\n",
    "# Check if the best model exists\n",
    "reg_technique = 'l2'\n",
    "pretrained = False\n",
    "hidden_dim_str = '_'.join(map(str, hidden_dim))\n",
    "model_path = f'models/best_model_{dataset}_{reg_technique}_{l2_lambda}_N{int(X_train_torch.shape[0])}_hidden{hidden_dim_str}.pth'\n",
    "if os.path.exists(model_path) and pretrained:\n",
    "    print(\"Loading best model and training history...\")\n",
    "    checkpoint = torch.load(model_path)\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    loss_train = checkpoint['loss_train']\n",
    "    loss_val = checkpoint['loss_val']\n",
    "    acc_train = checkpoint['acc_train']\n",
    "    acc_val = checkpoint['acc_val']\n",
    "    best_accuracy = checkpoint['best_accuracy']\n",
    "else:\n",
    "    print(\"Best model not found. Running optimization...\")\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "\n",
    "        for xb, yb in train_loader:\n",
    "            # Ensure the input tensors are of type float32\n",
    "            xb = xb.float()\n",
    "            yb = yb.reshape(-1, 1).float() if dataset == 'DogsCats' else yb.argmax(axis=-1)\n",
    "\n",
    "            #Compute network output and cross-entropy loss for current minibatch\n",
    "            pred = model(xb)\n",
    "            loss = loss_func(pred, yb)\n",
    "\n",
    "            #Compute gradients and optimize parameters\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "                \n",
    "        #At the end of each epoch, evaluate overall network performance\n",
    "        with torch.no_grad():\n",
    "            #Computing network performance after iteration\n",
    "            pred = model(X_train_torch.float())\n",
    "            loss_train[epoch] = loss_func(pred, y_train_torch.reshape(-1, 1).float()).item() if dataset == 'DogsCats' else loss_func(pred, y_train_torch.argmax(axis=-1)).item()\n",
    "            acc_train[epoch] = accuracy(pred, y_train_torch.reshape(-1, 1).float()).item() if dataset == 'DogsCats' else accuracy(pred, y_train_torch).item()\n",
    "            pred_val = model(X_val_torch.float())\n",
    "            loss_val[epoch] = loss_func(pred_val, y_val_torch.reshape(-1, 1).float()).item() if dataset == 'DogsCats' else loss_func(pred_val, y_val_torch.argmax(axis=-1)).item()\n",
    "            acc_val[epoch] = accuracy(pred_val, y_val_torch.reshape(-1, 1).float()).item() if dataset == 'DogsCats' else accuracy(pred_val, y_val_torch).item()\n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss_train[epoch]:.4f}, Accuracy: {acc_train[epoch]:.4f}, Val Loss: {loss_val[epoch]:.4f}, Val Accuracy: {acc_val[epoch]:.4f}')\n",
    "        \n",
    "        # Save model when accuracy beats best accuracy\n",
    "        if acc_val[epoch] > best_accuracy:\n",
    "            best_accuracy = acc_val[epoch]\n",
    "            # Save the model state and training history\n",
    "            torch.save({\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'loss_train': loss_train,\n",
    "                'loss_val': loss_val,\n",
    "                'acc_train': acc_train,\n",
    "                'acc_val': acc_val,\n",
    "                'best_accuracy': best_accuracy\n",
    "            }, model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(14,5))\n",
    "plt.subplot(1, 2, 1), plt.plot(loss_train, 'b'), plt.plot(loss_val, 'r'), plt.legend(['train', 'val']), plt.title('Cross-entropy loss')\n",
    "plt.subplot(1, 2, 2), plt.plot(acc_train, 'b'), plt.plot(acc_val, 'r'), plt.legend(['train', 'val']), plt.title('Accuracy')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color='teal'> 5.3. Early stopping </font>\n",
    "This technique monitors the validation performance and halts training when the improvement stops, preventing overfitting due to excessive training.\n",
    "It's a bit controversial as we are stopping the model mid training so we are not converging to the optimum solution.\n",
    "\n",
    "**Key parameters**:\n",
    "- **patience**: Number of epochs without improvement to wait before stopping.\n",
    "- **min_delta**: Minimum improvement to consider as improvement.\n",
    "\n",
    "**Steps to carry out early stopping**:\n",
    "We will use the following early stopping technique:\n",
    "1. If the **validation loss does not decrease** for a certain number of epochs, we will stop training.\n",
    "2. We will **save** the model with the **best validation loss**.\n",
    "3. We will use the model with the best validation loss to **make predictions** on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dim = train_loader.dataset.tensors[0].shape[1]\n",
    "hidden_dim = (200, 50, 20)\n",
    "output_dim = train_loader.dataset.tensors[1].shape[1] if dataset == 'digits' else 1\n",
    "\n",
    "# Instantiate model class and assign to object\n",
    "model = FeedforwardNeuralNetModel(input_dim, output_dim, hidden_dim)\n",
    "\n",
    "# Push model to CUDA device if available\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "# Loss function\n",
    "loss_func = nn.BCEWithLogitsLoss() if dataset == 'DogsCats' else nn.CrossEntropyLoss()\n",
    "\n",
    "# Optimizer\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 100\n",
    "best_accuracy = 0\n",
    "loss_train = np.zeros(num_epochs)\n",
    "loss_val = np.zeros(num_epochs)\n",
    "acc_train = np.zeros(num_epochs)\n",
    "acc_val = np.zeros(num_epochs)\n",
    "\n",
    "# Early stopping parameters\n",
    "early_stop = False \n",
    "min_val_loss = float('inf')\n",
    "patience = 10\n",
    "min_delta = 0.01\n",
    "epochs_no_improve = 0\n",
    "\n",
    "# Check if the best model exists\n",
    "reg_technique = 'early_stop'\n",
    "pretrained = False\n",
    "hidden_dim_str = '_'.join(map(str, hidden_dim))\n",
    "model_path = f'models/best_model_{dataset}_{reg_technique}_N{int(X_train_torch.shape[0])}_hidden{hidden_dim_str}.pth'\n",
    "if os.path.exists(model_path) and pretrained:\n",
    "    print(\"Loading best model and training history...\")\n",
    "    checkpoint = torch.load(model_path)\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    loss_train = checkpoint['loss_train']\n",
    "    loss_val = checkpoint['loss_val']\n",
    "    acc_train = checkpoint['acc_train']\n",
    "    acc_val = checkpoint['acc_val']\n",
    "    best_accuracy = checkpoint['best_accuracy']\n",
    "else:\n",
    "    print(\"Best model not found. Running optimization...\")\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "\n",
    "        for xb, yb in train_loader:\n",
    "            # Ensure the input tensors are of type float32\n",
    "            xb = xb.float()\n",
    "            yb = yb.reshape(-1, 1).float() if dataset == 'DogsCats' else yb.argmax(axis=-1)\n",
    "\n",
    "            #Compute network output and cross-entropy loss for current minibatch\n",
    "            pred = model(xb)\n",
    "            loss = loss_func(pred, yb)\n",
    "\n",
    "            #Compute gradients and optimize parameters\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "                \n",
    "        #At the end of each epoch, evaluate overall network performance\n",
    "        with torch.no_grad():\n",
    "            #Computing network performance after iteration\n",
    "            pred = model(X_train_torch.float())\n",
    "            loss_train[epoch] = loss_func(pred, y_train_torch.reshape(-1, 1).float()).item() if dataset == 'DogsCats' else loss_func(pred, y_train_torch.argmax(axis=-1)).item()\n",
    "            acc_train[epoch] = accuracy(pred, y_train_torch.reshape(-1, 1).float()).item() if dataset == 'DogsCats' else accuracy(pred, y_train_torch).item()\n",
    "            pred_val = model(X_val_torch.float())\n",
    "            loss_val[epoch] = loss_func(pred_val, y_val_torch.reshape(-1, 1).float()).item() if dataset == 'DogsCats' else loss_func(pred_val, y_val_torch.argmax(axis=-1)).item()\n",
    "            acc_val[epoch] = accuracy(pred_val, y_val_torch.reshape(-1, 1).float()).item() if dataset == 'DogsCats' else accuracy(pred_val, y_val_torch).item()\n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss_train[epoch]:.4f}, Accuracy: {acc_train[epoch]:.4f}, Val Loss: {loss_val[epoch]:.4f}, Val Accuracy: {acc_val[epoch]:.4f}')\n",
    "        \n",
    "        ### Early Stopping Check ###\n",
    "        if loss_val[epoch] < min_val_loss - min_delta:\n",
    "            min_val_loss = loss_val[epoch]\n",
    "            epochs_no_improve = 0\n",
    "            torch.save(model.state_dict(), model_path)\n",
    "        else:\n",
    "            epochs_no_improve += 1\n",
    "            if epochs_no_improve >= patience:\n",
    "                print('Early stopping!')\n",
    "                loss_val = loss_val[:epoch+1]\n",
    "                acc_val = acc_val[:epoch+1]\n",
    "                loss_train = loss_train[:epoch+1]\n",
    "                acc_train = acc_train[:epoch+1]\n",
    "                early_stop = True\n",
    "                break\n",
    "\n",
    "        # Save model when accuracy beats best accuracy\n",
    "        if acc_val[epoch] > best_accuracy:\n",
    "            best_accuracy = acc_val[epoch]\n",
    "            # Save the model state and training history\n",
    "            torch.save({\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'loss_train': loss_train,\n",
    "                'loss_val': loss_val,\n",
    "                'acc_train': acc_train,\n",
    "                'acc_val': acc_val,\n",
    "                'best_accuracy': best_accuracy\n",
    "            }, model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(14,5))\n",
    "plt.subplot(1, 2, 1), plt.plot(loss_train, 'b'), plt.plot(loss_val, 'r'), plt.legend(['train', 'val']), plt.title('Cross-entropy loss')\n",
    "plt.subplot(1, 2, 2), plt.plot(acc_train, 'b'), plt.plot(acc_val, 'r'), plt.legend(['train', 'val']), plt.title('Accuracy')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color='teal'> 5.4. Dropout </font>\n",
    "\n",
    "Dropout is a regularization technique designed to prevent overfitting in neural networks. It works by randomly deactivating a subset of neurons during each forward pass in training. This forces the network to distribute its learning across a wider set of features, rather than relying on a few specific neurons.\n",
    "\n",
    "**Key parameters:**\n",
    "- **Dropout Rate (`p`)**: Defines the proportion of neurons to deactivate (e.g., 20%, 50%).\n",
    "- **Training vs. Inference**:\n",
    "  - During **training**, neurons are randomly dropped.\n",
    "  - During **inference**, dropout is disabled, and weights are scaled (multiplied by 1-`p`) to account for the inherent scaling during training's dropout.\n",
    "\n",
    "**Steps to carry out dropout**:\n",
    "1. A dropout mask is applied to the activations of a layer, randomly setting a fraction of them to zero.\n",
    "2. The unaffected activations remain unchanged, allowing the model to continue training on the remaining subset.\n",
    "\n",
    "This process ensures the model does not rely too heavily on any single neuron, promoting robust and generalized learning.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedforwardNeuralNetModel(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim, hidden_dim= (200, 50, 20), initialize_weights=False, p=0.):\n",
    "        super(FeedforwardNeuralNetModel, self).__init__()\n",
    "\n",
    "        sizes = [input_dim] + list(map(int, hidden_dim)) + [output_dim]\n",
    "        layers = []\n",
    "        for i in range(len(sizes) - 2):\n",
    "            layers.append(nn.Linear(sizes[i], sizes[i + 1]))\n",
    "            if initialize_weights:\n",
    "                # Initialize weights with Xavier initialization\n",
    "                torch.nn.init.kaiming_uniform_(layers[-1].weight)\n",
    "            layers.append(nn.ReLU())\n",
    "            layers.append(nn.Dropout(p=p))\n",
    "        layers.append(nn.Linear(sizes[-2], sizes[-1]))\n",
    "        self.net = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):        \n",
    "        out = self.net(x)    \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dim = train_loader.dataset.tensors[0].shape[1]\n",
    "hidden_dim = (200, 50, 20)\n",
    "output_dim = train_loader.dataset.tensors[1].shape[1] if dataset == 'digits' else 1\n",
    "\n",
    "p = 0.5\n",
    "# Instantiate model class and assign to object\n",
    "model = FeedforwardNeuralNetModel(input_dim, output_dim, hidden_dim, p = p)\n",
    "\n",
    "# Push model to CUDA device if available\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "# Loss function\n",
    "loss_func = nn.BCEWithLogitsLoss() if dataset == 'DogsCats' else nn.CrossEntropyLoss()\n",
    "\n",
    "# Optimizer\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 100\n",
    "best_accuracy = 0\n",
    "loss_train = np.zeros(num_epochs)\n",
    "loss_val = np.zeros(num_epochs)\n",
    "acc_train = np.zeros(num_epochs)\n",
    "acc_val = np.zeros(num_epochs)\n",
    "\n",
    "# Check if the best model exists\n",
    "reg_technique = 'dropout'\n",
    "pretrained = False\n",
    "hidden_dim_str = '_'.join(map(str, hidden_dim))\n",
    "model_path = f'models/best_model_{dataset}_{reg_technique}_{p}_N{int(X_train_torch.shape[0])}_hidden{hidden_dim_str}.pth'\n",
    "if os.path.exists(model_path) and pretrained:\n",
    "    print(\"Loading best model and training history...\")\n",
    "    checkpoint = torch.load(model_path)\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    loss_train = checkpoint['loss_train']\n",
    "    loss_val = checkpoint['loss_val']\n",
    "    acc_train = checkpoint['acc_train']\n",
    "    acc_val = checkpoint['acc_val']\n",
    "    best_accuracy = checkpoint['best_accuracy']\n",
    "else:\n",
    "    print(\"Best model not found. Running optimization...\")\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "\n",
    "        for xb, yb in train_loader:\n",
    "            # Ensure the input tensors are of type float32\n",
    "            xb = xb.float()\n",
    "            yb = yb.reshape(-1, 1).float() if dataset == 'DogsCats' else yb.argmax(axis=-1)\n",
    "\n",
    "            #Compute network output and cross-entropy loss for current minibatch\n",
    "            pred = model(xb)\n",
    "            loss = loss_func(pred, yb)\n",
    "\n",
    "            #Compute gradients and optimize parameters\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "                \n",
    "        #At the end of each epoch, evaluate overall network performance\n",
    "        with torch.no_grad():\n",
    "            #Computing network performance after iteration\n",
    "            pred = model(X_train_torch.float())\n",
    "            loss_train[epoch] = loss_func(pred, y_train_torch.reshape(-1, 1).float()).item() if dataset == 'DogsCats' else loss_func(pred, y_train_torch.argmax(axis=-1)).item()\n",
    "            acc_train[epoch] = accuracy(pred, y_train_torch.reshape(-1, 1).float()).item() if dataset == 'DogsCats' else accuracy(pred, y_train_torch).item()\n",
    "            pred_val = model(X_val_torch.float())\n",
    "            loss_val[epoch] = loss_func(pred_val, y_val_torch.reshape(-1, 1).float()).item() if dataset == 'DogsCats' else loss_func(pred_val, y_val_torch.argmax(axis=-1)).item()\n",
    "            acc_val[epoch] = accuracy(pred_val, y_val_torch.reshape(-1, 1).float()).item() if dataset == 'DogsCats' else accuracy(pred_val, y_val_torch).item()\n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss_train[epoch]:.4f}, Accuracy: {acc_train[epoch]:.4f}, Val Loss: {loss_val[epoch]:.4f}, Val Accuracy: {acc_val[epoch]:.4f}')\n",
    "        \n",
    "        # Save model when accuracy beats best accuracy\n",
    "        if acc_val[epoch] > best_accuracy:\n",
    "            best_accuracy = acc_val[epoch]\n",
    "            # Save the model state and training history\n",
    "            torch.save({\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'loss_train': loss_train,\n",
    "                'loss_val': loss_val,\n",
    "                'acc_train': acc_train,\n",
    "                'acc_val': acc_val,\n",
    "                'best_accuracy': best_accuracy\n",
    "            }, model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(14,5))\n",
    "plt.subplot(1, 2, 1), plt.plot(loss_train, 'b'), plt.plot(loss_val, 'r'), plt.legend(['train', 'val']), plt.title('Cross-entropy loss')\n",
    "plt.subplot(1, 2, 2), plt.plot(acc_train, 'b'), plt.plot(acc_val, 'r'), plt.legend(['train', 'val']), plt.title('Accuracy')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color='teal'> 6. Indirect regularisation </font>\n",
    "\n",
    "Here, we will present techniques that are not explicitly designed as regularization techniques but have a similar impact by improving generalization as a side effect.\n",
    "\n",
    "### <font color='teal'> 6.1. Batch Normalisation </font>\n",
    "Batch Normalization is a technique to stabilize and accelerate the training of deep neural networks by normalizing the inputs to each layer. It ensures that the inputs to a layer have a consistent distribution throughout training, which can help mitigate issues caused by internal covariate shift.\n",
    "\n",
    "**Benefits of Batch Normalization**\n",
    "1. **Faster Convergence**: Normalized inputs allow the network to learn faster, often reducing the number of epochs required.\n",
    "2. **Higher Learning Rates**: Helps the model tolerate larger learning rates without divergence.\n",
    "3. **Regularization**: Introduces noise through mini-batch statistics, reducing the dependency on dropout or other regularization techniques.\n",
    "4. **Reduced Internal Covariate Shift**: Stabilizes the distribution of activations, making deeper networks easier to train."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedforwardNeuralNetModel(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim, hidden_dim= (200, 50, 20), initialize_weights=False, p=0., use_batchnorm=False):\n",
    "        super(FeedforwardNeuralNetModel, self).__init__()\n",
    "\n",
    "        sizes = [input_dim] + list(map(int, hidden_dim)) + [output_dim]\n",
    "        layers = []\n",
    "        for i in range(len(sizes) - 2):\n",
    "            layers.append(nn.Linear(sizes[i], sizes[i + 1]))\n",
    "            if initialize_weights:\n",
    "                torch.nn.init.kaiming_uniform_(layers[-1].weight)\n",
    "            if use_batchnorm:\n",
    "                layers.append(nn.BatchNorm1d(sizes[i + 1]))\n",
    "            layers.append(nn.ReLU())\n",
    "            layers.append(nn.Dropout(p=p))\n",
    "        layers.append(nn.Linear(sizes[-2], sizes[-1]))\n",
    "        self.net = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):        \n",
    "        out = self.net(x)    \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dim = train_loader.dataset.tensors[0].shape[1]\n",
    "hidden_dim = (200, 50, 20)\n",
    "output_dim = train_loader.dataset.tensors[1].shape[1] if dataset == 'digits' else 1\n",
    "\n",
    "# Instantiate model class and assign to object\n",
    "model = FeedforwardNeuralNetModel(input_dim, output_dim, hidden_dim, use_batchnorm=True)\n",
    "\n",
    "# Push model to CUDA device if available\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "# Loss function\n",
    "loss_func = nn.BCEWithLogitsLoss() if dataset == 'DogsCats' else nn.CrossEntropyLoss()\n",
    "\n",
    "# Optimizer\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 100\n",
    "best_accuracy = 0\n",
    "loss_train = np.zeros(num_epochs)\n",
    "loss_val = np.zeros(num_epochs)\n",
    "acc_train = np.zeros(num_epochs)\n",
    "acc_val = np.zeros(num_epochs)\n",
    "\n",
    "# Check if the best model exists\n",
    "reg_technique = 'batchnorm'\n",
    "pretrained = False\n",
    "hidden_dim_str = '_'.join(map(str, hidden_dim))\n",
    "model_path = f'models/best_model_{dataset}_{reg_technique}_N{int(X_train_torch.shape[0])}_hidden{hidden_dim_str}.pth'\n",
    "if os.path.exists(model_path) and pretrained:\n",
    "    print(\"Loading best model and training history...\")\n",
    "    checkpoint = torch.load(model_path)\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    loss_train = checkpoint['loss_train']\n",
    "    loss_val = checkpoint['loss_val']\n",
    "    acc_train = checkpoint['acc_train']\n",
    "    acc_val = checkpoint['acc_val']\n",
    "    best_accuracy = checkpoint['best_accuracy']\n",
    "else:\n",
    "    print(\"Best model not found. Running optimization...\")\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "\n",
    "        for xb, yb in train_loader:\n",
    "            # Ensure the input tensors are of type float32\n",
    "            xb = xb.float()\n",
    "            yb = yb.reshape(-1, 1).float() if dataset == 'DogsCats' else yb.argmax(axis=-1)\n",
    "\n",
    "            #Compute network output and cross-entropy loss for current minibatch\n",
    "            pred = model(xb)\n",
    "            loss = loss_func(pred, yb)\n",
    "\n",
    "            #Compute gradients and optimize parameters\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "                \n",
    "        #At the end of each epoch, evaluate overall network performance\n",
    "        with torch.no_grad():\n",
    "            #Computing network performance after iteration\n",
    "            pred = model(X_train_torch.float())\n",
    "            loss_train[epoch] = loss_func(pred, y_train_torch.reshape(-1, 1).float()).item() if dataset == 'DogsCats' else loss_func(pred, y_train_torch.argmax(axis=-1)).item()\n",
    "            acc_train[epoch] = accuracy(pred, y_train_torch.reshape(-1, 1).float()).item() if dataset == 'DogsCats' else accuracy(pred, y_train_torch).item()\n",
    "            pred_val = model(X_val_torch.float())\n",
    "            loss_val[epoch] = loss_func(pred_val, y_val_torch.reshape(-1, 1).float()).item() if dataset == 'DogsCats' else loss_func(pred_val, y_val_torch.argmax(axis=-1)).item()\n",
    "            acc_val[epoch] = accuracy(pred_val, y_val_torch.reshape(-1, 1).float()).item() if dataset == 'DogsCats' else accuracy(pred_val, y_val_torch).item()\n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss_train[epoch]:.4f}, Accuracy: {acc_train[epoch]:.4f}, Val Loss: {loss_val[epoch]:.4f}, Val Accuracy: {acc_val[epoch]:.4f}')\n",
    "        \n",
    "        # Save model when accuracy beats best accuracy\n",
    "        if acc_val[epoch] > best_accuracy:\n",
    "            best_accuracy = acc_val[epoch]\n",
    "            # Save the model state and training history\n",
    "            torch.save({\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'loss_train': loss_train,\n",
    "                'loss_val': loss_val,\n",
    "                'acc_train': acc_train,\n",
    "                'acc_val': acc_val,\n",
    "                'best_accuracy': best_accuracy\n",
    "            }, model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(14,5))\n",
    "plt.subplot(1, 2, 1), plt.plot(loss_train, 'b'), plt.plot(loss_val, 'r'), plt.legend(['train', 'val']), plt.title('Cross-entropy loss')\n",
    "plt.subplot(1, 2, 2), plt.plot(acc_train, 'b'), plt.plot(acc_val, 'r'), plt.legend(['train', 'val']), plt.title('Accuracy')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color='teal'> 6.2. Data augmentation </font>\n",
    "Data augmentation is a technique used to artificially increase the size and diversity of a training dataset by applying transformations or perturbations to the existing data. It is commonly used in computer vision and natural language processing, to improve model generalization and reduce overfitting.\n",
    "\n",
    "**Benefits of data augmentation**\n",
    "1. **Improves Generalization**: Models trained on augmented data are exposed to diverse variations of the input, making them more robust to unseen data.\n",
    "2. **Mitigates Overfitting**: By artificially enlarging the dataset, the model learns from more examples, reducing its tendency to memorize the training set.\n",
    "3. **Increases Data Size**: Augmented datasets effectively provide more samples without collecting new data, which is beneficial for tasks with limited training data.\n",
    "\n",
    "**Common Data Augmentation Techniques**\n",
    "- **For Images:**\n",
    "1. **Geometric Transformations**: Rotation, flipping, cropping, scaling, and translation.\n",
    "2. **Color Transformations**: Adjust brightness, contrast, saturation, and hue.\n",
    "3. **Noise Injection**: Add Gaussian noise or salt-and-pepper noise to simulate real-world imperfections.\n",
    "4. **Random Erasing**: Randomly erase a portion of the image to mimic occlusions.\n",
    "- **For Text:**\n",
    "1. **Synonym Replacement**: Replace words with their synonyms to introduce variations.\n",
    "2. **Back Translation**: Translate text to another language and back to introduce subtle paraphrasing.\n",
    "3. **Word Deletion or Shuffling**: Randomly remove or shuffle words in a sentence.\n",
    "- **For Time-Series or Audio:**\n",
    "1. **Time Stretching/Compression**: Speed up or slow down the audio signal.\n",
    "2. **Pitch Shifting**: Shift the frequency of the audio.\n",
    "3. **Adding Noise**: Overlay random noise to the signal.\n",
    "\n",
    "Let's start by generating new samples with rotations and brightness and contrast."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset, ConcatDataset\n",
    "import torchvision.transforms as transforms\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Define the data augmentation transformations\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToPILImage(),\n",
    "    transforms.ColorJitter(),\n",
    "    transforms.RandomRotation(10),\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "# Create a function to generate augmented samples\n",
    "def create_augmented_samples(X, y, transform, num_augmentations=1):\n",
    "    augmented_samples = []\n",
    "    augmented_labels = []\n",
    "    for i in range(len(X)):\n",
    "        original = X[i]\n",
    "        label = y[i]  # Keep the label as is (one-hot encoded)\n",
    "        for _ in range(num_augmentations):\n",
    "            augmented_sample = transform(original)\n",
    "            augmented_samples.append(augmented_sample)\n",
    "            augmented_labels.append(label)  # Append the one-hot encoded label\n",
    "    return torch.stack(augmented_samples), torch.stack(augmented_labels)\n",
    "\n",
    "# Original dataset (reshaped for transformation compatibility)\n",
    "X_train_reshaped = X_train_torch.reshape(-1, img_size, img_size)\n",
    "y_train = y_train_torch\n",
    "\n",
    "# Generate augmented samples (e.g., 2 augmentations per original sample)\n",
    "num_augmentations = 1\n",
    "aug_X, aug_y = create_augmented_samples(X_train_reshaped, y_train, transform, num_augmentations=num_augmentations)\n",
    "\n",
    "# Combine original and augmented datasets\n",
    "augmented_X = torch.cat([X_train_reshaped.unsqueeze(1), aug_X])\n",
    "augmented_y = torch.cat([y_train, aug_y])  # Combine the one-hot encoded labels\n",
    "\n",
    "# Create a new DataLoader for the combined dataset\n",
    "augmented_dataset = TensorDataset(augmented_X, augmented_y)\n",
    "train_loader_augmented = DataLoader(augmented_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# Verify the augmented dataset by visualizing some samples\n",
    "images, labels = next(iter(train_loader_augmented))\n",
    "\n",
    "def show_images(images, labels, n=10):\n",
    "    plt.figure(figsize=(15, 5))\n",
    "    for i in range(n):\n",
    "        ax = plt.subplot(1, n, i + 1)\n",
    "        plt.imshow(images[i].squeeze().numpy(), cmap=\"gray\")\n",
    "        plt.title(labels[i].argmax().item())\n",
    "        plt.axis(\"off\")\n",
    "    plt.show()\n",
    "\n",
    "# Get a batch of augmented images\n",
    "images, labels = next(iter(train_loader_augmented))\n",
    "show_images(images, labels)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The visualisation shows that there are some slight changes in the values of our images. Note that converting to and from PIL may introduce small interpolation effects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dim = train_loader.dataset.tensors[0].shape[1]\n",
    "hidden_dim = (200, 50, 20)\n",
    "output_dim = train_loader.dataset.tensors[1].shape[1] if dataset == 'digits' else 1\n",
    "\n",
    "# Instantiate model class and assign to object\n",
    "model = FeedforwardNeuralNetModel(input_dim, output_dim, hidden_dim)\n",
    "\n",
    "# Push model to CUDA device if available\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "# Loss function\n",
    "loss_func = nn.BCEWithLogitsLoss() if dataset == 'DogsCats' else nn.CrossEntropyLoss()\n",
    "\n",
    "# Optimizer\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 100\n",
    "best_accuracy = 0\n",
    "loss_train = np.zeros(num_epochs)\n",
    "loss_val = np.zeros(num_epochs)\n",
    "acc_train = np.zeros(num_epochs)\n",
    "acc_val = np.zeros(num_epochs)\n",
    "\n",
    "# Check if the best model exists\n",
    "reg_technique = 'batchnorm'\n",
    "pretrained = False\n",
    "hidden_dim_str = '_'.join(map(str, hidden_dim))\n",
    "model_path = f'models/best_model_{dataset}_{reg_technique}_N{int(X_train_torch.shape[0])}_hidden{hidden_dim_str}.pth'\n",
    "if os.path.exists(model_path) and pretrained:\n",
    "    print(\"Loading best model and training history...\")\n",
    "    checkpoint = torch.load(model_path)\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    loss_train = checkpoint['loss_train']\n",
    "    loss_val = checkpoint['loss_val']\n",
    "    acc_train = checkpoint['acc_train']\n",
    "    acc_val = checkpoint['acc_val']\n",
    "    best_accuracy = checkpoint['best_accuracy']\n",
    "else:\n",
    "    print(\"Best model not found. Running optimization...\")\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "\n",
    "        for xb, yb in train_loader_augmented:\n",
    "            # Ensure the input tensors are of type float32\n",
    "            xb = xb.float().reshape(-1, img_size * img_size)\n",
    "            yb = yb.reshape(-1, 1).float() if dataset == 'DogsCats' else yb.argmax(axis=-1)\n",
    "\n",
    "            #Compute network output and cross-entropy loss for current minibatch\n",
    "            pred = model(xb)\n",
    "            loss = loss_func(pred, yb)\n",
    "\n",
    "            #Compute gradients and optimize parameters\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "                \n",
    "        #At the end of each epoch, evaluate overall network performance\n",
    "        with torch.no_grad():\n",
    "            #Computing network performance after iteration\n",
    "            pred = model(X_train_torch.float())\n",
    "            loss_train[epoch] = loss_func(pred, y_train_torch.reshape(-1, 1).float()).item() if dataset == 'DogsCats' else loss_func(pred, y_train_torch.argmax(axis=-1)).item()\n",
    "            acc_train[epoch] = accuracy(pred, y_train_torch.reshape(-1, 1).float()).item() if dataset == 'DogsCats' else accuracy(pred, y_train_torch).item()\n",
    "            pred_val = model(X_val_torch.float())\n",
    "            loss_val[epoch] = loss_func(pred_val, y_val_torch.reshape(-1, 1).float()).item() if dataset == 'DogsCats' else loss_func(pred_val, y_val_torch.argmax(axis=-1)).item()\n",
    "            acc_val[epoch] = accuracy(pred_val, y_val_torch.reshape(-1, 1).float()).item() if dataset == 'DogsCats' else accuracy(pred_val, y_val_torch).item()\n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss_train[epoch]:.4f}, Accuracy: {acc_train[epoch]:.4f}, Val Loss: {loss_val[epoch]:.4f}, Val Accuracy: {acc_val[epoch]:.4f}')\n",
    "        \n",
    "        # Save model when accuracy beats best accuracy\n",
    "        if acc_val[epoch] > best_accuracy:\n",
    "            best_accuracy = acc_val[epoch]\n",
    "            # Save the model state and training history\n",
    "            torch.save({\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'loss_train': loss_train,\n",
    "                'loss_val': loss_val,\n",
    "                'acc_train': acc_train,\n",
    "                'acc_val': acc_val,\n",
    "                'best_accuracy': best_accuracy\n",
    "            }, model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(14,5))\n",
    "plt.subplot(1, 2, 1), plt.plot(loss_train, 'b'), plt.plot(loss_val, 'r'), plt.legend(['train', 'val']), plt.title('Cross-entropy loss')\n",
    "plt.subplot(1, 2, 2), plt.plot(acc_train, 'b'), plt.plot(acc_val, 'r'), plt.legend(['train', 'val']), plt.title('Accuracy')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that you have learned how to implement regularization techniques in PyTorch, you can experiment with different regularization techniques and hyperparameters to improve the performance of your neural network models. You can also try different datasets and network architectures to see how regularization techniques affect the training and generalization of deep learning models."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
