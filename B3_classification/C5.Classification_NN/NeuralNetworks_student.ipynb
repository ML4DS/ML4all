{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wgJ-CjGp8wr6",
    "tags": []
   },
   "source": [
    "# <font color='teal'> Introduction to Neural Networks and Pytorch </font>\n",
    "\n",
    "    Notebook version: 0.4. (Nov 4, 2024)\n",
    "\n",
    "    Authors: Jerónimo Arenas García (jarenas@ing.uc3m.es)\n",
    "             Jesús Cid-Sueiro (jcid@tsc.uc3m.es)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1699270515157,
     "user": {
      "displayName": "JESUS CID SUEIRO",
      "userId": "07576244288315618853"
     },
     "user_tz": -60
    },
    "id": "EO_PH68t8wr_",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# from IPython.core.display import HTML\n",
    "# HTML(\"\"\"\n",
    "# <style>\n",
    "# body {\n",
    "#   counter-reset: section subsection;\n",
    "# }\n",
    "# h2 {\n",
    "#   counter-reset: subsection;\n",
    "# }\n",
    "# h2:before {\n",
    "#     counter-increment: section;\n",
    "#     content: \"Section \" counter(section) \". \";\n",
    "# }\n",
    "# h3:before {\n",
    "#     counter-increment: subsection;\n",
    "#     content: counter(section) \".\" counter(subsection) \" \";\n",
    "# }\n",
    "# </style>\n",
    "# \"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XNNG8b-M8wsB",
    "tags": []
   },
   "source": [
    "    Changes: v.0.1. (Nov 14, 2020) - First version\n",
    "             v.0.2. (Nov 5, 2021) - Structuring code, revisiting formulation\n",
    "             v.0.3. (Nov, 1, 2022) - Revisiting text.\n",
    "             v.0.4. (Nov, 4, 2024) - General notebook updates\n",
    "             \n",
    "    Pending changes:\n",
    "        Add an example with dropout\n",
    "        Add theory about CNNs\n",
    "        Define some functions to simplify code cells"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1699270515157,
     "user": {
      "displayName": "JESUS CID SUEIRO",
      "userId": "07576244288315618853"
     },
     "user_tz": -60
    },
    "id": "Yx2HPTNQ8wsC"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "size = 14\n",
    "params = {'legend.fontsize': 'Large',\n",
    "          'axes.labelsize': size,\n",
    "          'axes.titlesize': size,\n",
    "          'xtick.labelsize': size*0.75,\n",
    "          'ytick.labelsize': size*0.75}\n",
    "plt.rcParams.update(params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3crlnxV18wsD"
   },
   "source": [
    "## <font color='teal'> 1. Introduction and purpose of this Notebook </font>\n",
    "\n",
    "### <font color='teal'> 1.1. About Neural Networks </font>\n",
    "\n",
    "* Neural Networks (NN) have become the state of the art for many machine learning problems\n",
    "    * Natural Language Processing\n",
    "    * Computer Vision\n",
    "    * Image Recognition\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CJjNeFoB8wsD"
   },
   "source": [
    "* They are in widespread use for many applications, e.g.,\n",
    "    * Language translation\n",
    "    * Automatic speech recognition (<a href=\"https://machinelearning.apple.com/research/hey-siri\">Hey Siri!</a> DNN overview)\n",
    "    * Autonomous navigation (<a href=\"https://venturebeat.com/2020/04/13/facebooks-ai-teaches-robots-to-navigate-environments-using-less-data/\">Facebook Robot Autonomous 3D Navigation</a>)\n",
    "    * Automatic plate recognition\n",
    "    \n",
    "<center><img src=\"figures/ComputerVision.png\" /></center>\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DEumljEN8wsE",
    "tags": []
   },
   "source": [
    "Feed Forward Neural Networks [have been around since 1960](https://www.skynettoday.com/overviews/neural-net-history) but only recently (last 10-15 years) have they met their expectations, and improve other machine learning algorithms\n",
    "\n",
    "* Computation resources are now available at large scale\n",
    "* Cloud Computing (AWS, Azure)\n",
    "* From MultiLayer Perceptrons to Deep Learning\n",
    "* Big Data sets\n",
    "* This has also made possible an intense research effort resulting in\n",
    "    * Topologies better suited to particular problems (CNNs, RNNs, Transformers)\n",
    "    * New training strategies providing better generalization\n",
    "\n",
    "In parallel, Deep Learning Platforms have emerged that make design, implementation, training, and production of DNNs feasible for everyone"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "n04h2qC-8wsE"
   },
   "source": [
    "### <font color='teal'> 1.2. Scope of this notebook</font>\n",
    "\n",
    "* To provide just overview of most important NNs and DNNs concepts\n",
    "* Connecting with already studied methods as starting point (mainly logistic regression)\n",
    "* Introduction to PyTorch\n",
    "* Providing links to external sources for further study\n",
    "* Set the basis for learning about moder topologies for Natural Language Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bc71E1it8wsF"
   },
   "source": [
    "### <font color='teal'> 1.3. Outline</font>\n",
    "\n",
    "1. Introduction and purpose of this Notebook\n",
    "2. Introduction to Neural Networks\n",
    "3. Implementing Deep Networks with PyTorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hHvvnsaL8wsF"
   },
   "source": [
    "### <font color='teal'> 1.4. Other resources </font>\n",
    "\n",
    "* We point here to external resources and tutorials that are excellent material for further study of the topic\n",
    "* Most of them include examples and exercises using numpy and PyTorch\n",
    "* This notebook uses examples and other material from some of these sources"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ak-Ctg9q8wsG"
   },
   "source": [
    "|Tutorial|Description|\n",
    "|-----|---------------------|\n",
    "|<a href=\"https://www.simplilearn.com/tutorials/deep-learning-tutorial\"> <img src=\"figures/simplilearn.png\" width=\"100\"/> </a>|Very general tutorial including videos and an overview of top deep learning platforms|\n",
    "|<a href=\"http://d2l.ai/\"> <img src=\"figures/dl2ai.png\" width=\"100\"/> </a>|Very complete book with a lot of theory and examples for MxNET, PyTorch, and TensorFlow|\n",
    "|<a href=\"https://pytorch.org/tutorials/\"> <img src=\"figures/PyTorch.png\" width=\"100\"/> </a>|Official tutorials from the PyTorch project. Contains a 60 min overview, and a very practical *learning PyTorch with examples* tutorial|\n",
    "|<a href=\"https://www.kaggle.com/kanncaa1/deep-learning-tutorial-for-beginners\"> <img src=\"figures/kaggle.png\" width=\"100\"/> </a>|Kaggle tutorials covering an introduction to Neural Networks using Numpy, and a second one offering a PyTorch tutorial|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DZBGXAo98wsG"
   },
   "source": [
    "In addition to this, PyTorch MOOCs can be followed for free in main sites: edX, Coursera, Udacity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <font color='olive'>Preliminary work</font>\n",
    "\n",
    "Complete the following tutorials for basic knowledge of pyTorch\n",
    "\n",
    "   - [Introduction to PyTorch tensors](https://pytorch.org/tutorials/beginner/basics/tensorqs_tutorial.html)\n",
    "   - [Introduction to automatic differentiation with PyTorch](https://pytorch.org/tutorials/beginner/basics/autogradqs_tutorial.html)\n",
    "   - [PyTorch tutorial available in Google Colab](https://colab.research.google.com/github/phlippe/uvadlc_notebooks/blob/master/docs/tutorial_notebooks/tutorial2/Introduction_to_PyTorch.ipynb) (Complete just until de XOR example)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_pYUwClN8wsG"
   },
   "source": [
    "## <font color='teal'> 2. Datasets </font>\n",
    "\n",
    "Along this notebook, we will run some experiments to solve classification problems using two image datasets, that we name \"digits\" and \"DogCats\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HHOW9fT98wsH"
   },
   "source": [
    "### <font color='olive'>Digits: a sign language digits data set</font>\n",
    "\n",
    "* Dataset is taken from <a href=\"https://www.kaggle.com/ardamavi/sign-language-digits-dataset\"> Kaggle</a> and used in the above referred tutorial\n",
    "* 2062 digits in sign language. $64 \\times 64$ images\n",
    "* Problem with 10 classes. One hot encoding for the label matrix\n",
    "* Input data are images, we create also a flattened version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 422
    },
    "executionInfo": {
     "elapsed": 7,
     "status": "error",
     "timestamp": 1699033154379,
     "user": {
      "displayName": "JESUS CID SUEIRO",
      "userId": "07576244288315618853"
     },
     "user_tz": -60
    },
    "id": "EgfcqqzP8wsH",
    "outputId": "7f226680-583b-4609-b615-2426981ab1bc"
   },
   "outputs": [],
   "source": [
    "# Load images and labels\n",
    "digitsX = np.load('./data/Sign-language-digits-dataset/X.npy')\n",
    "digitsY = np.load('./data/Sign-language-digits-dataset/Y.npy')\n",
    "\n",
    "# Flatten images (to get 1-dimensional inputs\n",
    "K = digitsX.shape[0]\n",
    "img_size = digitsX.shape[1]\n",
    "digitsX_flatten = digitsX.reshape(K,img_size*img_size)\n",
    "\n",
    "print('Size of Input Data Matrix:', digitsX.shape)\n",
    "print('Size of Flattened Input Data Matrix:', digitsX_flatten.shape)\n",
    "print('Size of label Data Matrix:', digitsY.shape)\n",
    "\n",
    "# Show sample images\n",
    "selected = [260, 1400]\n",
    "plt.subplot(1, 2, 1), plt.imshow(digitsX[selected[0]].reshape(img_size, img_size)), plt.axis('off')\n",
    "plt.subplot(1, 2, 2), plt.imshow(digitsX[selected[1]].reshape(img_size, img_size)), plt.axis('off')\n",
    "plt.show()\n",
    "print('Labels corresponding to figures:', digitsY[selected,])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9ziksh818wsI",
    "tags": []
   },
   "source": [
    "### <font color='olive'> DogCats: a dataset of dogs and cat images </font>\n",
    "\n",
    "* Dataset is taken from <a href=\"https://www.kaggle.com/c/dogs-vs-cats\"> Kaggle</a>\n",
    "* 25000 pictures of dogs and cats\n",
    "* Binary problem\n",
    "* Input data are images, we create also a flattened version\n",
    "* Original images are RGB, and arbitrary size\n",
    "* Preprocessed images are $64 \\times 64$ and gray scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nILd2Jx28wsI"
   },
   "outputs": [],
   "source": [
    "# Preprocessing of original Dogs and Cats Pictures\n",
    "# Adapted from\n",
    "# https://medium.com/@mrgarg.rajat/kaggle-dogs-vs-cats-challenge-complete-step-by-step-guide-part-1-a347194e55b1\n",
    "# RGB channels are collapsed in GRAYSCALE\n",
    "# Images are resampled to 64x64\n",
    "# This code has been used to generate the adapted dataset used in this notebook, that is stored in\n",
    "# ./data/DogsCats/ .\n",
    "# You can uncomment this code to re-generate the dataset, if needed.\n",
    "\"\"\"\n",
    "import os, cv2  # cv2 -- OpenCV\n",
    "\n",
    "train_dir = './data/DogsCats/train/'\n",
    "rows, cols = 64, 64\n",
    "train_images = sorted([train_dir+i for i in os.listdir(train_dir)])\n",
    "\n",
    "def read_image(file_path):\n",
    "    image = cv2.imread(file_path, cv2.IMREAD_GRAYSCALE)\n",
    "    return cv2.resize(image, (rows, cols),interpolation=cv2.INTER_CUBIC)\n",
    "\n",
    "def prep_data(images):\n",
    "    m = len(images)\n",
    "    X = np.ndarray((m, rows, cols), dtype=np.uint8)\n",
    "    y = np.zeros((m,))\n",
    "    print(\"X.shape is {}\".format(X.shape))\n",
    "\n",
    "    for i,image_file in enumerate(images) :\n",
    "        image = read_image(image_file)\n",
    "        X[i,] = np.squeeze(image.reshape((rows, cols)))\n",
    "        if 'dog' in image_file.split('/')[-1].lower():\n",
    "            y[i] = 1\n",
    "        elif 'cat' in image_file.split('/')[-1].lower():\n",
    "            y[i] = 0\n",
    "\n",
    "        if i%5000 == 0 :\n",
    "            print(f\"Proceed {i} of {m}\")\n",
    "\n",
    "    return X,y\n",
    "\n",
    "X_train, y_train = prep_data(train_images)\n",
    "np.save(X.npy', X_train)\n",
    "np.save('./data/DogsCats/Y.npy', y_train)\n",
    "\"\"\"\n",
    "None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vbeAIxVF8wsJ",
    "outputId": "94086654-8fa6-4959-93ce-1437a558d83f"
   },
   "outputs": [],
   "source": [
    "# Load images and labels\n",
    "DogsCatsX = np.load('./data/DogsCats/X.npy')\n",
    "DogsCatsY = np.load('./data/DogsCats/Y.npy')\n",
    "\n",
    "# Flatten images to get 1D inputs\n",
    "K = DogsCatsX.shape[0]\n",
    "img_size = DogsCatsX.shape[1]\n",
    "DogsCatsX_flatten = DogsCatsX.reshape(K,img_size*img_size)\n",
    "\n",
    "print('Size of Input Data Matrix:', DogsCatsX.shape)\n",
    "print('Size of Flattened Input Data Matrix:', DogsCatsX_flatten.shape)\n",
    "print('Size of label Data Matrix:', DogsCatsY.shape)\n",
    "\n",
    "# Show sample images\n",
    "selected = [260, 16000]\n",
    "plt.subplot(1, 2, 1), plt.imshow(DogsCatsX[selected[0]].reshape(img_size, img_size)), plt.axis('off')\n",
    "plt.subplot(1, 2, 2), plt.imshow(DogsCatsX[selected[1]].reshape(img_size, img_size)), plt.axis('off')\n",
    "plt.show()\n",
    "print('Labels corresponding to figures:', DogsCatsY[selected,])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hl51B6O08wsJ",
    "tags": []
   },
   "source": [
    "Now we define a function that, given the dataset name, prepares the data for binary or multiclass classification. The data are normalized and split into two sets for training and validation. This method will be used later to select the appropriate datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ARdERJgb8wsJ",
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def get_dataset(dataset_name, forze_binary=False):\n",
    "    \"\"\"\n",
    "    Loads the selected dataset, among two options: DogsCats or digits.\n",
    "\n",
    "    If dataset_name == 'digits', you can take a dataset with two classes only,\n",
    "    using forze_binary == True\n",
    "    \"\"\"\n",
    "\n",
    "    if dataset_name == 'DogsCats':\n",
    "        X = DogsCatsX_flatten\n",
    "        y = DogsCatsY\n",
    "    elif dataset_name == 'digits':\n",
    "        if forze_binary:\n",
    "            # Zero and Ones are one hot encoded in columns 1 and 4\n",
    "            X0 = digitsX_flatten[np.argmax(digitsY, axis=1)==1,]\n",
    "            X1 = digitsX_flatten[np.argmax(digitsY, axis=1)==4,]\n",
    "            X = np.vstack((X0, X1))\n",
    "            y = np.zeros(X.shape[0])\n",
    "            y[X0.shape[0]:] = 1\n",
    "        else:\n",
    "            X = digitsX_flatten\n",
    "            y = digitsY\n",
    "    else:\n",
    "        print(\"-- ERROR: Unknown dataset\")\n",
    "        return\n",
    "\n",
    "    # Joint normalization of all data. For images [-.5, .5] scaling is frequent\n",
    "    min_max_scaler = MinMaxScaler(feature_range=(-.5, .5))\n",
    "    X = min_max_scaler.fit_transform(X)\n",
    "\n",
    "    # Generate train and validation data, shuffle\n",
    "    X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42, shuffle=True)\n",
    "\n",
    "    return X_train, X_val, y_train, y_val"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NMHZkdfX8wsK"
   },
   "source": [
    "## <font color='teal'> 3. Introduction to Neural Networks </font>\n",
    "\n",
    "In this section, we will implement neural networks from scratch using Numpy arrays (i.e., no PyTorch will be used in this section)\n",
    "\n",
    "* No need to learn any new Python libraries\n",
    "* But we need to deal with complexity of multilayer networks\n",
    "* Low-level implementation will be useful to grasp the most important concepts concerning DNNs\n",
    "    * Back-propagation\n",
    "    * Activation functions\n",
    "    * Loss functions\n",
    "    * Optimization methods\n",
    "    * Generalization\n",
    "    * Special layers and configurations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NEGe9gzj8wsK",
    "tags": []
   },
   "source": [
    "### <font color='teal'> 3.1. A Single-Layer Neural Network for binary classification</font>\n",
    "\n",
    "#### <font color='teal'> 3.1.1. Architecture </font>\n",
    "\n",
    "One of the simplest neural network architectures for binary classification is shown in the figure\n",
    "\n",
    "<center><img src=\"figures/LR_network.png\" width=\"450\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-7qHLL3Z8wsK",
    "tags": []
   },
   "source": [
    "The main components are:\n",
    "\n",
    " *  A **linear combination** of the input features is computed to produce the intermediate output\n",
    "$$\n",
    "o = {\\bf w}^\\intercal {\\bf x} + b\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YMcHZl638wsK",
    "tags": []
   },
   "source": [
    " * An **activation function**, which maps the linear combination to values in a bounded range, to produce the *soft* prediction\n",
    "$$\n",
    "q = g(o)\n",
    "$$\n",
    "A common choice for binary classification is the logistic function, which provides probabilistic predictions $q\\in [0, 1]$,\n",
    "$$\n",
    "q = \\text{logistic}(o) = \\frac{1}{1 + \\exp(-o)}.\n",
    "$$\n",
    "However, other activation functions are possible."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E6O9mIrH8wsL",
    "tags": []
   },
   "source": [
    " * A **binary threshold**, transforming the *soft* prediction into a *hard* decision (the class prediction) in $\\{0, 1\\}$. Following the probabilistic interpretation of the soft prediction, a common choice is to apply a threshold $\\frac12$, so that\n",
    "$$\n",
    "\\hat{y} = \\left[\n",
    "    \\begin{array}{ll}\n",
    "    1, &  \\text{if } q \\ge \\frac12   \\\\\n",
    "    0, &  \\text{if } q < \\frac12\n",
    "    \\end{array} \\right.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fDIW7KEz8wsL",
    "tags": []
   },
   "source": [
    "We will define a ${\\tt forward}$ method to implement the computation of the soft prediction, $q$. To do so, we define a method to implement the logistic function, too."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "q-1OIxi78wsL",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define some useful functions\n",
    "def logistic(t):\n",
    "    \"\"\"\n",
    "    Computes the logistic function\n",
    "    \"\"\"\n",
    "    return 1.0 / (1 + np.exp(-t))\n",
    "\n",
    "def forward(w,b,x):\n",
    "    \"\"\"\n",
    "    Computes the network output\n",
    "    \"\"\"\n",
    "    # return logistic(x.dot(w) + b)\n",
    "    return logistic(x @ w + b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xc7089gJ8wsL",
    "tags": []
   },
   "source": [
    "For binary classification, our goal is to fit the weights so that the hard predictions are correct. Therefore, a natural measure of the classification performance is the accuracy, defined as the average number of correct decisions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JAdKGMtV8wsL",
    "tags": []
   },
   "outputs": [],
   "source": [
    "def accuracy(y, q):\n",
    "    return np.mean(y == (q >= 0.5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KdTyEpQm8wsM",
    "tags": []
   },
   "source": [
    "#### <font color='teal'> 3.1.2. Loss functions </font>\n",
    "\n",
    "The accuracy is a good measure for the evaluation of the classifiers, but it is not useful to define the learning algorithm. This is because learning algorithms for neural networs are mostly based on gradient-based optimization techniques. The thresholding function is not differentiable at $\\frac{1}{2}$ and its derivative is zero elsewhere. Therefore, the derivatives of the accuracy with respect to the weights are not useful to guide learning.\n",
    "\n",
    "For this reason, we need a **loss function**, that is, a measure of discrepancy between the true class, $y$, and the soft prediction $q$,\n",
    "$$\n",
    "\\ell(y, q)\n",
    "$$\n",
    "that could be used for training. A basic learning algorithm will try to minimize the **empirical risk**, defined as cumulative loss over the whole training set\n",
    "$$\n",
    "R({\\bf w}, b) = \\sum_{k=0}^{K-1} \\ell(y_k, q_k)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZECSnHuZ8wsM",
    "tags": []
   },
   "source": [
    "Many losses have been proposed for neural networks. Some examples are:\n",
    "* **Square error**: $\\ell_2(y, q) = (y-q)^2$\n",
    "* **Absolute error**: $\\ell_1(y, q) = |y - q|$\n",
    "* **Cross entropy**: $\\ell_\\text{CE}(y, q) = - y \\log(q) - (1-y) \\log(1-q)$\n",
    "\n",
    "For binary classification, cross entroy is the most common choice."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dwZS0E2A8wsM",
    "tags": []
   },
   "source": [
    "#### <font color='teal'> 3.1.3. Logistic Regression vs Single Layer NN</font>\n",
    "\n",
    "Any neural network with probabilistic soft decisions defines a parametric probability model of the data. For the single-layer NN, the parametric model will be\n",
    "$$\n",
    "P(y=1|{\\bf w}, b, {\\bf x}) = g({\\bf w}^\\intercal {\\bf x} + b)\n",
    "$$\n",
    "\n",
    "Therefore, we can train a neural network following a probabilistic approach. For instance, the negative log likelihood will be given by\n",
    "\\begin{align}\n",
    "\\text{NLL}({\\bf w}, b) &= - \\sum_{k=0}^{K-1} \\log(P(y_k|{\\bf w}, b, {\\bf x}))    \\\\\n",
    "    &= - \\sum_{k=0}^{K-1} \\left(y_k \\log(P(1|{\\bf w}, b, {\\bf x})) + (1-y_k) \\log( P(0|{\\bf w}, b, {\\bf x})) \\right)  \\\\\n",
    "    &= - \\sum_{k=0}^{K-1} \\left(y_k \\log(q_k) + (1-y_k) \\log(1-q_k) \\right)   \\\\\n",
    "    &= \\sum_{k=0}^{K-1} \\ell_\\text{CE}(y_k, q_k)\n",
    "\\end{align}\n",
    "which shows that the empirical risk of the cross entropy is the NLL and, thus, optimizing the cross entropy provides ML estimates of the weights.\n",
    "\n",
    "This also shows that a single-layer NN with logistic activation and cross-entropy loss is completely equivalent to a logistic regression model adjusted with ML."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ttzDlNSN8wsM",
    "tags": []
   },
   "source": [
    "#### <font color='teal'> 3.1.4. Training </font>\n",
    "\n",
    "In order to find parameters $\\bf w$ and $b$, we will minimize the NLL via gradient descent optimization.\n",
    "\n",
    "The gradient computation can be simplified using the **<font color='navy'>chain rule</font>**\n",
    "\n",
    "<br>\n",
    "\\begin{align}\n",
    "\\frac{\\partial \\text{NLL}}{\\partial {\\bf w}}\n",
    "    & = \\frac{\\partial \\text{NLL}}{\\partial q} \\cdot \\frac{\\partial q}{\\partial o}\n",
    "                                               \\cdot \\frac{\\partial o}{\\partial {\\bf w}} \\\\\n",
    "    & = \\sum_{k=0}^{K-1} \\left[\\frac{1-y_k}{1-q_k} - \\frac{y_k}{q_k}\\right]q_k (1-q_k) {\\bf x}_k \\\\\n",
    "    & = \\sum_{k=0}^{K-1} (q_k - y_k) {\\bf x}_k \\\\\n",
    "\\frac{\\partial \\text{NLL}}{\\partial b} & = \\sum_{k=0}^{K-1} (q_k - y_k)\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IWD068Pw8wsN",
    "tags": []
   },
   "source": [
    "Therefore, the gradient descent rules are\n",
    "$${\\bf w}_{n+1} = {\\bf w}_n + \\rho_n \\sum_{k=0}^{K-1} (y_k - q_{k,n}){\\bf x}_k$$\n",
    "$$b_{n+1} = b_n + \\rho_n \\sum_{k=0}^{K-1} (y_k - q_{k,n}),$$\n",
    "\n",
    "where $q_{k,n}$ is the probabilistic prediction for sample $k$. It depends on $n$ because it depends on the weights, which change at each iteration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GdEMHMbU8wsN",
    "tags": []
   },
   "outputs": [],
   "source": [
    "def backward(y, q, x):\n",
    "    \"\"\"\n",
    "    Computes the gradient of the loss function for a single sample x with\n",
    "    ouput y_hat, given label y.\n",
    "    \"\"\"\n",
    "    # w_grad = x.T.dot((1-y)*q - y*(1-q))/len(y)\n",
    "    # b_grad = np.sum((1-y)*q - y*(1-q))/len(y)\n",
    "    w_grad = x.T @ (q - y) / len(y)\n",
    "    b_grad = np.mean(q - y)\n",
    "    return w_grad, b_grad\n",
    "\n",
    "def loss(y, q):\n",
    "    return - (y @ np.log(q) + (1 - y) @ np.log(1 - q)) / len(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7GizaSvQ8wsN",
    "tags": []
   },
   "source": [
    "#### <font color='teal'> 3.1.5. Testing the single layer NN </font>\n",
    "\n",
    "Now, we will test the behavior of the single-layer NN with the given datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rpm3Qut48wsN",
    "outputId": "ac78ff20-21c7-4eb2-ddc5-0f3137bae2a4",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load normalized data\n",
    "X_train, X_val, y_train, y_val = get_dataset('digits', forze_binary=True)\n",
    "\n",
    "# Neural Network Training\n",
    "epochs = 400\n",
    "rho = .05    # Use this setting for Sign Digits Dataset\n",
    "\n",
    "# Parameter initialization\n",
    "w = .1 * np.random.randn(X_train.shape[1])\n",
    "b = .1 * np.random.randn(1)\n",
    "\n",
    "loss_train = np.zeros(epochs)\n",
    "loss_val = np.zeros(epochs)\n",
    "acc_train = np.zeros(epochs)\n",
    "acc_val = np.zeros(epochs)\n",
    "\n",
    "for epoch in np.arange(epochs):\n",
    "    print(f\"-- Epoch {epoch + 1} out of {epochs}    \\r\", end=\"\")\n",
    "    q_train = forward(w, b, X_train)\n",
    "    q_val = forward(w, b, X_val)\n",
    "    w_grad, b_grad = backward(y_train, q_train, X_train)\n",
    "    w = w - rho * w_grad\n",
    "    b = b - rho * b_grad\n",
    "\n",
    "    loss_train[epoch] = loss(y_train, q_train)\n",
    "    loss_val[epoch] = loss(y_val, q_val)\n",
    "    acc_train[epoch] = accuracy(y_train, q_train)\n",
    "    acc_val[epoch] = accuracy(y_val, q_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-pilH_9o8wsO",
    "outputId": "753e3667-1887-4e44-f976-04720ae0d5bd"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,3))\n",
    "plt.subplot(1, 2, 1), plt.plot(loss_train, 'b'), plt.plot(loss_val, 'r'), plt.legend(['train', 'val']),\n",
    "plt.title('Cross-entropy loss')\n",
    "plt.subplot(1, 2, 2), plt.plot(acc_train, 'b'), plt.plot(acc_val, 'r'), plt.legend(['train', 'val']),\n",
    "plt.title('Accuracy')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZYzaQnbg8wsO"
   },
   "source": [
    "#### <font color='olive'>Exercise 1 </font>\n",
    "\n",
    "Study the behavior of the algorithm changing the number of epochs and the learning rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aoXwAoR28wsT"
   },
   "outputs": [],
   "source": [
    "# <Write your code here>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "D8qhSXiQ8wsT",
    "tags": []
   },
   "source": [
    "#### <font color='olive'>Exercise 2 </font>\n",
    "\n",
    "Repeat the analysis for the other dataset, trying to obtain as large an accuracy value as possible. What do you believe are the reasons for the very different performance for both datasets?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "f8nOXzmP8wsT"
   },
   "outputs": [],
   "source": [
    "# <Write your code here>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZpjfffWT8wsT",
    "tags": []
   },
   "source": [
    "Linear logistic regression allowed us to review a few concepts that are key for Neural Networks:\n",
    "\n",
    "* Network topology (In this case, a linear network with one layer)\n",
    "* Activation functions\n",
    "* Parametric approach ($\\bf w$/$b$)\n",
    "* Parameter initialization\n",
    "* Obtaining the network prediction using *forward* computation\n",
    "* Loss function\n",
    "* Parameter gradient calculus using *backward* computation\n",
    "* Optimization method for parameters update (here, GD)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZELCZZYB8wsU",
    "tags": []
   },
   "source": [
    "### <font color='teal'> 3.2. Single-Layer Neural Networks for Multiclass Classification </font>\n",
    "\n",
    "#### <font color='teal'> 3.2.1. Multiclass problems and one-hot encoding </font>\n",
    "\n",
    "The single-layer NN can be easily extended to problems with $M \\ge 2$ classes, $0, 1, \\ldots, M-1$.\n",
    "\n",
    "To do so, we will represent classes using one-hot encoding, that is, $M$-dimensional vectors with zero componentes unless for a value 1 in the position indicated by the class.\n",
    "\n",
    "For instance, classes in $\\{0, 1, 2, 3\\}$ will be represented by vectors\n",
    "$$\n",
    "\\begin{pmatrix} 1 \\\\ 0 \\\\ 0 \\\\ 0 \\end{pmatrix} \\, \\,\n",
    "\\begin{pmatrix} 0 \\\\ 1 \\\\ 0 \\\\ 0 \\end{pmatrix}, \\, \\,\n",
    "\\begin{pmatrix} 0 \\\\ 0 \\\\ 1 \\\\ 0 \\end{pmatrix} \\text{ and }\n",
    "\\begin{pmatrix} 0 \\\\ 0 \\\\ 0 \\\\ 1 \\end{pmatrix},\n",
    "$$\n",
    "respectively.\n",
    "\n",
    "Thus, both the true-class, ${\\bf y}$, and the prediction, $\\hat{\\bf y}$, will be one-hot $M$-dimensional vectors ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5NcEYi_s8wsU"
   },
   "source": [
    "#### <font color='teal'> 3.2.2. Architecture </font>\n",
    "\n",
    "A natural extension of the single layer NN to multiple classes is shown in the figure\n",
    "\n",
    "<center><img src=\"figures/SR_network.png\" width=\"500\"/></center>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DJJwPh9_8wsU",
    "tags": []
   },
   "source": [
    "The components of the multiclass model are multidimensional extensions of those of the single-layer NN for the binary classification problem:\n",
    "\n",
    " * A **linear combination** is computed per each class. Note that, defining the matrix ${\\bf W}=({\\bf w}_0| {\\bf w}_1 | \\cdots | {\\bf w}_{M-1})^\\intercal$, we can write\n",
    "$$\n",
    "{\\bf o} = {\\bf W}{\\bf x} + {\\bf b}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "U0KGILQR8wsU",
    "tags": []
   },
   "source": [
    " * **Activation function**. The **softmax** function is the most common choice. It is a multidimensional generalization of the logistic function (invented in 1959 by the social scientist R. Duncan Luce) and defined as\n",
    "\\begin{align}\n",
    "q_i = \\frac{\\exp(o_i)}{\\sum_{j=0}^{M-1} \\exp(o_j)},\n",
    "\\end{align}\n",
    "and it provides probabilistic soft predictions because\n",
    "$$0 \\le q_i \\le 1$$\n",
    "$$\\sum_{j=0}^{M-1} q_j =1$$\n",
    "The derivatives of the softmax components, that will be required for training, are given by\n",
    "\\begin{align}\n",
    "\\frac{\\partial q_i}{\\partial o_i} &= q_i (1 - q_i) \\\\\n",
    "\\frac{\\partial q_i}{\\partial o_j} &= - q_i q_j,  \\qquad  j \\neq i\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-nj1WGOn8wsV",
    "tags": []
   },
   "source": [
    "* **Class prediction**: the final transformation maps the probabilistic predictions into a class prediction in one-hot form. Following the probabilistic interpretation of the soft prediction, we can use the **hardmax** function, which outputs a zero vector with a unit value in the ouput corresponding to the highest probabilistic input, that is,\n",
    "\\begin{align*}\n",
    "\\hat{y}_i =\n",
    "\\left[ \\begin{array}{ll}\n",
    "        1, & {\\rm if}\\,\\,  q_i = \\max_j q_j  \\\\\n",
    "        0, & {\\rm otherwise}\n",
    "\\end{array} \\right.\n",
    "\\end{align*}\n",
    "The classifier is still linear, in the sense that\n",
    "$$\\text{hardmax}({\\bf q}) = \\text{hardmax}({\\bf o}) = \\text{hardmax}({\\bf W} {\\bf x} + {\\bf b}\\}$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2HHF4lBU8wsV",
    "tags": []
   },
   "source": [
    "#### <font color='teal'> 3.2.3. Loss function </font>\n",
    "\n",
    "The losses defined for the binary case can be easily extended to the multiclass setting:\n",
    "\n",
    "The multi-class version of the cross entropy is defined as\n",
    "* **Square error**:    $\\,\\, \\ell_2({\\bf y}, {\\bf q}) = \\|{\\bf y} - {\\bf q}\\|^2$\n",
    "* **Absolute error**:  $\\,\\, \\ell_1({\\bf y}, {\\bf q}) = \\|{\\bf y} - {\\bf q}\\|_1$\n",
    "* **Cross entropy**:   $\\,\\, \\ell_\\text{CE}({\\bf y}, {\\bf q}) = - \\sum_{j=0}^{M-1} y_j \\log(q_j)$\n",
    "\n",
    "We will implement the cross entropy. For evaluation purposes, the accuracy will be used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2TCx0SLq8wsV",
    "tags": []
   },
   "outputs": [],
   "source": [
    "def accuracy(y, q):\n",
    "    return np.mean(np.argmax(y, axis=1) == np.argmax(q, axis=1))\n",
    "\n",
    "def loss(y, q):\n",
    "    return - np.sum(y * np.log(q))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RD01rOdV8wsV",
    "tags": []
   },
   "source": [
    "#### <font color='teal'> 3.2.4. Probabilistic model </font>\n",
    "\n",
    "As in the binary case, any neural network architecture with a probabilistic activation function defines a parametric probability model. For the architecture in the figure, such model is given by\n",
    "$$\n",
    "P(y_i=1|{\\bf x}, {\\bf W}, {\\bf b}) = q_i\n",
    "$$\n",
    "$$\n",
    "{\\bf q} = \\text{softmax}({\\bf W}{\\bf x} + {\\bf b})\n",
    "$$\n",
    "\n",
    "Consequently, the negative log-likelihood is identical to the empirical risk defined by the cross entropy, that is\n",
    "$$\n",
    "\\text{NLL}({\\bf W}, {\\bf b}) = \\sum_{k=0}^{K-1} \\ell_\\text{CE}({\\bf y}_k, {\\bf q}_k)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bM8j1XEd8wsW",
    "tags": []
   },
   "source": [
    "We will define a method to compute the softmax activation, and a ${\\tt forward}$ method to compute the soft prediction from the inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kvKL4tAX8wsW",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define some useful functions\n",
    "def softmax(t):\n",
    "    \"\"\"Compute softmax values for each sets of scores in t.\"\"\"\n",
    "    e_t = np.exp(t)\n",
    "    return e_t / e_t.sum(axis=1, keepdims=True)\n",
    "\n",
    "def forward(w, b, x):\n",
    "    # Compute the soft prediction of the network\n",
    "    return softmax(x @ w.T + b.T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WZtU-9Pu8wsW",
    "tags": []
   },
   "source": [
    "#### <font color='teal'> 3.2.4. Training </font>\n",
    "\n",
    "The Gradient Descent learning rules are given by\n",
    "$${\\bf W}_{n+1} = {\\bf W}_n - \\rho_n \\sum_{k=0}^{K-1} \\frac{\\partial l({\\bf y}_k,{{\\bf q}_k})}{\\partial {\\bf W}}$$\n",
    "$${\\bf b}_{n+1} = {\\bf b}_n - \\rho_n \\sum_{k=0}^{K-1} \\frac{\\partial l({\\bf y}_k,{{\\bf q}_k})}{\\partial {\\bf b}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8DqD47Ai8wsW",
    "tags": []
   },
   "source": [
    "Applying the chain rule, and using the derivatives of the softmax function, the derivatives can be computed as follows:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jkUPtz3P8wsX",
    "tags": []
   },
   "source": [
    "\\begin{align}\n",
    "\\frac{\\partial l({\\bf y}, {\\bf q})}{\\partial {\\bf W}}\n",
    "    &= \\frac{\\partial l({\\bf y}, {\\bf q})}{\\partial {\\bf o}}\n",
    "                 \\cdot \\frac{\\partial {\\bf o}}{\\partial {\\bf W}} \\\\\n",
    "    &= \\sum_{i=0}^{M-1}\n",
    "           \\frac{\\partial l({\\bf y}, {\\bf q})}{\\partial o_i}\n",
    "               \\cdot \\frac{\\partial o_i}{\\partial {\\bf W}} \\\\\n",
    "    &= \\frac{\\partial l({\\bf y}, {\\bf q})}{\\partial {\\bf o}}\n",
    "           \\cdot {\\bf x}^\\intercal \\\\\n",
    "    &= \\frac{\\partial {\\bf q}}{\\partial {\\bf o}}\n",
    "           \\cdot \\frac{\\partial l({\\bf y}, {\\bf q})}{\\partial {\\bf q}}\n",
    "           \\cdot {\\bf x}^\\intercal \\\\\n",
    "    & = \\left[\\begin{array}{ccccc}\n",
    "                  q_1 (1 - q_1) & - q_1 q_2      & \\dots  & - q_1 q_{M-1} \\\\\n",
    "                  - q_2 q_1      & q_2 (1 - q_2) & \\dots  & - q_2 q_{M-1} \\\\\n",
    "                  \\vdots                  & \\vdots                  & \\ddots & \\vdots                 \\\\\n",
    "                  - q_{M-1} q_1 & -q_{M-1} q_2  & \\dots  & q_{M-1} (1-q_{M-1})  \n",
    "              \\end{array}\\right]\n",
    "        \\left[\\begin{array}{c} -y_1/q_1 \\\\ -y_2/q_2 \\\\ \\vdots \\\\ - y_{M-1}/q_{M-1} \\end{array}\\right]\n",
    "        {\\bf x}^\\intercal \\\\\n",
    "    & = ({\\bf q} - {\\bf y}){\\bf x}^\\intercal \\\\\n",
    "\\\\\n",
    "\\frac{\\partial l({\\bf y},{{\\bf q}})}{\\partial {\\bf b}}\n",
    "    & = {\\bf q} - {\\bf y}\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "I8pS73Gl8wsX",
    "tags": []
   },
   "source": [
    "Thus, the gradient descent learning rules are\n",
    "$${\\bf W}_{n+1} = {\\bf W}_n + \\rho_n \\sum_{k=0}^{K-1} ({\\bf y}_k - {\\bf q}_{k,n}) \\cdot {\\bf x}_k^\\intercal$$\n",
    "$${\\bf b}_{n+1} = {\\bf b}_n + \\rho_n \\sum_{k=0}^{K-1} ({\\bf y}_k - {\\bf q}_{k,n}) $$\n",
    "where ${\\bf q}_{k,n}$ is the soft prediction for sample $k$. It depends on $n$ because the soft prediction depends on the weights, which change at each iteration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NVkBvrtp8wsX",
    "tags": []
   },
   "outputs": [],
   "source": [
    "def backward(y, q, x):\n",
    "    # Calcula los gradientes\n",
    "    W_grad = (q - y).T @ x / len(y)\n",
    "    b_grad = (q - y).T.mean(axis=1, keepdims=True)\n",
    "    return W_grad, b_grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d7xy8S1j8wsX",
    "tags": []
   },
   "source": [
    "#### <font color='teal'> 3.2.5. Testing the multi-class single-layer NN </font>\n",
    "\n",
    "Now, we will test the behavior of the multiclass NN with the `digits` dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3uC6Y67P8wsY",
    "tags": []
   },
   "outputs": [],
   "source": [
    "dataset = 'digits'\n",
    "X_train, X_val, y_train, y_val = get_dataset('digits')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DFjNkDUK8wsY",
    "outputId": "c29e2769-2e42-46b2-fa8d-f2460600c705"
   },
   "outputs": [],
   "source": [
    "# Neural Network Training\n",
    "\n",
    "epochs = 300\n",
    "rho = .1\n",
    "\n",
    "#Parameter initialization\n",
    "W = .1 * np.random.randn(y_train.shape[1], X_train.shape[1])\n",
    "b = .1 * np.random.randn(y_train.shape[1], 1)\n",
    "\n",
    "loss_train = np.zeros(epochs)\n",
    "loss_val = np.zeros(epochs)\n",
    "acc_train = np.zeros(epochs)\n",
    "acc_val = np.zeros(epochs)\n",
    "\n",
    "for epoch in np.arange(epochs):\n",
    "    print(f\"Epoch {epoch + 1} out of {epochs}   \\r\", end=\"\")\n",
    "    q_train = forward(W, b, X_train)\n",
    "    q_val = forward(W, b, X_val)\n",
    "    W_grad, b_grad = backward(y_train, q_train, X_train)\n",
    "    W = W - rho * W_grad\n",
    "    b = b - rho * b_grad\n",
    "\n",
    "    loss_train[epoch] = loss(y_train, q_train)\n",
    "    loss_val[epoch] = loss(y_val, q_val)\n",
    "    acc_train[epoch] = accuracy(y_train, q_train)\n",
    "    acc_val[epoch] = accuracy(y_val, q_val)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yvf56nBx8wsY",
    "outputId": "54b98e67-7f45-4c78-f67b-e802af9be120"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,5))\n",
    "plt.subplot(1, 2, 1), plt.plot(loss_train, 'b'), plt.plot(loss_val, 'r'),\n",
    "plt.legend(['train', 'val']), plt.title('Cross-entropy loss')\n",
    "plt.subplot(1, 2, 2), plt.plot(acc_train, 'b'), plt.plot(acc_val, 'r'),\n",
    "plt.legend(['train', 'val']), plt.title('Accuracy')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dGDyPUsg8wsZ",
    "tags": []
   },
   "source": [
    "#### <font color='olive'>Exercise 3</font>\n",
    "\n",
    "Study the behavior of the algorithm changing the number of epochs and the learning rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_a3M8qmW8wsZ",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Write your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "G4WSIoUQ8wsZ",
    "tags": []
   },
   "source": [
    "#### <font color='olive'>Exercise 4</font>\n",
    "\n",
    "Obtain the confusion matrix, and study which classes are more difficult to classify\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xxGPbvT58wsZ"
   },
   "outputs": [],
   "source": [
    "# Write your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "O8yb5Cx_8wsa",
    "tags": []
   },
   "source": [
    "#### <font color='olive'>Exercise 5</font>\n",
    "\n",
    "Think about the differences between using this 10-class network, vs training 10 binary classifiers, one for each class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LE8eqXnF8wsa"
   },
   "outputs": [],
   "source": [
    "# Write your response here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bbKJK2Rl8wsm",
    "tags": []
   },
   "source": [
    "As in linear logistic regression note that we covered the following aspects of neural network design, implementation, and training:\n",
    "\n",
    "* Network topology (In this case, a linear network with one layer and $M$ ouptuts)\n",
    "* Activation functions (softmax activation)\n",
    "* Initialization of parameters ($\\bf W$, $\\bf b$)\n",
    "* Obtaining the network prediction using *forward* computation\n",
    "* Loss function\n",
    "* Gradient calculus using *backward* computation\n",
    "* Optimization method for parameters update (here, GD)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WJJCCkah8wsm",
    "tags": []
   },
   "source": [
    "### <font color='teal'> 3.3. Multi Layer Networks (Deep Networks) </font>\n",
    "\n",
    "Previous networks are constrained in the sense that they can only implement linear classifiers: the boundary decision of a binary single-layer NN is linear (an hyperplane) and the boundary sepearating each pair of classes in a multi-class single-layer NN is also linear.\n",
    "\n",
    "As in logistic regression, we can easily apply the single-layer NN to non-linear classification problems by using fixed non-linear transformations of the inputs: ${\\bf z} = {\\bf{f}}({\\bf x})$, as the inputs to the linear layer. However, a fixed non-linear transformation limits the adaptability of the network to different datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "I323nPFS8wsn",
    "tags": []
   },
   "source": [
    "An interesting alternative is to parametrize the transformation using one or more non-linear layers of neurons. This is the central idea of the **multi-layer perceptron** (MLP).\n",
    "\n",
    "<center><img src=\"figures/LR_MLPnetwork.png\" width=\"500\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OMiVoldm8wsn",
    "tags": []
   },
   "source": [
    "* When counting layers, we normally ignore the input layer, since there is no computation involved\n",
    "* Intermediate layers are normally referred to as \"**hidden layers**\"\n",
    "* **Non-linear activations** result in an overall non-linear classifier\n",
    "* We can still use **gradient descent optimization** as long as the derivatives of the loss function with respect to all parameters exist.\n",
    "* This is already **deep learning**. We can have two layers or more, each with different numbers of neurons. But as long as derivatives with respect to parameters can be calculated, the network can be optimized\n",
    "* **Structural optimization**: Finding an appropriate number of layers for a particular problem, as well as the number of neurons per layer, requires exploration\n",
    "* The more data we have for training the network, the more parameters we can afford, making feasible the use of more complex topologies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aYHMvPVE8wsn",
    "tags": []
   },
   "source": [
    "#### <font color='teal'> 3.3.1. Example: a 2-layer network for binary classification</font>\n",
    "\n",
    "##### <font color='teal'> Network topology\n",
    "\n",
    "The forward computation graph, shown in the figure, illustrates the computation steps that produce the network prediction and the loss computation\n",
    "<center><img src=\"figures/forward_graph.png\" width=\"500\"/></center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5R6VheQe8wso",
    "tags": []
   },
   "source": [
    "  * **Hidden layers**: one hidden layer with $n_h$ neurons with hyperbolic tangent activation. The hyperbolic tangent is just a shifted version of the logistic function producing outputs in the interval $[-1, 1]$ $(\\text{tanh}(o) = 2 \\text{logistic}(o)-1)$. It does not produce probabilistic outputs, but they are not needed at intermediate layers.\n",
    "\n",
    "  * **Output layer**: a single neuron with logistic activation function.\n",
    "  * **Loss function**: Cross-entropy\n",
    "\n",
    "\n",
    "         \n",
    "The network equations are, thus:\n",
    "\n",
    "$${\\bf h} = \\text{tanh}({\\bf o}^{(1)})= \\text{tanh}\\left({\\bf W}^{(1)} {\\bf x} + {\\bf b}^{(1)}\\right)$$\n",
    "$$q = \\text{logistic}(o) = \\text{logistic}\\left({{\\bf w}^{(2)}}^\\top {\\bf h} + b^{(2)}\\right)$$\n",
    "\n",
    "(where the hyperbolic tangent of a vector is computed component-wise). They are implemented in the forward method, below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OqghuhES8wso"
   },
   "outputs": [],
   "source": [
    "# Define some useful functions\n",
    "def logistic(t):\n",
    "    return 1.0 / (1 + np.exp(-t))\n",
    "\n",
    "def forward(W1, b1, w2, b2, x):\n",
    "    # Compute the network output\n",
    "    h = 2 * logistic(x.dot(W1.T) + b1) - 1\n",
    "    q = logistic(h.dot(w2) + b2)\n",
    "    # Return also hidden units value for backward gradient step\n",
    "    return h, q"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GCwo4kr98wsp"
   },
   "source": [
    "##### <font color='teal'> Training\n",
    "\n",
    "We will train the neural network by applying the gradient descent learning rule to the minimization of the NLL (i.e. the cumulative cross entropy).\n",
    "\n",
    "  To do so, we need to compute the derivatives of the loss with respect to every network parameter. We will do it by applying extensively the chain rule:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GtNaLRGu8wsp"
   },
   "source": [
    "  * **Output layer** weights: the derivatives are the same that we have computed for the single-layer NN, since the dependency of the loss on the output layer weights is the same (we just need to use ${\\bf h}$ instead of ${\\bf x}$):\n",
    "\n",
    "  $${\\bf w}_{n+1}^{(2)} = {\\bf w}_n^{(2)} + \\rho_n \\sum_{k=0}^{K-1} (y_k - q_{k,n}){\\bf h}_{k,n}$$\n",
    "  $$b_{n+1}^{(2)} = b_n^{(2)} + \\rho_n \\sum_{k=0}^{K-1} (y_k - q_{k,n})$$\n",
    "  \n",
    "  * **Hidden layer** weights: we need to use the chain rule (we ignore dimensions and rearrange at the end)    \n",
    "\\begin{align}\n",
    "\\\\\n",
    "\\frac{\\partial \\ell_\\text{CE}(y, q)}{\\partial {\\bf W}^{(1)}}\n",
    "    & = \\frac{\\partial \\ell_\\text{CE}(y, q)}{\\partial o}\n",
    "            \\cdot \\frac{\\partial o}{\\partial {\\bf h}}\n",
    "            \\cdot \\frac{\\partial {\\bf h}}{\\partial {\\bf o}^{(1)}}\n",
    "            \\cdot \\frac{\\partial {\\bf o}^{(1)}}{\\partial {\\bf W}^{(1)}} \\\\\n",
    "    & = (q - y) \\left[{\\bf w}^{(2)} \\odot ({\\bf 1}-{\\bf h})^2\\right] {\\bf x}^{\\top}\n",
    "\\end{align}\n",
    "    where $\\odot$ denotes component-wise multiplication and the square after $({\\bf 1}-{\\bf h})$ should be computed component-wise. (Note, also, that $\\frac{\\partial {\\bf o}^{(1)}}{\\partial {\\bf W}^{(1)}}$ is actually a three dimensional matrix (i.e. a *tensor*). To apply the chain rule properly, the multiplications in the above equation must represent the adequate tensor products)\n",
    "\\begin{align}\n",
    "\\\\\n",
    "\\frac{\\partial \\ell_\\text{CE}(y, q)}{\\partial {\\bf b}^{(1)}}\n",
    "    & = \\frac{\\partial \\ell_\\text{CE}(y, q)}{\\partial o}\n",
    "            \\cdot \\frac{\\partial o}{\\partial {\\bf h}}\n",
    "            \\cdot \\frac{\\partial {\\bf h}}{\\partial {\\bf o}^{(1)}}\n",
    "            \\cdot \\frac{\\partial {\\bf o}^{(1)}}{\\partial {\\bf b}^{(1)}} \\\\\n",
    "    & = (q - y) \\left[{\\bf w}^{(2)} \\odot ({\\bf 1}-{\\bf h})^2\\right]\n",
    "\\end{align}\n",
    "\n",
    "* GD update rules become\n",
    "$${\\bf W}_{n+1}^{(1)} = {\\bf W}_n^{(1)} + \\rho_n \\sum_{k=0}^{K-1} (y_k - q_{k,n})\\left[{\\bf w}^{(2)} \\odot ({\\bf 1}-{\\bf h}_{k,n})^2\\right] {\\bf x}_k^{\\top}$$\n",
    "$${\\bf b}_{n+1}^{(1)} = {\\bf b}_n^{(1)} + \\rho_n \\sum_{k=0}^{K-1} (y_k - q_{k,n})\\left[{\\bf w}^{(2)} \\odot ({\\bf 1}-{\\bf h}_{k,n})^2\\right]$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BKcsV3dy8wsp"
   },
   "outputs": [],
   "source": [
    "def backward(y, q, h, x, w2):\n",
    "    #Calcula los gradientes\n",
    "    w2_grad = h.T.dot(q - y) / len(y)\n",
    "    b2_grad = np.sum(q - y) / len(y)\n",
    "    W1_grad = ((w2[np.newaxis,] * ((1 - h)**2) * (q - y)[:,np.newaxis]).T.dot(x)) / len(y)\n",
    "    b1_grad = ((w2[np.newaxis,] * ((1 - h)**2) * (q - y)[:,np.newaxis]).sum(axis=0)) / len(y)\n",
    "    return w2_grad, b2_grad, W1_grad, b1_grad\n",
    "\n",
    "def accuracy(y, q):\n",
    "    return np.mean(y == (q >= 0.5))\n",
    "\n",
    "def loss(y, q):\n",
    "    return - np.sum(y * np.log(q) + (1 - y) * np.log(1 - q)) / len(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "A-TEoUfm8wsq",
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "#### <font color='teal'> 3.3.2. The back-propagation algorithm </font>\n",
    "\n",
    "The process that we have followed to compute the loss derivatives with respect to the weights can be extended to networks with an arbitrary number of layers.\n",
    "\n",
    "Note that derivatives are computed backwards: from the last layer to the first hidden layer, so that we can use intermediate computations at a some layer to compute derivatives at layers that are further back.\n",
    "\n",
    "For this reason, the gradient descent method is called the **back-propagation** algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "un8toFrm8wsq",
    "tags": []
   },
   "source": [
    "#### <font color='teal'> 3.3.3. Testing the 2-layer network</font>\n",
    "\n",
    "Now we are ready to evaluate the two layer network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Od6nW0TX8wsq",
    "tags": []
   },
   "outputs": [],
   "source": [
    "def evaluate_model(\n",
    "    X_train, X_val, y_train, y_val, n_h=5, epochs=1000, rho=.005):\n",
    "\n",
    "    W1 = .01 * np.random.randn(n_h, X_train.shape[1])\n",
    "    b1 = .01 * np.random.randn(n_h)\n",
    "    w2 = .01 * np.random.randn(n_h)\n",
    "    b2 = .01 * np.random.randn(1)\n",
    "\n",
    "    loss_train = np.zeros(epochs)\n",
    "    loss_val = np.zeros(epochs)\n",
    "    acc_train = np.zeros(epochs)\n",
    "    acc_val = np.zeros(epochs)\n",
    "\n",
    "    for epoch in np.arange(epochs):\n",
    "        print(f'Current epoch: {epoch + 1}  \\r', end=\"\")\n",
    "\n",
    "        h, q_train = forward(W1, b1, w2, b2, X_train)\n",
    "        dum, q_val = forward(W1, b1, w2, b2, X_val)\n",
    "        w2_grad, b2_grad, W1_grad, b1_grad = backward(y_train, q_train, h, X_train, w2)\n",
    "        W1 = W1 - rho/10 * W1_grad\n",
    "        b1 = b1 - rho/10 * b1_grad\n",
    "        w2 = w2 - rho * w2_grad\n",
    "        b2 = b2 - rho * b2_grad\n",
    "\n",
    "        loss_train[epoch] = loss(y_train, q_train)\n",
    "        loss_val[epoch] = loss(y_val, q_val)\n",
    "        acc_train[epoch] = accuracy(y_train, q_train)\n",
    "        acc_val[epoch] = accuracy(y_val, q_val)\n",
    "\n",
    "    return loss_train, loss_val, acc_train, acc_val\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "y6bsrtTu8wsr",
    "tags": []
   },
   "source": [
    "##### <font color='olive'>Results in Dogs vs Cats dataset</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8y6Cq0tf8wsr",
    "outputId": "fbdb1d7a-870e-4bf0-d49e-84f1a4604f1f"
   },
   "outputs": [],
   "source": [
    "dataset = 'DogsCats'\n",
    "\n",
    "X_train, X_val, y_train, y_val = get_dataset(dataset)\n",
    "loss_train, loss_val, acc_train, acc_val = evaluate_model(\n",
    "    X_train, X_val, y_train, y_val, n_h=5, epochs=5000, rho=0.05)\n",
    "\n",
    "plt.figure(figsize=(14,5))\n",
    "plt.subplot(1, 2, 1), plt.plot(loss_train, 'b'), plt.plot(loss_val, 'r'),\n",
    "plt.legend(['train', 'val']), plt.title('Cross-entropy loss')\n",
    "plt.subplot(1, 2, 2), plt.plot(acc_train, 'b'), plt.plot(acc_val, 'r'),\n",
    "plt.legend(['train', 'val']), plt.title('Accuracy')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AOB6n_Vk8wsr",
    "outputId": "39b05eb2-6f3b-4fe8-bc1f-3246d4b82111",
    "tags": []
   },
   "outputs": [],
   "source": [
    "dataset = 'DogsCats'\n",
    "\n",
    "X_train, X_val, y_train, y_val = get_dataset(dataset)\n",
    "loss_train, loss_val, acc_train, acc_val = evaluate_model(\n",
    "    X_train, X_val, y_train, y_val, n_h=5, epochs=500, rho=0.5)\n",
    "\n",
    "plt.figure(figsize=(14,5))\n",
    "plt.subplot(1, 2, 1), plt.plot(loss_train, 'b'), plt.plot(loss_val, 'r'),\n",
    "plt.legend(['train', 'val']), plt.title('Cross-entropy loss')\n",
    "plt.subplot(1, 2, 2), plt.plot(acc_train, 'b'), plt.plot(acc_val, 'r'),\n",
    "plt.legend(['train', 'val']), plt.title('Accuracy')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-Z-Ne2L_8wss",
    "tags": []
   },
   "source": [
    "##### <font color='olive'>Results in Binary Sign Digits Dataset</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "A9sdBI8K8wss",
    "outputId": "81563a61-51c1-4778-87b7-013df6375773"
   },
   "outputs": [],
   "source": [
    "dataset = 'digits'\n",
    "X_train, X_val, y_train, y_val = get_dataset(dataset, forze_binary=True)\n",
    "loss_train, loss_val, acc_train, acc_val = evaluate_model(\n",
    "    X_train, X_val, y_train, y_val, n_h=5, epochs=10000, rho=0.001)\n",
    "\n",
    "plt.figure(figsize=(14,5))\n",
    "plt.subplot(1, 2, 1), plt.plot(loss_train, 'b'), plt.plot(loss_val, 'r'),\n",
    "plt.legend(['train', 'val']), plt.title('Cross-entropy loss')\n",
    "plt.subplot(1, 2, 2), plt.plot(acc_train, 'b'), plt.plot(acc_val, 'r'),\n",
    "plt.legend(['train', 'val']), plt.title('Accuracy')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0g1PVLEL8wst",
    "tags": []
   },
   "source": [
    "#### <font color='olive'>Exercise 6</font>\n",
    "\n",
    "Train the network using other settings for:\n",
    "\n",
    "* The number of epochs\n",
    "* The learning step\n",
    "* The number of neurons in the hidden layer\n",
    "   \n",
    "You may find **divergence issues** for some settings\n",
    "\n",
    "* Related to the use of the hyperbolic tangent function in the hidden layer (numerical issues)\n",
    "* This is also why learning step was selected smaller for the hidden layer\n",
    "* **Optimized libraries rely on certain modifications to obtain more robust implementations**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sSJF5vZo8wst",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Write your solution here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ptE8qyuI8wst",
    "tags": []
   },
   "source": [
    "#### <font color='olive'>Exercise 7</font>\n",
    "\n",
    "Try to solve both problems using the <a href=\"https://scikit-learn.org/stable/modules/generated/sklearn.neural_network.MLPClassifier.html\">scikit-learn implementation of the MLP</a>\n",
    "\n",
    "* You can also explore other activation functions\n",
    "* You can also explore other solvers to speed up convergence\n",
    "* You can also adjust the size of minibatches\n",
    "* Take a look at the *early_stopping* parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QaIxo9Uo8wst",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Write your solution here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZBiQOZ8_8wsu",
    "tags": []
   },
   "source": [
    "### <font color='teal'> 3.4. Activation Functions</font>\n",
    "\n",
    "The MLP with two layers that we have used as an example contains sigmoid-type activation functions (logistic or hyperbolic tangent), which produce bounded outputs.\n",
    "\n",
    "A major inconvenient of these kind of activations is that their derivatives vanish for large values of the input. As a consequence, learning can get stucked in *flat* regions of the parameter space.\n",
    "\n",
    "Activation functions must be non-linear (otherwise, all network layers could be colapsed into a single one), but they do not need to be neither probabilistic nor bounded (with the possible exception of the final layer)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uDCc0LEt8wsu",
    "tags": []
   },
   "source": [
    "For this reason, many other activation functions have been proposed. Some examples are\n",
    "\n",
    "  * **ReLU** (Rectified Linear Unit):\n",
    "    * ${\\rm ReLU}(t) = \\max(0, t)$.\n",
    "    * It is a one-side linear function. Its derivative is the step function.\n",
    "  * **PReLU** (Parametric Rectified Linear Unit):\n",
    "    * ${\\rm PReLU} = \\max(\\alpha t, t)$.\n",
    "    * A modification of the ReLU that replaces the constant term 0 by a linear term with an adjustable parameter, that avoids zero derivatives. For $\\alpha=0.01$, it is named **Leaky ReLU**.\n",
    "  * **Softplus**:\n",
    "    * ${\\rm softplus}(t) = \\log(1 + \\exp(t))$.\n",
    "    * Its derivative is the logistic function. It is a \"soft\" version of the ReLU: for large $|t|$, ${\\rm softplus}(t) \\approx {\\rm ReLU}(t)$\n",
    "\n",
    "(you can refer to the <a href=\"https://pytorch.org/docs/stable/nn.html#non-linear-activations-weighted-sum-nonlinearity\">pytorch documentation </a> or the <a href=\"https://en.wikipedia.org/wiki/Activation_function\"> Wikipedia</a> to see many other examples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TCJ2WYjU8wsu",
    "outputId": "96225e70-b7a4-411d-f539-6187699b01a8"
   },
   "outputs": [],
   "source": [
    "x_array = np.linspace(-6,6,100)\n",
    "relu = np.clip(x_array, 0, a_max=None)\n",
    "softplus = np.log(1 + np.exp(x_array))\n",
    "LeakyLU = np.clip(x_array, 0.1 * x_array, a_max=None)\n",
    "\n",
    "fig, axs = plt.subplots(1, 3)\n",
    "fig.set_figwidth(15)\n",
    "axs[0].plot(x_array, relu)\n",
    "axs[1].plot(x_array, LeakyLU)\n",
    "axs[2].plot(x_array, softplus)\n",
    "axs[0].grid()\n",
    "axs[1].grid()\n",
    "axs[2].grid()\n",
    "axs[0].set_title('ReLU activation function')\n",
    "axs[1].set_title('PReLU ($\\\\alpha=0.1$) activation function')\n",
    "axs[2].set_title('SoftPlus activation function')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "q7CGh4Cs8wsu",
    "tags": []
   },
   "source": [
    "Surprisingly, as explained in the <a href=\"http://d2l.ai/chapter_multilayer-perceptrons/mlp.html#activation-functions\">Dive into Deep Learning book</a>, the most popular choice for the hidden layers is the ReLU: despite its simplicity, it has shown good performance on many predictive tasks. Morever, despite it derivative is zero on one side, ReLU has demonstrated to mitigate the problem of vanishing gradients that seriously affected sigmoid-based neural networks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ldh38cUt8wsv"
   },
   "source": [
    "### <font color='teal'> 3.5. Multi Layer Networks for Regression </font>\n",
    "\n",
    "Deep Learning networks can be used to solve regression problems with the following common adjustments\n",
    "\n",
    "  * Linear activation for the output unit\n",
    "    \n",
    "  * Square loss (or other than the cross entropy):\n",
    "\n",
    "    $$\\ell(y, \\hat y) = (y - \\hat y)^2, \\qquad \\text{where} \\qquad y, \\hat y \\in \\mathbb{R}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "B7kQp2XQ8wsv",
    "tags": []
   },
   "source": [
    "## <font color='teal'> 4. Implementing Deep Networks with PyTorch </font>\n",
    "\n",
    "* Pytorch is a Python library that provides different levels of abstraction for implementing deep neural networks\n",
    "\n",
    "* The main features of PyTorch are:\n",
    "    * Definition of numpy-like **n-dimensional tensors**. They can be stored in (or moved to) GPU for **parallel execution** of operations\n",
    "    * **Automatic calculation of gradients**, making *backward gradient calculation* transparent to the user\n",
    "    * **Pre-defined components**: common loss functions, different types of NN layers, optimization methods, data loaders, etc, simplifying NN implementation and training\n",
    "    * Provides **different levels of abstraction**, thus a good balance between flexibility and simplicity\n",
    "    \n",
    "* This notebook provides just a basic review of the main concepts necessary to train NNs with PyTorch taking materials from:\n",
    "    * <a href=\"https://pytorch.org/tutorials/beginner/pytorch_with_examples.html\">Learning PyTorch with Examples</a>, by Justin Johnson\n",
    "    * <a href=\"https://pytorch.org/tutorials/beginner/nn_tutorial.html\">What is *torch.nn* really?</a>, by Jeremy Howard\n",
    "    * <a href=\"https://www.kaggle.com/kanncaa1/pytorch-tutorial-for-deep-learning-lovers\">Pytorch Tutorial for Deep Learning Lovers</a>, by Kaggle user kanncaa1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "40987i_W8wsv"
   },
   "source": [
    "### <font color='teal'> 4.1. Installation and PyTorch introduction</font>\n",
    "\n",
    "* PyTorch can be installed with or without GPU support\n",
    "    * If you have an Anaconda installation, you can install from the command line, using the <a href=\"https://pytorch.org/\">instructions of the project website</a>\n",
    "    \n",
    "* PyTorch is also preinstalled in Google Collab with free GPU access\n",
    "    * Follow RunTime -> Change runtime type, and select GPU for HW acceleration\n",
    "    \n",
    "* Please, refer to Pytorch [getting started](https://pytorch.org/get-started/locally/) tutorial for a quick introduction regarding tensor definition, GPU vs CPU storage of tensors, operations, and bridge to Numpy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "S_YN_pEZ8wsv"
   },
   "source": [
    "### <font color='teal'> 4.2. Torch tensors (very) general overview</font>\n",
    "\n",
    "We can create tensors with different construction methods provided by the library, either to create new tensors from scratch or from a Numpy array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Jr0kf9dR8wsv",
    "outputId": "63fedd7b-8eec-489a-a386-1c05deb683f3"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "x = torch.rand((100,200))\n",
    "digitsX_flatten_tensor = torch.from_numpy(digitsX_flatten)\n",
    "\n",
    "print(x.type())\n",
    "print(digitsX_flatten_tensor.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "R5yh_y5x8wsw"
   },
   "source": [
    "* Tensors can be converted back to numpy arrays\n",
    "* Note that in this case, a tensor and its corresponding numpy array **will share memory**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9SVCUYbo8wsw"
   },
   "source": [
    "Operations and slicing use a syntax similar to numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KDUkBvoc8wsw",
    "outputId": "aeec9016-2e7e-467c-8f05-95fe7e9a0f38"
   },
   "outputs": [],
   "source": [
    "print('Size of tensor x:', x.size())\n",
    "print('Tranpose of vector has size', x.t().size()) #Transpose and compute size\n",
    "print('Extracting upper left matrix of size 3 x 3:', x[:3,:3])\n",
    "print(x.mm(x.t()).size())  #mm for matrix multiplications\n",
    "xpx = x.add(x)\n",
    "xpx2 = torch.add(x,x)\n",
    "print((xpx != xpx2).sum())   # Since all are equal, count of different terms is zero"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uWTnyjMS8wsw"
   },
   "source": [
    "* Adding underscore performs operations \"*in place*\", e.g., ```x.add_(y)```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Yp98mefH8wsx"
   },
   "source": [
    "If a GPU is available, tensors can be moved to and from the GPU device Operations on tensors stored in a GPU will be carried out using GPU resources and will typically be highly parallelized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hvypMFu18wsx",
    "outputId": "884bf776-5220-46ff-fa0d-f899849f14bc"
   },
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "    x = x.to(device)\n",
    "    y = x.add(x)\n",
    "    y = y.to('cpu')\n",
    "else:\n",
    "    print('No GPU card is available')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YPHErPHO8wsx"
   },
   "source": [
    "### <font color='teal'> 4.3. Automatic gradient calculation </font>\n",
    "\n",
    "PyTorch tensors have a property ```requires_grad```. When true, PyTorch automatic gradient calculation will be activated for that variable\n",
    "\n",
    "* In order to compute these derivatives numerically, PyTorch keeps track of all operations carried out on these variables, organizing them in a forward computation graph.\n",
    "* When executing the ```backward()``` method, derivatives will be calculated\n",
    "* However, this should only be activated when necessary, to save computation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "z1jT_f8Z8wsx",
    "outputId": "c166a9e6-3648-4b4b-a6c6-da26aece0e59"
   },
   "outputs": [],
   "source": [
    "x.requires_grad = True\n",
    "y = (3 * torch.log(x)).sum()\n",
    "y.backward()\n",
    "print(x.grad[:2,:2])\n",
    "print(3/x[:2,:2])\n",
    "\n",
    "x.requires_grad = False\n",
    "x.grad.zero_()\n",
    "print('Automatic gradient calculation is deactivated, and gradients set to zero')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "G1tQfHE18wsy"
   },
   "source": [
    "#### <font color='olive'>Exercise 8</font>\n",
    "\n",
    "**1.1.** Initialize a tensor `x` with the upper right $5 \\times 10$ submatrix of flattened digits. Activate `x`as a variable required for gradient computation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "W__gdGNw8wsy",
    "outputId": "c06c0392-6f29-4ca5-c603-b800d9881a17"
   },
   "outputs": [],
   "source": [
    "# Write your solution here\n",
    "# <SOL>\n",
    "# </SOL>\n",
    "\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EswnoG238wsy"
   },
   "source": [
    "**1.2.** Compute output vector `y` as the component-wise square root of `x`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "goNo5YWk8wsy",
    "outputId": "a8ae09c4-8c39-4634-e588-609582931593"
   },
   "outputs": [],
   "source": [
    "# y = <FILL IN>\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LJoIpZjf8wsz"
   },
   "source": [
    "**1.3.** Compute scalar value ```z``` as the sum of all elements in `y` squared. You can easily test if it is equal to the sum of all elements in `x`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9TUGC1We8wsz",
    "outputId": "5efbefa5-09af-44be-8e7f-763b16fdc3ec"
   },
   "outputs": [],
   "source": [
    "# z = <FILL IN>\n",
    "\n",
    "print(z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Aw1PBmY08wsz"
   },
   "source": [
    "**1.4.** Compute the derivatives of `z` with the `backward` method, an check if they are correct.\n",
    "\n",
    "**Note:** The backward method can only be run on scalar variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-H3YZ_3J8wsz"
   },
   "outputs": [],
   "source": [
    "# Write your solution here\n",
    "# <SOL>\n",
    "# </SOL>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XXNXMMcy8wsz"
   },
   "source": [
    "**1.5.** If you try to run the last cell multiple times, yoy will likely get an error. Implement the necessary modifications so that you can run the backward method multiple times, but the gradient does not change from run to run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ITpzHs5-8ws0",
    "outputId": "48647052-430f-49c9-ca97-e6f23a5faf24"
   },
   "outputs": [],
   "source": [
    "# Write your solution here\n",
    "# <SOL>\n",
    "# </SOL>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "G1brwTMl8ws0"
   },
   "source": [
    "### <font color='teal'> 4.4. Feed Forward Neural Network using PyTorch </font>\n",
    "\n",
    "In this section we will change our code for a neural network to use tensors instead of numpy arrays. We will work with the sign `digits` datasets.\n",
    "\n",
    "We will introduce all concepts using a single layer perceptron (softmax regression), and then implement networks with additional hidden layers\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KA67C26f8ws0"
   },
   "source": [
    "#### <font color='olive'> 4.4.1. Using Automatic differentiation </font>\n",
    "\n",
    "We start by loading the data, and converting to tensors.\n",
    "\n",
    "* As a first step, we refactor our code to use tensor operations\n",
    "* We do not need to pay too much attention to particular details regarding tensor operations, since these will not be necessary when moving to higher PyTorch abstraction levels\n",
    "* We do not need to implement gradient calculation. PyTorch will take care of that"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "boIGRoWL8ws0"
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "dataset = 'digits'\n",
    "\n",
    "# Joint normalization of all data. For images [-.5, .5] scaling is frequent\n",
    "min_max_scaler = MinMaxScaler(feature_range=(-.5, .5))\n",
    "X = min_max_scaler.fit_transform(digitsX_flatten)\n",
    "\n",
    "# Generate train and validation data, shuffle\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, digitsY, test_size=0.2, random_state=42, shuffle=True)\n",
    "\n",
    "# Convert to Torch tensors\n",
    "X_train_torch = torch.from_numpy(X_train)\n",
    "X_val_torch = torch.from_numpy(X_val)\n",
    "y_train_torch = torch.from_numpy(y_train)\n",
    "y_val_torch = torch.from_numpy(y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_jN06JmZ8ws2"
   },
   "outputs": [],
   "source": [
    "# Define some useful functions\n",
    "def softmax(t):\n",
    "    \"\"\"Compute softmax values for each sets of scores in t\"\"\"\n",
    "    return t.exp() / t.exp().sum(-1).unsqueeze(-1)\n",
    "\n",
    "def model(w,b,x):\n",
    "    # Compute the probabilistic prediction\n",
    "    return softmax(x.mm(w) + b)\n",
    "\n",
    "def accuracy(y, q):\n",
    "    return (y.argmax(axis=-1) == q.argmax(axis=-1)).float().mean()\n",
    "\n",
    "def nll(y, q):\n",
    "    return -(y * q.log()).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dn14CzFb8ws2"
   },
   "source": [
    "Note that:\n",
    "\n",
    "* Syntaxis is a bit different because input variables are tensors, not arrays\n",
    "* This time we did not need to implement the backward function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rkfFZj2V8ws2"
   },
   "outputs": [],
   "source": [
    "# Parameter initialization\n",
    "W = .1 * torch.randn(X_train_torch.size()[1], y_train_torch.size()[1])\n",
    "W.requires_grad_()\n",
    "b = torch.zeros(y_train_torch.size()[1], requires_grad=True)\n",
    "\n",
    "epochs = 500\n",
    "rho = .5\n",
    "\n",
    "loss_train = np.zeros(epochs)\n",
    "loss_val = np.zeros(epochs)\n",
    "acc_train = np.zeros(epochs)\n",
    "acc_val = np.zeros(epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NcWfg_oD8ws2",
    "outputId": "64bd5e8c-09c7-41f5-b1bd-fff892bc43b6"
   },
   "outputs": [],
   "source": [
    "# Network training\n",
    "for epoch in range(epochs):\n",
    "\n",
    "    print(f'Current epoch: {epoch + 1}  \\r', end=\"\")\n",
    "\n",
    "    # Compute network output and cross-entropy loss\n",
    "    pred = model(W, b, X_train_torch)\n",
    "    loss = nll(y_train_torch, pred)\n",
    "\n",
    "    # Compute gradients\n",
    "    loss.backward()\n",
    "\n",
    "    # Deactivate gradient automatic updates\n",
    "    with torch.no_grad():\n",
    "        #Computing network performance after iteration\n",
    "        loss_train[epoch] = loss.item()\n",
    "        acc_train[epoch] = accuracy(y_train_torch, pred).item()\n",
    "        pred_val = model(W, b, X_val_torch)\n",
    "        loss_val[epoch] = nll(y_val_torch, pred_val).item()\n",
    "        acc_val[epoch] = accuracy(y_val_torch, pred_val).item()\n",
    "\n",
    "        #Weight update\n",
    "        W -= rho * W.grad\n",
    "        b -= rho * b.grad\n",
    "\n",
    "        # Reset gradients\n",
    "        W.grad.zero_()\n",
    "        b.grad.zero_()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kqra6GDl8ws3"
   },
   "source": [
    "**It is important to deactivate gradient updates after the network has been evaluated on training data, and gradients of the loss function have been computed:**\n",
    "   - Validation data should never be used for updating the network parameters\n",
    "   - Save computation to accelerate the code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JE7jceSx8ws3",
    "outputId": "32236e71-c0ad-4978-ac91-3318b1dbc295"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(14,5))\n",
    "plt.subplot(1, 2, 1), plt.plot(loss_train, 'b'), plt.plot(loss_val, 'r'), plt.legend(['train', 'val']), plt.title('Cross-entropy loss')\n",
    "plt.subplot(1, 2, 2), plt.plot(acc_train, 'b'), plt.plot(acc_val, 'r'), plt.legend(['train', 'val']), plt.title('Accuracy')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IoNlhms_8ws3"
   },
   "source": [
    "#### <font color='olive'> 4.4.2. Using torch *nn* module </font>\n",
    "\n",
    "PyTorch *nn* module provides many attributes and methods that make the implementation and training of Neural Networks simpler\n",
    "\n",
    "* ```nn.Module``` and ```nn.Parameter``` allow to implement a more concise training loop\n",
    "\n",
    "* ```nn.Module``` is a PyTorch class that will be used to encapsulate and design a specific neural network, thus, it is central to the implementation of deep neural nets using PyTorch\n",
    "\n",
    "* ```nn.Parameter``` allow the definition of trainable network parameters. In this way, we will simplify the implementation of the training loop.\n",
    "\n",
    "* All parameters defined with ```nn.Parameter``` will have ```requires_grad = True```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YKSIrzvJ8ws3"
   },
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "\n",
    "class my_multiclass_net(nn.Module):\n",
    "    def __init__(self, nin, nout):\n",
    "        \"\"\" This method initializes the network parameters\n",
    "        Parameters nin and nout stand for the number of input parameters (features in X)\n",
    "        and output parameters (number of classes) \"\"\"\n",
    "        super().__init__()\n",
    "        self.W = nn.Parameter(.1 * torch.randn(nin, nout))\n",
    "        self.b = nn.Parameter(torch.zeros(nout))\n",
    "\n",
    "    def forward(self, x):\n",
    "        return softmax(x.mm(self.W) + self.b)\n",
    "\n",
    "    def softmax(t):\n",
    "        \"\"\"Compute softmax values for each sets of scores in t\"\"\"\n",
    "        return t.exp() / t.exp().sum(-1).unsqueeze(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XUmy6uMi8ws4",
    "outputId": "b74a91dc-0ff4-478c-8912-bbd7806597cf"
   },
   "outputs": [],
   "source": [
    "my_net = my_multiclass_net(X_train_torch.size()[1], y_train_torch.size()[1])\n",
    "\n",
    "epochs = 500\n",
    "rho = .5\n",
    "\n",
    "loss_train = np.zeros(epochs)\n",
    "loss_val = np.zeros(epochs)\n",
    "acc_train = np.zeros(epochs)\n",
    "acc_val = np.zeros(epochs)\n",
    "\n",
    "for epoch in range(epochs):\n",
    "\n",
    "    print(f'Current epoch: {epoch + 1}  \\r', end=\"\")\n",
    "\n",
    "    #Compute network output and cross-entropy loss\n",
    "    pred = my_net(X_train_torch)\n",
    "    loss = nll(y_train_torch, pred)\n",
    "\n",
    "    #Compute gradients\n",
    "    loss.backward()\n",
    "\n",
    "    #Deactivate gradient automatic updates\n",
    "    with torch.no_grad():\n",
    "        #Computing network performance after iteration\n",
    "        loss_train[epoch] = loss.item()\n",
    "        acc_train[epoch] = accuracy(y_train_torch, pred).item()\n",
    "        pred_val = my_net(X_val_torch)\n",
    "        loss_val[epoch] = nll(y_val_torch, pred_val).item()\n",
    "        acc_val[epoch] = accuracy(y_val_torch, pred_val).item()\n",
    "\n",
    "        #Weight update\n",
    "        for p in my_net.parameters():\n",
    "            p -= p.grad * rho\n",
    "        #Reset gradients\n",
    "        my_net.zero_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ABD1o6sZ8ws4",
    "outputId": "006ba07b-109c-442d-9974-2f5c336af5d9"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(14,5))\n",
    "plt.subplot(1, 2, 1), plt.plot(loss_train, 'b'), plt.plot(loss_val, 'r'), plt.legend(['train', 'val']), plt.title('Cross-entropy loss')\n",
    "plt.subplot(1, 2, 2), plt.plot(acc_train, 'b'), plt.plot(acc_val, 'r'), plt.legend(['train', 'val']), plt.title('Accuracy')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-voXBEZ18ws4"
   },
   "source": [
    "* ```nn.Module``` comes with several kinds of pre-defined layers, thus making it even simpler to implement neural networks\n",
    "\n",
    "* We can also import the Cross Entropy Loss from ```nn.Module```. When doing so:\n",
    "    - We do not have to compute the softmax, since the ```nn.CrossEntropyLoss``` already does so\n",
    "    - ```nn.CrossEntropyLoss``` receives two input arguments, the first is the output of the network, and the second is the true label as a 1-D tensor (i.e., an array of integers, one-hot encoding should not be used)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0C5WTrk18ws5"
   },
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "\n",
    "class my_multiclass_net(nn.Module):\n",
    "    def __init__(self, nin, nout):\n",
    "        \"\"\"Note that now, we do not even need to initialize network parameters ourselves\"\"\"\n",
    "        super().__init__()\n",
    "        self.lin = nn.Linear(nin, nout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.lin(x)\n",
    "\n",
    "loss_func = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DWTrHvfG8ws5",
    "outputId": "dc56cbfa-a6a0-4567-d8a9-3da4272e1dd2"
   },
   "outputs": [],
   "source": [
    "my_net = my_multiclass_net(X_train_torch.size()[1], y_train_torch.size()[1])\n",
    "\n",
    "epochs = 500\n",
    "rho = .1\n",
    "\n",
    "loss_train = np.zeros(epochs)\n",
    "loss_val = np.zeros(epochs)\n",
    "acc_train = np.zeros(epochs)\n",
    "acc_val = np.zeros(epochs)\n",
    "\n",
    "for epoch in range(epochs):\n",
    "\n",
    "    print(f'Current epoch: {epoch + 1}  \\r', end=\"\")\n",
    "\n",
    "    #Compute network output and cross-entropy loss\n",
    "    pred = my_net(X_train_torch)\n",
    "    loss = loss_func(pred, y_train_torch.argmax(axis=-1))\n",
    "\n",
    "    #Compute gradients\n",
    "    loss.backward()\n",
    "\n",
    "    #Deactivate gradient automatic updates\n",
    "    with torch.no_grad():\n",
    "        #Computing network performance after iteration\n",
    "        loss_train[epoch] = loss.item()\n",
    "        acc_train[epoch] = accuracy(y_train_torch, pred).item()\n",
    "        pred_val = my_net(X_val_torch)\n",
    "        loss_val[epoch] = loss_func(pred_val, y_val_torch.argmax(axis=-1)).item()\n",
    "        acc_val[epoch] = accuracy(y_val_torch, pred_val).item()\n",
    "\n",
    "        #Weight update\n",
    "        for p in my_net.parameters():\n",
    "            p -= p.grad * rho\n",
    "        #Reset gradients\n",
    "        my_net.zero_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IE5s99ZY8ws5",
    "outputId": "2d77fbe0-d066-4399-ed16-ca7a53f644f5"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(14,5))\n",
    "plt.subplot(1, 2, 1), plt.plot(loss_train, 'b'), plt.plot(loss_val, 'r'), plt.legend(['train', 'val']), plt.title('Cross-entropy loss')\n",
    "plt.subplot(1, 2, 2), plt.plot(acc_train, 'b'), plt.plot(acc_val, 'r'), plt.legend(['train', 'val']), plt.title('Accuracy')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mA5peFV68ws5"
   },
   "source": [
    "Note that a faster convergence is observed in this case. It is actually due to a more convenient initialization of the hidden layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PLKMqQPI8ws6"
   },
   "source": [
    "#### <font color='olive'> 4.4.3. Network Optimization </font>\n",
    "\n",
    "We cover in this subsection two different aspects about network training using PyTorch:\n",
    "\n",
    "  * Using ```torch.optim``` allows an easier and more interpretable encoding of neural network training, and opens the door to more sophisticated training algorithms\n",
    "  * Using minibatches can speed up network convergence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vwAhf1SG8ws6"
   },
   "source": [
    "`torch.optim` provides two convenient methods for neural network training:\n",
    "\n",
    "  * `opt.step()` updates all network parameters using current gradients\n",
    "  * `opt.zero_grad()` resets all network parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7OMHMCwX8ws6",
    "outputId": "9306026f-768f-4518-f68d-1174a1ad44f4"
   },
   "outputs": [],
   "source": [
    "from torch import optim\n",
    "\n",
    "my_net = my_multiclass_net(X_train_torch.size()[1], y_train_torch.size()[1])\n",
    "opt = optim.SGD(my_net.parameters(), lr=0.1)\n",
    "\n",
    "epochs = 500\n",
    "\n",
    "loss_train = np.zeros(epochs)\n",
    "loss_val = np.zeros(epochs)\n",
    "acc_train = np.zeros(epochs)\n",
    "acc_val = np.zeros(epochs)\n",
    "\n",
    "for epoch in range(epochs):\n",
    "\n",
    "    print(f'Current epoch: {epoch + 1}  \\r', end=\"\")\n",
    "\n",
    "    # Compute network output and cross-entropy loss\n",
    "    pred = my_net(X_train_torch)\n",
    "    loss = loss_func(pred, y_train_torch.argmax(axis=-1))\n",
    "\n",
    "    # Compute gradients\n",
    "    loss.backward()\n",
    "\n",
    "    # Deactivate gradient automatic updates\n",
    "    with torch.no_grad():\n",
    "        #Computing network performance after iteration\n",
    "        loss_train[epoch] = loss.item()\n",
    "        acc_train[epoch] = accuracy(y_train_torch, pred).item()\n",
    "        pred_val = my_net(X_val_torch)\n",
    "        loss_val[epoch] = loss_func(pred_val, y_val_torch.argmax(axis=-1)).item()\n",
    "        acc_val[epoch] = accuracy(y_val_torch, pred_val).item()\n",
    "\n",
    "    opt.step()\n",
    "    opt.zero_grad()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OIieTNrV8ws7"
   },
   "source": [
    "Note network optimization is carried out outside ```torch.no_grad()``` but network evaluation (other than forward output calculation for the training patterns) still need to deactivate gradient updates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cVhY2j9m8ws7",
    "outputId": "a11f80c4-aef9-4c51-b59f-57ef77a4c46b"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(14,5))\n",
    "plt.subplot(1, 2, 1), plt.plot(loss_train, 'b'), plt.plot(loss_val, 'r'), plt.legend(['train', 'val']), plt.title('Cross-entropy loss')\n",
    "plt.subplot(1, 2, 2), plt.plot(acc_train, 'b'), plt.plot(acc_val, 'r'), plt.legend(['train', 'val']), plt.title('Accuracy')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mrJ6XOUq8ws7"
   },
   "source": [
    "##### <font color='olive'> Exercise 9 </font>\n",
    "\n",
    "Implement network training with two modifications:\n",
    "\n",
    "  * Replace the SGD optimization method by the Adam algorithm. You can refer to the <a href=\"https://pytorch.org/docs/stable/optim.html\">official documentation</a> and get help on this and other methods.\n",
    "  * Implement and adaptive learning rate using `torch.optim.lr_scheduler` (for instance, you can try the <a href=\"https://pytorch.org/docs/stable/generated/torch.optim.lr_scheduler.ExponentialLR.html#torch.optim.lr_scheduler.ExponentialLR\">exponentialLR</a> scheduler)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xEdqcOuh8ws7"
   },
   "outputs": [],
   "source": [
    "# Write your solution here\n",
    "# <SOL>\n",
    "# </SOL>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vHD3lwyN8ws7"
   },
   "source": [
    "#### <font color='olive'> 4.4.4. The DataLoader method. Using SGD with minibatches </font>\n",
    "\n",
    "Each epoch of the previous implementation of network training was actually implementing Gradient Descent\n",
    "* In SGD only a *minibatch* of training patterns are used at every iteration\n",
    "* In each epoch we iterate over all training patterns sequentially selecting non-overlapping *minibatches*\n",
    "* Overall, convergence is usually faster than when using Gradient Descent\n",
    "* Torch provides methods that simplify the implementation of this strategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "unTdTVDe8ws8"
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "train_ds = TensorDataset(X_train_torch, y_train_torch)\n",
    "train_dl = DataLoader(train_ds, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0XkTYuAw8ws8",
    "outputId": "c3b731e1-c7dc-4fe9-ec96-7217dc556408"
   },
   "outputs": [],
   "source": [
    "from torch import optim\n",
    "\n",
    "my_net = my_multiclass_net(X_train_torch.size()[1], y_train_torch.size()[1])\n",
    "opt = optim.SGD(my_net.parameters(), lr=0.1)\n",
    "\n",
    "epochs = 200\n",
    "\n",
    "loss_train = np.zeros(epochs)\n",
    "loss_val = np.zeros(epochs)\n",
    "acc_train = np.zeros(epochs)\n",
    "acc_val = np.zeros(epochs)\n",
    "\n",
    "for epoch in range(epochs):\n",
    "\n",
    "    print(f'Current epoch: {epoch + 1}  \\r', end=\"\")\n",
    "\n",
    "    for xb, yb in train_dl:\n",
    "\n",
    "        #Compute network output and cross-entropy loss for current minibatch\n",
    "        pred = my_net(xb)\n",
    "        loss = loss_func(pred, yb.argmax(axis=-1))\n",
    "\n",
    "        #Compute gradients and optimize parameters\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        opt.zero_grad()\n",
    "\n",
    "    #At the end of each epoch, evaluate overall network performance\n",
    "    with torch.no_grad():\n",
    "        #Computing network performance after iteration\n",
    "        pred = my_net(X_train_torch)\n",
    "        loss_train[epoch] = loss_func(pred, y_train_torch.argmax(axis=-1)).item()\n",
    "        acc_train[epoch] = accuracy(y_train_torch, pred).item()\n",
    "        pred_val = my_net(X_val_torch)\n",
    "        loss_val[epoch] = loss_func(pred_val, y_val_torch.argmax(axis=-1)).item()\n",
    "        acc_val[epoch] = accuracy(y_val_torch, pred_val).item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xyiXcRZw8ws8",
    "outputId": "abdab8de-d13d-4585-87c8-f5f51bf9a00e"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(14,5))\n",
    "plt.subplot(1, 2, 1), plt.plot(loss_train, 'b'), plt.plot(loss_val, 'r'), plt.legend(['train', 'val']), plt.title('Cross-entropy loss')\n",
    "plt.subplot(1, 2, 2), plt.plot(acc_train, 'b'), plt.plot(acc_val, 'r'), plt.legend(['train', 'val']), plt.title('Accuracy')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gp6m4pJ88ws8"
   },
   "source": [
    "#### <font color='olive'> 4.4.4. Multi Layer networks using ```nn.Sequential``` </font>\n",
    "\n",
    "PyTorch simplifies considerably the implementation of neural network training, since we do not need to implement derivatives ourselves\n",
    "\n",
    "We can also make a simpler implementation of multilayer networks using ```nn.Sequential``` function\n",
    "\n",
    "It returns directly a network with the requested topology, including parameters **and forward evaluation method**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-MgkFgmT8ws9"
   },
   "outputs": [],
   "source": [
    "my_net = nn.Sequential(\n",
    "    nn.Linear(X_train_torch.size()[1], 200),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(200,50),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(50,20),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(20,y_train_torch.size()[1])\n",
    ")\n",
    "\n",
    "opt = optim.SGD(my_net.parameters(), lr=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-ptALc5w8ws9",
    "outputId": "282ee1ee-6fee-48f1-b48a-6794c0e422b3"
   },
   "outputs": [],
   "source": [
    "epochs = 200\n",
    "\n",
    "loss_train = np.zeros(epochs)\n",
    "loss_val = np.zeros(epochs)\n",
    "acc_train = np.zeros(epochs)\n",
    "acc_val = np.zeros(epochs)\n",
    "\n",
    "for epoch in range(epochs):\n",
    "\n",
    "    print(f'Current epoch: {epoch + 1}  \\r', end=\"\")\n",
    "\n",
    "    for xb, yb in train_dl:\n",
    "\n",
    "        #Compute network output and cross-entropy loss for current minibatch\n",
    "        pred = my_net(xb)\n",
    "        loss = loss_func(pred, yb.argmax(axis=-1))\n",
    "\n",
    "        #Compute gradients and optimize parameters\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        opt.zero_grad()\n",
    "\n",
    "    #At the end of each epoch, evaluate overall network performance\n",
    "    with torch.no_grad():\n",
    "        #Computing network performance after iteration\n",
    "        pred = my_net(X_train_torch)\n",
    "        loss_train[epoch] = loss_func(pred, y_train_torch.argmax(axis=-1)).item()\n",
    "        acc_train[epoch] = accuracy(y_train_torch, pred).item()\n",
    "        pred_val = my_net(X_val_torch)\n",
    "        loss_val[epoch] = loss_func(pred_val, y_val_torch.argmax(axis=-1)).item()\n",
    "        acc_val[epoch] = accuracy(y_val_torch, pred_val).item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pTTeq7aS8ws9",
    "outputId": "869b39e5-7ec3-4886-bb77-83b3e7e96003"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(14,5))\n",
    "plt.subplot(1, 2, 1), plt.plot(loss_train, 'b'), plt.plot(loss_val, 'r'), plt.legend(['train', 'val']), plt.title('Cross-entropy loss')\n",
    "plt.subplot(1, 2, 2), plt.plot(acc_train, 'b'), plt.plot(acc_val, 'r'), plt.legend(['train', 'val']), plt.title('Accuracy')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "i7SPAxyc8ws9",
    "outputId": "1b8b5809-7abe-4462-de85-b89d9df23f9e"
   },
   "outputs": [],
   "source": [
    "print('Validation accuracy with this net:', acc_val[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9DwC2MfD8ws-",
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "### <font color='teal'> 4.5. Generalization</font>\n",
    "\n",
    "As the number of neurons and parameters in a neural network model grows, the training process can incur in **over-fitting** issues: the model learns to perform exceptionally well on the training data but fails to **generalize** effectively to new, unseen data: the models captures noise and specific details in the training data, rather than the underlying patterns. Overfit models have poor predictive performance on data they haven't seen before because they essentially memorize the training data rather than learning the true relationships. \n",
    "\n",
    "A standard procedure to avoid overfiting is cross validation. We can train neural network configurations with different complexity, and select the most appropriate using a validation set, of through $n$-fold cross validation. However, the number of possible configurations (number of layers, neurons per layer, etc) may be too large, and combining cross-validation with other techniques is usually more inefficient. These are some of them:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  - **Regularization by a penalty term**: the loss function is extended with an additional term penalizing large weights. Two common examples of regularization for an empirical risk $R({\\bf w})$ are :\n",
    "       - L2 regularization:  $R({\\bf w}) + \\lambda \\|{\\bf w}\\|^2$\n",
    "       - L1 regularization:  $R({\\bf w}) + \\lambda \\|{\\bf w}\\|_1$\n",
    "  - **Early stopping**: It involves monitoring the model's performance on a validation dataset during training. If the validation performance starts to degrade (e.g., validation loss increases), training is stopped early to avoid overfitting and save the model with the best performance on the validation data.\n",
    "  - **Dropout regularization**: During training, dropout randomly \"drops out\" (deactivates) a fraction of neurons or units in a layer, preventing co-adaptation of neurons and reducing overfitting. It helps the model generalize better by making it more robust and less reliant on specific neurons during prediction. During inference, dropout is typically turned off, and all neurons are used.\n",
    "      \n",
    "\n",
    "<center><a href=\"https://medium.com/analytics-vidhya/a-simple-introduction-to-dropout-regularization-with-code-5279489dda1e\"><img src=\"figures/dropout.png\" width=\"450\"/>Image Source</a></center>\n",
    "\n",
    "* **Data augmentation**: it involves applying random transformations to the training data to create additional training examples. These transformations can include operations like rotation, scaling, cropping, flipping, brightness adjustments, and more. By exposing the network to a wider variety of training examples, data augmentation helps the model learn to be invariant to these transformations and generalize better to unseen data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ogWKvwsY8ws-"
   },
   "source": [
    "### <font color='teal'> 4.6. Convolutional Networks for Image Processing </font>\n",
    "\n",
    "A Convolutional Neural Network (CNN or ConvNet) is a class of deep neural networks that is particularly effective at processing grid-like data, such as images and videos. A typical structure of a CNN is shown in the figure:\n",
    "\n",
    "<center><a href=\"https://pytorch.org/tutorials/beginner/blitz/neural_networks_tutorial.html\"><img src=\"figures/CNN.png\" width=\"800\"/>Image Source</a></center>\n",
    "\n",
    "The key components and operations in a CNN are:\n",
    "\n",
    "1. **Convolutional Layers**: These layers apply convolution operations to the input data. Convolution involves sliding a small filter (also known as a kernel) over the input to extract local patterns and features. The network learns to detect various features like edges, corners, and textures.\n",
    "\n",
    "2. **Pooling Layers**: Pooling layers downsample the output from the convolutional layers, by combining several inputs into a single one. Common pooling operations include **max-pooling** and **average-pooling**, which reduce the spatial dimensions and retain the most important information.\n",
    "\n",
    "3. **Fully Connected Layers**: After a series of convolutional and pooling layers, one or more fully connected layers are typically used to make final predictions. These layers are similar to those in an MLP and can learn complex, global patterns and relationships in the data.\n",
    "\n",
    "4. **Activation Functions**: At the end of each layer, to introduce non-linearities, as in an MLP."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "CNNs have several advantages for tasks involving grid-like data:\n",
    "\n",
    "- **Parameter Sharing**: Convolutional layers use the same set of weights (filters) to scan the entire input, which enables them to detect the same features in different parts of the image. This parameter sharing makes CNNs computationally efficient and effective.\n",
    "\n",
    "- **Translation Invariance**: CNNs are capable of recognizing patterns regardless of their position in the input. This property is crucial for tasks like image recognition, where the position of an object may vary.\n",
    "\n",
    "CNNs have revolutionized computer vision and are widely used in various applications, including image classification, object detection, facial recognition, medical image analysis, and more. They have also been applied to non-image data with grid-like structures, such as text data and time-series data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "PyTorch include facilities for the implementation of CNN. You can see an example below:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OWfcCxqa8ws-"
   },
   "outputs": [],
   "source": [
    "dataset = 'digits'\n",
    "\n",
    "#Generate train and validation data, shuffle\n",
    "X_train, X_val, y_train, y_val = train_test_split(digitsX[:,np.newaxis,:,:], digitsY, test_size=0.2, random_state=42, shuffle=True)\n",
    "\n",
    "#Convert to Torch tensors\n",
    "X_train_torch = torch.from_numpy(X_train)\n",
    "X_val_torch = torch.from_numpy(X_val)\n",
    "y_train_torch = torch.from_numpy(y_train)\n",
    "y_val_torch = torch.from_numpy(y_val)\n",
    "\n",
    "train_ds = TensorDataset(X_train_torch, y_train_torch)\n",
    "train_dl = DataLoader(train_ds, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dhjL2nBq8ws-"
   },
   "outputs": [],
   "source": [
    "class Lambda(nn.Module):\n",
    "    def __init__(self, func):\n",
    "        super().__init__()\n",
    "        self.func = func\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.func(x)\n",
    "\n",
    "my_net = nn.Sequential(\n",
    "    nn.Conv2d(1, 16, kernel_size=3, stride=2, padding=1),\n",
    "    nn.ReLU(),\n",
    "    nn.Conv2d(16, 16, kernel_size=3, stride=2, padding=1),\n",
    "    nn.ReLU(),\n",
    "    nn.Conv2d(16, 10, kernel_size=3, stride=2, padding=1),\n",
    "    nn.ReLU(),\n",
    "    nn.AvgPool2d(4),\n",
    "    Lambda(lambda x: x.view(x.size(0), -1)),\n",
    ")\n",
    "\n",
    "opt = optim.SGD(my_net.parameters(), lr=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-reuau2f8ws_",
    "outputId": "859b063e-b287-4f1f-c0e5-13fed9046d9c"
   },
   "outputs": [],
   "source": [
    "epochs = 2500\n",
    "\n",
    "loss_train = np.zeros(epochs)\n",
    "loss_val = np.zeros(epochs)\n",
    "acc_train = np.zeros(epochs)\n",
    "acc_val = np.zeros(epochs)\n",
    "\n",
    "for epoch in range(epochs):\n",
    "\n",
    "    print(f'Número de épocas: {epoch + 1}\\r', end=\"\")\n",
    "\n",
    "    for xb, yb in train_dl:\n",
    "\n",
    "        #Compute network output and cross-entropy loss for current minibatch\n",
    "        pred = my_net(xb)\n",
    "        loss = loss_func(pred, yb.argmax(axis=-1))\n",
    "\n",
    "        #Compute gradients and optimize parameters\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        opt.zero_grad()\n",
    "\n",
    "    #At the end of each epoch, evaluate overall network performance\n",
    "    with torch.no_grad():\n",
    "        # Computing network performance after iteration\n",
    "        pred = my_net(X_train_torch)\n",
    "        loss_train[epoch] = loss_func(pred, y_train_torch.argmax(axis=-1)).item()\n",
    "        acc_train[epoch] = accuracy(y_train_torch, pred).item()\n",
    "        pred_val = my_net(X_val_torch)\n",
    "        loss_val[epoch] = loss_func(pred_val, y_val_torch.argmax(axis=-1)).item()\n",
    "        acc_val[epoch] = accuracy(y_val_torch, pred_val).item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "B-7gZAFB8ws_",
    "outputId": "b4eafa32-2364-4c15-ff11-237fc22294dc"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(14,5))\n",
    "plt.subplot(1, 2, 1), plt.plot(loss_train, 'b'), plt.plot(loss_val, 'r'), plt.legend(['train', 'val']), plt.title('Cross-entropy loss')\n",
    "plt.subplot(1, 2, 2), plt.plot(acc_train, 'b'), plt.plot(acc_val, 'r'), plt.legend(['train', 'val']), plt.title('Accuracy')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pLXRBIAF8ws_"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
