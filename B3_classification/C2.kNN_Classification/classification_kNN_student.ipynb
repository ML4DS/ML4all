{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "# The $k$-Nearest Neighbor Classification Algorithm\n",
    "\n",
    "    Notebook version: 2.3 (Sep 17, 2024)\n",
    "\n",
    "    Author: Jesús Cid Sueiro (jcid@tsc.uc3m.es)\n",
    "            Jerónimo Arenas García (jarenas@tsc.uc3m.es)\n",
    "            \n",
    "    Changes: v.1.0 - First version\n",
    "             v.1.1 - Function loadDataset updated to work with any number of dimensions\n",
    "             v.2.0 - Compatible with Python 3 (backcompatible with Python 2.7)\n",
    "                     Added solution to Exercise 3\n",
    "             v.2.1 - Minor corrections regarding notation\n",
    "             v.2.2 - Adaptation for slides conversion\n",
    "             v.2.3 - (JCS) Removed implementation of knn from scratch. sklearn is used.\n",
    "                     KNN and cross validation classes from sklearn are used.\n",
    "                     Updated code.\n",
    "             "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# To visualize plots in the notebook\n",
    "%matplotlib inline \n",
    "\n",
    "# Import some libraries that will be necessary for working with data and displaying plots\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import train_test_split, KFold, cross_val_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn import neighbors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 1. The binary classification problem.\n",
    "\n",
    "In a binary classification problem, we are given an observation vector ${\\bf x}\\in \\mathbb{R}^N$ which is known to belong to one and only one *category* or *class*, $y$, in the set ${\\mathcal Y} = \\{0, 1\\}$. The goal of a classifier system is to predict the value of $y$ based on ${\\bf x}$.\n",
    "\n",
    "To design the classifier, we are given a collection of labelled observations ${\\mathcal D} = \\{({\\bf x}_k, y_k)\\}_{k=0}^{K-1}$ where, for each observation ${\\bf x}_k$, the value of its true category, $y_k$, is known. All samples are outcomes of an unknown distribution $p_{{\\bf X},Y}({\\bf x}, y)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 2. The Iris dataset\n",
    "\n",
    "(Iris dataset presentation is based on this <a href=http://machinelearningmastery.com/tutorial-to-implement-k-nearest-neighbors-in-python-from-scratch/> Tutorial </a> by <a href=http://machinelearningmastery.com/about/> Jason Brownlee</a>) \n",
    "\n",
    "To illustrate the algorithms, we will consider the <a href = http://archive.ics.uci.edu/ml/datasets/Iris> Iris dataset </a>, taken from the <a href=http://archive.ics.uci.edu/ml/> UCI Machine Learning repository </a>. Quoted from the dataset description:\n",
    "\n",
    "> This is perhaps the best known database to be found in the pattern recognition literature. The data set contains 3 classes of 50 instances each, where each class refers to a type of iris plant. [...] One class is linearly separable from the other 2; the latter are NOT linearly separable from each other. \n",
    "\n",
    "The *class* is the species, which is one of *setosa*, *versicolor* or *virginica*. Each instance contains 4 measurements of given flowers: sepal length, sepal width, petal length and petal width, all in centimeters. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "df_data = pd.read_csv('datasets/iris.data', header=None)\n",
    "df_data.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can inspect some basic statistics of the data features using the `describe` method from the `pandas` package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_data.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will separate the features and the target categories into a feature matrix and a list of class labels. Labels will be mapped to integers to facilitate the treatment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the features and the classes from the dataframe\n",
    "X = df_data.iloc[:, 0:4].values\n",
    "C = df_data.iloc[:, 4].values\n",
    "\n",
    "# Map the classes to integers\n",
    "Y = LabelEncoder().fit_transform(C)\n",
    "# Get a dictionary mapping from integers to classes\n",
    "int_to_class = dict(zip(Y, C))\n",
    "# Get a dictionary mapping from classes to integers\n",
    "class_to_int = dict(zip(C, Y))\n",
    "\n",
    "n_clases = len(int_to_class)\n",
    "n_samples, n_features = X.shape\n",
    "\n",
    "# Show the data dimensions:\n",
    "print(f\"-- The feature matrix contains {n_samples} samples and {n_features} features.\")\n",
    "\n",
    "# Pretty print the mapping\n",
    "print(\"-- The following mapping from integers to classes will be used:\")\n",
    "for k, v in int_to_class.items():\n",
    "    print(f\"     {k} -> {v}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "###  2.1. Training and validation\n",
    "\n",
    "Next, we will split the data into two sets:\n",
    "\n",
    "* **Training set**, that will be used to learn the classification model\n",
    "* **Test set**, that will be used to evaluate the classification performance\n",
    "\n",
    "To make the partition, we can use the `train_test_split` method from `sklearn`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The code fragment below defines a function `loadDataset` that loads the data in a CSV with the provided filename, converts the flower measures (that were loaded as strings) into numbers and, finally, it splits the data into a training and test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# Split the dataset into training and testing sets\n",
    "X_trn_all, X_tst_all, Y_trn_all, Y_tst_all = train_test_split(\n",
    "    X, Y, test_size=0.33, random_state=42)\n",
    "\n",
    "# Dataset sizes\n",
    "n_trn = len(Y_trn_all)\n",
    "n_tst = len(Y_tst_all)\n",
    "\n",
    "# Show the data dimensions:\n",
    "print(f\"-- {n_trn} training samples.\")\n",
    "print(f\"-- {n_tst} test samples.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 2.2. Versicolor vs Virginica\n",
    "\n",
    "In the following, we will design a classifier to separate classes \"Versicolor\" and \"Virginica\" using $x_0$ and $x_1$ only. To do so, we build a training set with samples from these categories, and a bynary label $y^{(k)} = 1$ for samples in class \"Virginica\", and $0$ for \"Versicolor\" data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# Select two classes\n",
    "ignored_class = 'Iris-setosa'\n",
    "c0 = 'Iris-versicolor' \n",
    "c1 = 'Iris-virginica'\n",
    "# Select two features\n",
    "ind = [0, 1]\n",
    "\n",
    "# New map indices to classes\n",
    "int_to_class_2 = {0: c0, 1: c1}\n",
    "class_to_int_2 = {c0: 0, c1: 1}\n",
    "\n",
    "# Select samples\n",
    "X_trn = X_trn_all[Y_trn_all != class_to_int[ignored_class], :]\n",
    "X_tst = X_tst_all[Y_tst_all != class_to_int[ignored_class], :]\n",
    "# Select features\n",
    "X_trn = X_trn[:, ind]\n",
    "X_tst = X_tst[:, ind]\n",
    "\n",
    "# Select labels for the selected features\n",
    "Y_trn = Y_trn_all[Y_trn_all != class_to_int[ignored_class]]\n",
    "Y_tst = Y_tst_all[Y_tst_all != class_to_int[ignored_class]]\n",
    "\n",
    "# Map to new class indices\n",
    "Y_trn = Y_trn == class_to_int[c1]\n",
    "Y_tst = Y_tst == class_to_int[c1]\n",
    "\n",
    "# Dataset sizes\n",
    "n_trn = len(Y_trn)\n",
    "n_tst = len(Y_tst)\n",
    "# Show the data dimensions:\n",
    "print(f\"-- {n_trn} training samples.\")\n",
    "print(f\"-- {n_tst} test samples.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "A scatter plot is useful to get some insights on the difficulty of the classification problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(9, 3))\n",
    "for i in [0, 1]:\n",
    "    # Plot the samples of class i\n",
    "    plt.scatter(X_trn[Y_trn == i, 0], X_trn[Y_trn == i, 1],\n",
    "                label=int_to_class_2[i])\n",
    "plt.axis('equal')\n",
    "plt.xlabel(f'Feature {ind[0]}')\n",
    "plt.ylabel(f'Feature {ind[1]}')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 3. Baseline Classifier: Maximum A Priori.\n",
    "\n",
    "For the selected data set, we have two clases and a dataset with the following class proportions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "print(f'Class 0 ({c0}): {n_trn - sum(Y_trn)} samples')\n",
    "print(f'Class 1 ({c1}): {sum(Y_trn)} samples')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "The maximum a priori classifier assigns any sample ${\\bf x}$ to the most frequent class in the training set. Therefore, the class prediction $y$ for any sample ${\\bf x}$ is"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "y = int(2*sum(Y_trn) > n_trn)\n",
    "print(f'y = {y} ({c1 if y==1 else c0})')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The error rate for this baseline classifier is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Training and test error arrays\n",
    "E_trn = (Y_trn != y)\n",
    "E_tst = (Y_tst != y)\n",
    "\n",
    "# Error rates\n",
    "pe_trn = float(sum(E_trn)) / n_trn\n",
    "pe_tst = float(sum(E_tst)) / n_tst\n",
    "print('Pe(train):', str(pe_trn))\n",
    "print('Pe(test): ', str(pe_tst))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "The error rate of the baseline classifier is a simple benchmark for classification. Since the maximum a priori decision is independent on the observation, ${\\bf x}$, any classifier based on ${\\bf x}$ should have a better (or, at least, not worse) performance than the baseline classifier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 4. The Nearest-Neighbour Classifier (1-NN).\n",
    "\n",
    "\n",
    "The 1-NN classifier assigns any instance ${\\bf x}$ to the category of the nearest neighbor in the training set.\n",
    "$$\n",
    "d = f({\\bf x}) = y_n, {\\rm~where} \\\\\n",
    "n = \\arg \\min_k \\|{\\bf x}-{\\bf x}_k\\|\n",
    "$$\n",
    "In case of ties (i.e. if there is more than one instance at minimum distance) the class of one of them, taken arbitrarily, is assigned to ${\\bf x}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Let us apply the 1-NN classifier to the given dataset. First, we will show the decision regions of the classifier. To do so, we compute the classifier output for all points in a rectangular grid from the sample space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# Create a regtangular grid.\n",
    "n_points = 200\n",
    "x_min, x_max = X_trn[:, 0].min(), X_trn[:, 0].max() \n",
    "y_min, y_max = X_trn[:, 1].min(), X_trn[:, 1].max()\n",
    "dx = x_max - x_min\n",
    "dy = y_max - y_min\n",
    "h = dy / n_points\n",
    "xx, yy = np.meshgrid(np.arange(x_min - 0.1 * dx, x_max + 0.1 * dx, h),\n",
    "                     np.arange(y_min - 0.1 * dx, y_max + 0.1 * dy, h))\n",
    "X_grid = np.array([xx.ravel(), yy.ravel()]).T\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To apply the 1-NN classifier, we will use the `KNeigborsClassifier` class from `sklearn`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the classifier object\n",
    "clf = neighbors.KNeighborsClassifier(1, weights='uniform')\n",
    "# Train the classifier\n",
    "clf.fit(X_trn, Y_trn)\n",
    "# Predict the class of the samples in the grid\n",
    "Z = clf.predict(X_grid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the data are two-dimensional, we can visualize the decision boundary by coloring the grid according to the predicted class. The following code does this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# Put the result into a color plot\n",
    "plt.plot(X_trn[Y_trn==0, 0], X_trn[Y_trn==0, 1],'r.', label=c0)\n",
    "plt.plot(X_trn[Y_trn==1, 0], X_trn[Y_trn==1, 1],'g+', label=c1)\n",
    "plt.xlabel('$x_' + str(ind[0]) + '$')\n",
    "plt.ylabel('$x_' + str(ind[1]) + '$')\n",
    "plt.legend(loc='best')\n",
    "\n",
    "Z = Z.reshape(xx.shape)\n",
    "plt.contourf(xx, yy, Z)\n",
    "plt.axis('equal')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "We can observe that the decision boudary of the 1-NN classifier is rather intricate, and it may contain small *islands* covering one or few samples from one class. Actually, the extension of these small regions usually reduces as we have more training samples, though the number of them may increase.\n",
    "\n",
    "Now we compute the error rates over the training and test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Predicions for the training and test inputs\n",
    "Z_trn = clf.predict(X_trn)\n",
    "Z_tst = clf.predict(X_tst)\n",
    "\n",
    "# Error rates\n",
    "pe_trn = np.mean(Z_trn != Y_trn)\n",
    "pe_tst = np.mean(Z_tst != Y_tst)\n",
    "\n",
    "print(f'Pe(train): {pe_trn:.4f}')\n",
    "print(f'Pe(test):  {pe_tst:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The training and test error rates of the 1-NN may be significantly different. In fact, the training error may go down to zero if samples do not overlap. In the selected problem, this is not the case, because samples from different classes coincide at the same point, causing some classification errors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### 4.1. Consistency of the 1-NN classifier\n",
    "\n",
    "Despite the 1-NN usually reduces the error rate with respect to the baseline classifier, the number of errors may be too large. Errors may be attributed to diferent causes:\n",
    "\n",
    "   1. The class distributions are overlapped, because the selected features have no complete information for discriminating between the classes: this would imply that, even the best possible classifier would be prone to errors.\n",
    "   2. The training sample is small, and it is not enough to obtaing a good estimate of the optimal classifiers.\n",
    "   3. The classifier has intrinsic limitations: even though we had an infinite number of samples, the classifier performance does not approach the optimal classifiers.\n",
    "\n",
    "In general, a classifier is said to be consistent if it makes nearly optimal decisions as the number of training samples increases. Actually, it can be shown that this is the case of the 1-NN classifier if the classification problem is separable, i.e. if there exist a decision boundary with zero error probability. Unfortunately, in a non-separable case, the 1-NN classifier is not consistent. It can be shown that the error rate of the 1-NN classifier converges to an error rate which is not worse than twice the minimum attainable error rate (Bayes error rate) as the number of training samples goes to infinity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**Exercise 1**: In this exercise we test the non-consistency of the 1-NN classifier for overlapping distributions. Generate an artificial dataset for classification as follows:\n",
    "\n",
    "- Generate $N$ binary labels at random with values '0' and '1'. Store them in vector ${\\bf y}$\n",
    "- For every label $y_k$ in ${\\bf y}$:\n",
    "    - If the label is 0, take sample $x_k$ at random from a uniform distribution $U(0,2)$.\n",
    "    - If the label is 1, take sample $x_k$ at random from a uniform distribution $U(1,5)$.\n",
    "\n",
    "Take $N=1000$ for the test set. This is a large sample to get accurate error rate estimates. Also, take $N=10$, $20$, $40$, $80$,... for the training set. Compute the 1-NN classifier, and observe the test error rate as a function of $N$. \n",
    "\n",
    "Now, compute the test error rate of the classifier making decision $1$ if $x_k>1.5$, and $0$ otherwise. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# <SOL>\n",
    "# </SOL>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 5. $k$-NN classifier\n",
    "\n",
    "A simple extension of the 1-NN classifier is the $k$-NN classifier, which, for any input sample ${\\bf x}$, computes the $k$ closest neighbors in the training set, and takes the majority class in the subset. To avoid ties, in the binary classification case $k$ is usually taken as an odd number.\n",
    "\n",
    "The following method implements the $k$-NN classifiers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can plot the decision boundaries for different values of $k$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 15\n",
    "# Create the classifier object\n",
    "clf = neighbors.KNeighborsClassifier(k, weights='uniform')\n",
    "# Train the classifier\n",
    "clf.fit(X_trn, Y_trn)\n",
    "\n",
    "# Compute the classifier output for all samples in the grid.\n",
    "Z = clf.predict(X_grid)\n",
    "\n",
    "# Put the result into a color plot\n",
    "plt.plot(X_trn[Y_trn==0, 0], X_trn[Y_trn==0, 1],'r.', label=c0)\n",
    "plt.plot(X_trn[Y_trn==1, 0], X_trn[Y_trn==1, 1],'g+', label=c1)\n",
    "plt.xlabel('$x_' + str(ind[0]) + '$')\n",
    "plt.ylabel('$x_' + str(ind[1]) + '$')\n",
    "plt.legend(loc='best')\n",
    "\n",
    "Z = Z.reshape(xx.shape)\n",
    "plt.contourf(xx, yy, Z)\n",
    "plt.axis('equal')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### 5.1. Influence of $k$\n",
    "\n",
    "To find a good value of $k$, we can apply a validation procedure. Since the dataset is too small, we will use the test set as a validation set.\n",
    "\n",
    "(Note that this implies that we will have no data to make a reliable evaluation of the final model performance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training and validation error as a function of parameter k.\n",
    "pe_trn = []\n",
    "pe_val = []\n",
    "k_list = [2*n+1 for n in range(int(np.floor(n_trn/2)))]\n",
    "\n",
    "for k in k_list:\n",
    "    clf = neighbors.KNeighborsClassifier(k, weights='uniform')\n",
    "    clf.fit(X_trn, Y_trn)\n",
    "\n",
    "    # Training and test predictions\n",
    "    Z_trn = clf.predict(X_trn)\n",
    "    Z_val = clf.predict(X_tst)\n",
    "\n",
    "    # Error rates\n",
    "    pe_trn.append(np.mean(Z_trn != Y_trn))\n",
    "    pe_val.append(np.mean(Z_val != Y_tst))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# Draw the result into a color plot\n",
    "markerline, stemlines, baseline = plt.stem(k_list, pe_trn, 'r', markerfmt='s', label='Training')\n",
    "plt.plot(k_list, pe_trn,'r:')\n",
    "plt.setp(markerline, 'markerfacecolor', 'r', )\n",
    "plt.setp(baseline, 'color','r', 'linewidth', 2)\n",
    "\n",
    "markerline, stemlines, baseline = plt.stem(k_list, pe_val, label='Validation')\n",
    "plt.plot(k_list, pe_val,'b:')\n",
    "plt.xlabel('$k$')\n",
    "plt.ylabel('Error rate')\n",
    "plt.legend(loc='best')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Exercise 2**: Observe the train and test error for large $k$. Could you relate the error rate of the baseline classifier with that to the $k$-NN for certain value of $k$? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "The figure above suggests that the optimal value of $k$ is"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "i = np.argmin(pe_tst)\n",
    "k_opt = k_list[i]\n",
    "print('k_opt:', k_opt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "However, using the test set to select the optimal value of the hyperparameter $k$ is not allowed. Instead, we should recur to cross validation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 5.2 Hyperparameter selection via cross-validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Since using the test labels during the training phase is not recommended (they should be kept aside to simultate the future application of the classification model on unseen patterns), we need to figure out some way to improve our estimation of the hyperparameter that requires only training data. Cross-validation allows us to do so by following the following steps:\n",
    "\n",
    "   1. **Split the training data** into several (generally non-overlapping) subsets. If we use $M$ subsets, the method is referred to as $M$-fold cross-validation. If we consider each pattern a different subset, the method is usually referred to as leave-one-out (LOO) cross-validation.\n",
    "   2. **Train** of the system $M$ times. For each run, use a different partition as a *validation* set, and use the restating partitions as the training set. **Evaluate** the performance for different choices of the hyperparameter (i.e., for different values of $k$ for the $k$-NN method).\n",
    "   3. **Average the validation error** over all partitions, and pick the hyperparameter that provided the minimum validation error.\n",
    "   4. **Rerun** the algorithm using all the training data, keeping the value of the parameter that came out of the cross-validation process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters:\n",
    "k_max = 20  # Maximum value of the k-nn hyperparameter to explore\n",
    "n_splits = 5  # Number of folds in the cross-validation\n",
    "# List of all odd integers from 1 to k_max\n",
    "k_list = [2 * j + 1 for j in range(k_max // 2)]\n",
    "\n",
    "# Now we create the KFold object with 5 folds\n",
    "kf = KFold(n_splits=n_splits)\n",
    "\n",
    "# Obtain the validation errors for each fold\n",
    "pe_trn = np.zeros((len(k_list)))\n",
    "pe_val = np.zeros((len(k_list)))\n",
    "for i, (trn_index, val_index) in enumerate(kf.split(X_trn, Y_trn)):\n",
    "    print(f\"Fold {i}:  \\r\", end=\"\", flush=True)\n",
    "\n",
    "    # Compute the Pe, for this fold, for each k\n",
    "    for i, k in enumerate(k_list):\n",
    "        clfk = neighbors.KNeighborsClassifier(k, weights='uniform')\n",
    "        clfk.fit(X_trn[trn_index, :], Y_trn[trn_index])\n",
    "        pe_trn[i] += np.mean(Y_trn[trn_index] != clfk.predict(X_trn[trn_index, :])) \n",
    "        pe_val[i] += np.mean(Y_trn[val_index] != clfk.predict(X_trn[val_index, :]))\n",
    "\n",
    "pe_trn = pe_trn / n_splits\n",
    "pe_val = pe_val / n_splits\n",
    "\n",
    "# Select the best k based on the validation error\n",
    "i_best = np.argmin(pe_val) + 1\n",
    "k_best = k_list[i_best]\n",
    "print(f'-- Best value of k: {k_best}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# We compute now the train and test errors curves\n",
    "k_opt = k_list[np.argmin(pe_val)]\n",
    "\n",
    "# Train the classifier and get predictions for ytest\n",
    "clf = neighbors.KNeighborsClassifier(k_opt, weights='uniform')\n",
    "clf.fit(X_trn, Y_trn)  \n",
    "pe_tst = np.mean(Y_tst != clf.predict(X_tst))\n",
    "\n",
    "print(len(pe_trn))\n",
    "print(len(k_list))\n",
    "print(pe_tst)\n",
    "\n",
    "plt.plot(k_list, pe_trn,'b--o',label='Training error')\n",
    "plt.plot(k_list, pe_val.T,'g--o',label='Validation error')\n",
    "plt.stem([k_opt], [pe_tst],'r-o',label='Test error')\n",
    "plt.legend(loc='best')\n",
    "plt.title('The optimal value of $k$ is ' + str(k_opt))\n",
    "plt.xlabel('$k$')\n",
    "plt.ylabel('Error rate')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross validation using cross_val_score\n",
    "\n",
    "Note that there is a simple way to apply cross validation withouth looping explicitely over the data particions. This is done internally by the `cross_val_score` methods, that returns the scores for all elements in the partition.\n",
    "\n",
    "The scoring method can be introduced as a parameter. We can use the accuracy, which is the opposite of the error rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters:\n",
    "k_max = 20  # Maximum value of the k-nn hyperparameter to explore\n",
    "n_splits = 5  # Number of folds in the cross-validation\n",
    "\n",
    "# List of all odd integers from 1 to k_max\n",
    "k_list = [2 * j + 1 for j in range(k_max // 2)]\n",
    "\n",
    "# Obtain the validation errors for each fold\n",
    "pe_val = np.zeros((len(k_list)))\n",
    "\n",
    "# Compute the Pe, for this fold, for each k\n",
    "for i, k in enumerate(k_list):\n",
    "    clfk = neighbors.KNeighborsClassifier(k, weights='uniform')\n",
    "    scores = cross_val_score(clfk, X_trn, Y_trn, cv=n_splits, scoring='accuracy')\n",
    "    pe_val[i] = 1 - np.mean(scores)\n",
    "\n",
    "# Select the best k based on the validation error\n",
    "i_best = np.argmin(pe_val) + 1\n",
    "k_best = k_list[i_best]\n",
    "print(f'-- Best value of k: {k_best}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<a href = http://scikit-learn.org/stable/auto_examples/neighbors/plot_classification.html> Here</a> you can find an example of the application of `KNeighborsClassifier` to the complete 3-class Iris flower classification problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 6. $k$-NN Classification and Probability Estimation.\n",
    "\n",
    "If a sample ${\\bf x}$ has $m$ neighbors from class 1 and $k-m$ neighbors from class $0$, we can estimate the **posterior probability** that an observation ${\\bf x}$ belongs to class 1 as\n",
    "$$\n",
    "\\hat P\\{y=1|x\\} = \\frac{m}{k}\n",
    "$$\n",
    "Therefore, besides computing a decision about the class of the data, we can modify the $k$-NN algorithm to obtain posterior probability estimates."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Note that the above equation is equivalent to\n",
    "$$\n",
    "\\hat P\\{y=1|x\\} = \\frac{\\sum_{n \\in {\\mathcal N}({\\bf x})} y^{(n)}}{k}\n",
    "$$\n",
    "where ${\\mathcal N}({\\bf x})$ is the set of indices for the samples in the neighborhood of $\\bf x$.\n",
    "\n",
    "In other words, $\\hat P\\{y=1|x\\}$ is the *average* of the neighbor labels. This is actually what the $k$-NN regression algorithm does. Thus, we can estimate the posterior using the `sklearn` regression methods from `KNeighborsRegressor`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**Exercise 3**: Plot a $k$-NN posterior probability map for the Iris flower data, for $k=15$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "#<SOL>\n",
    "#</SOL>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#<SOL>\n",
    "#</SOL>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
