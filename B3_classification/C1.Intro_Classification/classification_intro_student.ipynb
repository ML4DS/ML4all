{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "# Introduction to Classification.\n",
    "\n",
    "    Notebook version: 2.4 (Sep 17, 2024)\n",
    "\n",
    "    Author: Jesús Cid Sueiro (jcid@tsc.uc3m.es)\n",
    "            Jerónimo Arenas García (jarenas@tsc.uc3m.es)\n",
    "            \n",
    "    Changes: v.1.0 - First version. Extracted from a former notebook on K-NN\n",
    "             v.2.0 - Adapted to Python 3.0 (backcompatible with Python 2.7)\n",
    "             v.2.1 - Minor corrections affecting the notation and assumptions\n",
    "             v.2.2 - Updated index notation\n",
    "             v.2.3 - Adaptation to slides conversion\n",
    "             v.2.4 - (JCS) Use of pandas. Updated python code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib inline \n",
    "\n",
    "# Import some libraries that will be necessary for working with data and displaying plots\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 1. The Classification problem\n",
    "\n",
    "In a generic classification problem, we are given an **observation vector** ${\\bf x}\\in \\mathbb{R}^N$ which is known to belong to one and only one **category** or **class**, $y$, from the set ${\\mathcal Y} = \\{0, 1, \\ldots, M-1\\}$. \n",
    "\n",
    "The goal of a classifier system is to **predict** $y$ based on ${\\bf x}$.\n",
    "\n",
    "To design the classifier, we are given a collection of labelled observations ${\\mathcal D} = \\{({\\bf x}_k, y_k)\\}_{k=0}^{K-1}$ where, for each observation ${\\bf x}_k$, the value of its true category, $y_k$, is known."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### 1.1. Binary Classification\n",
    "\n",
    "We will focus mostly in binary classification problems, where the label set is binary, ${\\mathcal Y} = \\{0, 1\\}$. \n",
    "\n",
    "Despite its simplicity, this is the most frequent case. Many multi-class classification problems are usually solved by decomposing them into a collection of binary problems."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### 1.2. The independence and identical distribution (i.i.d.) assumption.\n",
    "\n",
    "The classification algorithms, as many other machine learning algorithms, are based on two major underlying hypothesis:\n",
    "\n",
    "   - **Independence**: All samples are statistically independent.\n",
    "   - **Identical distribution**: All samples in dataset ${\\mathcal D}$ have been generated by the same distribution $p_{{\\bf X}, Y}({\\bf x}, y)$.\n",
    "   \n",
    "The i.i.d. assumption is essential to guarantee that a classifier based on ${\\mathcal D}$ has a good perfomance when applied to new input samples. \n",
    "\n",
    "The **underlying distribution is unknown** (if we knew it, we could apply classic decision theory to make optimal predictions). This is why we need the data in ${\\mathcal D}$ to design the classifier. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 2. A simple classification problem: the Iris dataset\n",
    "\n",
    "(Iris dataset presentation is based on this <a href=http://machinelearningmastery.com/tutorial-to-implement-k-nearest-neighbors-in-python-from-scratch/> Tutorial </a> by <a href=http://machinelearningmastery.com/about/> Jason Brownlee</a>) \n",
    "\n",
    "As an illustration, consider the <a href = http://archive.ics.uci.edu/ml/datasets/Iris> Iris dataset </a>, taken from the <a href=http://archive.ics.uci.edu/ml/> UCI Machine Learning repository </a>. Quoted from the dataset description:\n",
    "\n",
    "> *This is perhaps the best known database to be found in the pattern recognition literature. The data set contains 3 classes of 50 instances each, where each class refers to a type of iris plant. [...] One class is linearly separable from the other 2; the latter are NOT linearly separable from each other.* \n",
    "\n",
    "The *class* is the species, which is one of *setosa*, *versicolor* or *virginica*. Each instance contains 4 measurements of given flowers: sepal length, sepal width, petal length and petal width, all in centimeters.  The last column shows the class of each instance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "df_data = pd.read_csv('datasets/iris.data', header=None)\n",
    "df_data.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can inspect some basic statistics of the data features using the `describe` method from the `pandas` package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_data.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will separate the features and the target categories into a feature matrix and a list of class labels. Labels will be mapped to integers to facilitate the treatment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the features and the classes from the dataframe\n",
    "X = df_data.iloc[:, 0:4].values\n",
    "C = df_data.iloc[:, 4].values\n",
    "\n",
    "# Map the classes to integers\n",
    "Y = LabelEncoder().fit_transform(C)\n",
    "# Get a dictionary mapping from integers to classes\n",
    "int_to_class = dict(zip(Y, C))\n",
    "# Get a dictionary mapping from classes to integers\n",
    "class_to_int = dict(zip(C, Y))\n",
    "\n",
    "n_clases = len(int_to_class)\n",
    "n_samples, n_features = X.shape\n",
    "\n",
    "# Show the data dimensions:\n",
    "print(f\"-- The feature matrix contains {n_samples} samples and {n_features} features.\")\n",
    "\n",
    "# Pretty print the mapping\n",
    "print(\"-- The following mapping from integers to classes will be used:\")\n",
    "for k, v in int_to_class.items():\n",
    "    print(f\"     {k} -> {v}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "###  2.1. Training and test\n",
    "\n",
    "Next, we will split the data into two sets:\n",
    "\n",
    "* **Training set**, that will be used to learn the classification model\n",
    "* **Test set**, that will be used to evaluate the classification performance\n",
    "\n",
    "To make the partition, we can use the `train_test_split` method from `sklearn`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# Split the dataset into training and testing sets\n",
    "X_trn_all, X_tst_all, Y_trn_all, Y_tst_all = train_test_split(\n",
    "    X, Y, test_size=0.33, random_state=42)\n",
    "\n",
    "# Dataset sizes\n",
    "n_trn = len(Y_trn_all)\n",
    "n_tst = len(Y_tst_all)\n",
    "\n",
    "# Show the data dimensions:\n",
    "print(f\"-- {n_trn} training samples.\")\n",
    "print(f\"-- {n_tst} test samples.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 2.2. Scatter plots\n",
    "\n",
    "To get some intuition about this four dimensional dataset we can plot 2-dimensional projections taking only two variables each time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fh = 2  # First feature to plot\n",
    "fv = 3  # Second feature to plot\n",
    "\n",
    "plt.figure(figsize=(9, 3))\n",
    "for i in range(n_clases):\n",
    "    # Plot the samples of class i\n",
    "    plt.scatter(X_trn_all[Y_trn_all == i, fh], X_trn_all[Y_trn_all == i, fv],\n",
    "                label=int_to_class[i])\n",
    "plt.axis('equal')\n",
    "plt.xlabel(f'Feature {fh}')\n",
    "plt.ylabel(f'Feature {fv}')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "In the following, we will design a classifier to separate classes \"Versicolor\" and \"Virginica\" using $x_0$ and $x_1$ only. To do so, we build a training set with samples from these categories, and a binary label $y^{(k)} = 1$ for samples in class \"Virginica\", and $0$ for \"Versicolor\" data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select two classes\n",
    "ignored_class = 'Iris-setosa'\n",
    "c0 = 'Iris-versicolor' \n",
    "c1 = 'Iris-virginica'\n",
    "# Select two features\n",
    "ind = [0, 1]\n",
    "\n",
    "# New map indices to classes\n",
    "int_to_class_2 = {0: c0, 1: c1}\n",
    "class_to_int_2 = {c0: 0, c1: 1}\n",
    "\n",
    "# Select samples\n",
    "X_trn = X_trn_all[Y_trn_all != class_to_int[ignored_class], ]\n",
    "X_tst = X_tst_all[Y_tst_all != class_to_int[ignored_class], :]\n",
    "# Select features\n",
    "X_trn = X_trn[:, ind]\n",
    "X_tst = X_tst[:, ind]\n",
    "\n",
    "# Select labels for the selected features\n",
    "Y_trn = Y_trn_all[Y_trn_all != class_to_int[ignored_class]]\n",
    "Y_tst = Y_tst_all[Y_tst_all != class_to_int[ignored_class]]\n",
    "\n",
    "# Map to new class indices\n",
    "Y_trn = Y_trn == class_to_int[c1]\n",
    "Y_tst = Y_tst == class_to_int[c1]\n",
    "\n",
    "# Dataset sizes\n",
    "n_trn = len(Y_trn)\n",
    "n_tst = len(Y_tst)\n",
    "# Show the data dimensions:\n",
    "print(f\"-- {n_trn} training samples.\")\n",
    "print(f\"-- {n_tst} test samples.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(9, 3))\n",
    "for i in [0, 1]:\n",
    "    # Plot the samples of class i\n",
    "    plt.scatter(X_trn[Y_trn == i, 0], X_trn[Y_trn == i, 1],\n",
    "                label=int_to_class_2[i])\n",
    "plt.axis('equal')\n",
    "plt.xlabel(f'Feature {ind[0]}')\n",
    "plt.ylabel(f'Feature {ind[1]}')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## 3. A Baseline Classifier: Maximum A Priori.\n",
    "\n",
    "For the selected data set, we have two clases and a dataset with the following class proportions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "print(f'Class 0 ({c0}): {n_trn - sum(Y_trn)} samples')\n",
    "print(f'Class 1 ({c1}): {sum(Y_trn)} samples')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The maximum a priori classifier assigns any sample ${\\bf x}$ to the most frequent class in the training set. Therefore, the class prediction $y$ for any sample ${\\bf x}$ is"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "y = int(2*sum(Y_trn) > n_trn)\n",
    "print(f'y = {y} ({c1 if y==1 else c0})')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The error rate for this baseline classifier is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Training and test error arrays\n",
    "E_trn = (Y_trn != y)\n",
    "E_tst = (Y_tst != y)\n",
    "\n",
    "# Error rates\n",
    "pe_trn = float(sum(E_trn)) / n_trn\n",
    "pe_tst = float(sum(E_tst)) / n_tst\n",
    "print('Pe(train):', str(pe_trn))\n",
    "print('Pe(test): ', str(pe_tst))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The error rate of the baseline classifier is a simple benchmark for classification. Since the maximum a priori decision is independent on the observation, ${\\bf x}$, any classifier based on ${\\bf x}$ should have a better (or, at least, not worse) performance than the baseline classifier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 3. Parametric vs non-parametric classification.\n",
    "\n",
    "Most classification algorithms can be fitted to one of two categories:\n",
    "\n",
    "1. **Parametric classifiers**: to classify any input sample ${\\bf x}$, the classifier applies some function $f_{\\bf w}({\\bf x})$ which depends on some parameters ${\\bf w}$. The training dataset is used to estimate ${\\bf w}$. Once the parameter has been estimated, the training data is no longer needed to classify new inputs.\n",
    "\n",
    "2. **Non-parametric classifiers**: the classifier decision for any input ${\\bf x}$ depend on the training data in a direct manner. The training data must be preserved to classify new data.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
