{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ada593d9",
   "metadata": {
    "id": "ada593d9",
    "tags": [],
    "toc": true
   },
   "source": [
    "# Laboratory exercise: Classification\n",
    "\n",
    "In this laboratory exercise we will apply classification and regression algorithms over a synthetic dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6bfad0d",
   "metadata": {},
   "source": [
    "\n",
    "## Data load.\n",
    "\n",
    "Load the data variables from the npz file provided with this exercise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c450bfa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Load data from file 10012345.mat .\n",
    "data = np.load('10012345.npz')\n",
    "\n",
    "xRtrain = data['xRtrain']\n",
    "xRtrainLost = data['xRtrainLost']\n",
    "xRval = data['xRval']\n",
    "sRtrain = data['sRtrain']\n",
    "sRval = data['sRval']\n",
    "xCtrain = data['xCtrain']\n",
    "xCval = data['xCval']\n",
    "yCtrain = data['yCtrain']\n",
    "yCval = data['yCval']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e3ad414",
   "metadata": {},
   "source": [
    "Initialize all requested variables to 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "53117023",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classification:\n",
    "w_full, e_full, p20, emin, nvar, wmin, cv0, rp_opt, fpr = 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
    "# Regression:\n",
    "wML, AAE, NLL, wmean, Vw = 0, 0, 0, 0, 0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2482c9af",
   "metadata": {},
   "source": [
    "## Part 1: Clasification\n",
    "\n",
    "Each of the data matrices `xCtrain` and `xCval` contains 240 data vectors with dimension $D=5$. \n",
    "\n",
    "Assume that the binary labels `yCtrain` and `yCval` (with values in {0, 1}) were generated according to a logistic regression model:\n",
    "$$p(y = 1 | {\\bf w}, {\\bf x}) = \\frac{1}{1 + \\exp(-{\\bf w}^T {\\bf z})}$$\n",
    "where  \n",
    "$$\n",
    "{\\bf z} = \\begin{pmatrix} 1 \\\\ {\\bf x} \\end{pmatrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4644ab3f",
   "metadata": {},
   "source": [
    "### Exercise C0 [extra]:\n",
    "\n",
    "Normalize the input matrices in such a way that each feature has zero mean and unit standard deviation. You can do it using standard python commands or by means of the `preprocessing.StandardScaler` from `sklearn`. Use `xCtrain` to estimate the mean and variance of each feature, and make sure that the same normalization is applied to any input, ${\\bf x}$, no matter if it belogs to the training or the validation set.\n",
    "\n",
    "Store the normalized matrices in the same variables, `xCtrain`and `xCval`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "99a10e56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your code here.\n",
    "# <SOL>\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(xCtrain)\n",
    "xCtrain = scaler.transform(xCtrain)\n",
    "xCval = scaler.transform(xCval)\n",
    "\n",
    "# </SOL>    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d726bc8",
   "metadata": {},
   "source": [
    "### Exercise C1:\n",
    "\n",
    "As a preliminary task, fit a logistic regression model using the training data available in `xCtrain` and `yCtrain`, using the implementation available from `sklearn`. Use regularization parameter $C=2$, set the `random_state` to 42 and use the default values for all other arguments. \n",
    "\n",
    "Store the resulting weight vector in the variable `w_full`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c86139e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.02964435 -0.12589325  1.15997149 -0.15527988  0.7133148   0.27936155]\n"
     ]
    }
   ],
   "source": [
    "# Write your code here.\n",
    "#<SOL>\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "logistic = LogisticRegression(C=2, random_state=42)\n",
    "logistic.fit(xCtrain, yCtrain)\n",
    "\n",
    "w_full = logistic.coef_.flatten()\n",
    "\n",
    "# Add intercept term to the weight vector.\n",
    "w_full = np.concatenate((logistic.intercept_, w_full))\n",
    "#</SOL>\n",
    "\n",
    "print(w_full)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ede95aeb",
   "metadata": {},
   "source": [
    "### Exercise C2.\n",
    "\n",
    "Determine the classification error rate measured on the validation data (`xCval` and `yCval`). Store the error rate in the variable `e_full`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "54ac4c32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.2510416666666667\n"
     ]
    }
   ],
   "source": [
    "# Write your code here.\n",
    "#<SOL>\n",
    "e_full = 1 - logistic.score(xCval, yCval)\n",
    "# </SOL>\n",
    "\n",
    "# Print the error rate.\n",
    "print(e_full)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d2e19d2",
   "metadata": {},
   "source": [
    "### Exercise C3\n",
    "\n",
    "Determine the probability that the $k$-th sample in the validation set belongs to category  $y_k = 1$, according to the model computed in exercise 1, for $k = 0, 1, \\dots, 19$. Store the result in the variable `p20`.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b6647046",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.88397831 0.28853908 0.08093859 0.02582069 0.87146138 0.84385989\n",
      " 0.19683427 0.33489592 0.41218717 0.23983442 0.23100049 0.41446499\n",
      " 0.15930776 0.16412988 0.8642481  0.19540716 0.2827784  0.28855852\n",
      " 0.02787595 0.39437038]\n"
     ]
    }
   ],
   "source": [
    "# Write your code here\n",
    "# <SOL>\n",
    "p20 = logistic.predict_proba(xCval[:20])[:,1]\n",
    "# </SOL>\n",
    "\n",
    "\n",
    "# Print the probabilities.\n",
    "print(p20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "184ca46b",
   "metadata": {},
   "source": [
    "### Exercise C4.\n",
    "\n",
    "It is known that all coefficients  $w_i$  (with  $i > n$) are zero, that is, all variables $x_{n+1}, \\dots, x_{D-1}$ are irrelevant for the classification task, but the value of $n$ is unknown. Consequently, the goal is to fit a model that includes only the relevant variables:\n",
    "\n",
    "Train $D$ different logistic regression models, starting with the model that uses only the first variable, and adding one variable at a time, so that the $i$-th model will use only the variables $x_0, x_1, \\dots, x_{i-1}$.  Using $C=2$, `random_state`=42 and all other default parameters.\n",
    "\n",
    "For each model, compute the classification error rate (on the validation data), and keep the best result. \n",
    "\n",
    "Store the following variables:\n",
    "\n",
    "  * `emin`: the lowest validation error\n",
    "  * `nvar`: an integer indicating the number of variables in the model, \n",
    "  * `wmin`: the corresponding weight vector (only for the best case).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "619dbdb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "emin = 0.2510416666666667\n",
      "nvar = 4\n",
      "[ 0.03121687 -0.13570936  1.16054771 -0.15423108  0.72103519]\n"
     ]
    }
   ],
   "source": [
    "# Write your code here.\n",
    "#<SOL>\n",
    "D = xCtrain.shape[1]\n",
    "emin = 1\n",
    "nvar = 0\n",
    "wmin = np.zeros(D+1)\n",
    "\n",
    "# Iterate over the number of variables.\n",
    "for i in range(1, D+1):\n",
    "    # Train the model.\n",
    "    logistic = LogisticRegression(C=2, random_state=42)\n",
    "    logistic.fit(xCtrain[:, :i], yCtrain)\n",
    "    \n",
    "    # Compute the error rate.\n",
    "    e = 1 - logistic.score(xCval[:, :i], yCval)\n",
    "    \n",
    "    # Check if the error rate is lower than the current minimum.\n",
    "    if e < emin:\n",
    "        emin = e\n",
    "        nvar = i\n",
    "        wmin = logistic.coef_.flatten()\n",
    "        wmin = np.concatenate((logistic.intercept_, wmin))\n",
    "#</SOL>\n",
    "\n",
    "# Print the results.\n",
    "print(f\"emin = {emin}\")\n",
    "print(f\"nvar = {nvar}\")\n",
    "print(wmin)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa77badb",
   "metadata": {},
   "source": [
    "### Exercise C5 [extra].\n",
    "\n",
    "In this exercise we will train a classifier based on **quadratic discriminant analysis**, using the appropriate class from `sklearn`.\n",
    "\n",
    "The algorithm has a regularization parameter, `reg_param`, that must take some value between 0 and 1. We will select the appropriate value by means of 10-fold cross validation. As a validation metric, we will use the <a href=https://scikit-learn.org/stable/modules/generated/sklearn.metrics.f1_score.html#sklearn.metrics.f1_score>F1-score</a>.\n",
    "\n",
    "To do so:\n",
    "\n",
    "  1. Join the train and validation sets into a single dataset, by stacking matrices `xCtrain` (on top) and `xCval` (down) into a single matrix `xCV`. In a similar way, join labels into aarray `yV`.\n",
    "  2. Using the CV dataset and the `cross_val_score` method from `sklearn.model_selection`, compute the cross validation F1-score (averaged over all folds), for `reg_param=0`. Save the result in variable `cv0`\n",
    "  3. Select the best value of `reg_param` in $\\{0, 0.1, 0.2, 0.3, \\ldots, 1.0\\}$ by 10-fold cross validation, according to the F1-score. Save the result in variable `rp_opt`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "de4906ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cv0 = 0.7416666666666668\n",
      "rp_opt = 0.9\n"
     ]
    }
   ],
   "source": [
    "# Write your code here.\n",
    "#<SOL>\n",
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# Join the train and validation sets into a single dataset.\n",
    "xCV = np.vstack((xCtrain, xCval))\n",
    "yCV = np.hstack((yCtrain, yCval))\n",
    "\n",
    "# Compute the cross-validation score for reg_param=0.\n",
    "qda = QuadraticDiscriminantAnalysis(reg_param=0)\n",
    "cv0 = np.mean(cross_val_score(qda, xCV, yCV, cv=10))\n",
    "\n",
    "# Select the best value of reg_param in {0, 0.1, 0.2, ..., 1.0}.\n",
    "best_score = 0\n",
    "rp_opt = 0\n",
    "for rp in np.arange(0, 1.1, 0.1):\n",
    "    qda = QuadraticDiscriminantAnalysis(reg_param=rp)\n",
    "    score = np.mean(cross_val_score(qda, xCV, yCV, cv=10, scoring='f1'))\n",
    "    if score > best_score:\n",
    "        best_score = score\n",
    "        rp_opt = rp\n",
    "#</SOL>\n",
    "\n",
    "# Print the results.\n",
    "print(f\"cv0 = {cv0}\")\n",
    "print(f\"rp_opt = {rp_opt}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bed6926",
   "metadata": {},
   "source": [
    "### Exercise C6 [extra].\n",
    "\n",
    "Take the regularization parameter selected in C5, train the quadratic discriminant using `xCtrain`and `yCtrain`, and compute the false positive rate (i.e. the ratio of false positives vs the total number of negative samples) of the classifier over the validation set. Save the result in variable `fpr`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "eb6d984b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.2698072805139186\n"
     ]
    }
   ],
   "source": [
    "# Write your code here.\n",
    "#<SOL>\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# Train the QDA classifier with the optimal regularization parameter.\n",
    "qda = QuadraticDiscriminantAnalysis(reg_param=rp_opt)\n",
    "qda.fit(xCtrain, yCtrain)\n",
    "\n",
    "# Predict the labels for the validation set.\n",
    "y_pred = qda.predict(xCval)\n",
    "\n",
    "# Compute the confusion matrix.\n",
    "tn, fp, fn, tp = confusion_matrix(yCval, y_pred).ravel()\n",
    "\n",
    "# Compute the false positive rate.\n",
    "fpr = fp / (fp + tn)\n",
    "#</SOL>\n",
    "\n",
    "# Print the false positive rate.\n",
    "print(fpr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95f0d800",
   "metadata": {},
   "source": [
    "## Part 2: Regression\n",
    "\n",
    "The mentioned variables include a training set consisting of 300 data points, each consisting of input-output pairs:  $D = \\{{\\bf x}_k, s_k\\}_{k=0}^{299}$. The input vectors are provided as the rows of the variable `xRtrain`, while their corresponding labels are available in the vector `sRtrain`. Use these variables as provided, without applying any normalization procedure.\n",
    "\n",
    "Assume the data were generated according to the following model:\n",
    "$$\n",
    "s = w_0 + w_1 x_0 + w_2 x_2^3 + w_3 \\exp(x_4) + \\varepsilon\n",
    "$$\n",
    "where the noise samples follow a Gaussian distribution with zero mean and variance $\\sigma^2_{\\varepsilon} = 0.4$.\n",
    "\n",
    "### Exercise R1\n",
    "\n",
    "Obtain the maximum likelihood estimator of the model. Store your result in the variable `wML`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e58bc2a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(300, 5)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xRtrain.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2e8b89e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.22762496  0.10460595  0.00210107  0.00307295]\n"
     ]
    }
   ],
   "source": [
    "# Write your code here.\n",
    "#<SOL>\n",
    "\n",
    "# Transform variables (x0, x1, x2, x3, x4, x5) into a new matrix.\n",
    "Z = np.column_stack((np.ones(xRtrain.shape[0]), xRtrain[:, 0], xRtrain[:, 2]**3, np.exp(xRtrain[:, 4])))\n",
    "\n",
    "\n",
    "# Compute the maximum likelihood estimator.\n",
    "wML = np.linalg.inv(Z.T @ Z) @ Z.T @ sRtrain\n",
    "#</SOL>\n",
    "\n",
    "# Print the result.\n",
    "print(wML)                    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a1605e8",
   "metadata": {},
   "source": [
    "### Exercise R2\n",
    "\n",
    "For the previously obtained estimator, determine the average absolute error on the training dataset, i.e.,\n",
    "$$\n",
    "\\text{AAE} = \\frac{1}{N} \\sum_{i=1}^{N} |s(i) - \\hat{s}(i)|\n",
    "$$\n",
    "where  N  is the number of training data points. Store your result in the variable AAE.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6db68be8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8189814763278366\n"
     ]
    }
   ],
   "source": [
    "# Write your code here.\n",
    "#<SOL>\n",
    "AAE = np.mean(np.abs(sRtrain - Z @ wML))\n",
    "#</SOL>\n",
    "\n",
    "# Print the result.\n",
    "print(AAE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4fbd634",
   "metadata": {},
   "source": [
    "### Exercise R3\n",
    "\n",
    "Compute the negative log-likelihood,  of the previously obtained estimator using the training data, and store the result in the variable `NLL`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9c2c6b7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your code here.\n",
    "#<SOL>\n",
    "n_samples = len(sRtrain)\n",
    "NLL = 0.5 * n_samples * np.log(2 * np.pi * 0.4) + 0.5 * np.sum((sRtrain - Z @ wML)**2) / 0.4\n",
    "#</SOL>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20a5cf57",
   "metadata": {},
   "source": [
    "### Exercise R4\n",
    "\n",
    "Assume that the weight vector ${\\bf w}$  has a prior distribution  $p_W({\\bf w})$ , which is Gaussian with zero mean, unit variances ($\\text{var}\\{w_i\\} = 1$), and covariances $\\text{cov}\\{w_i, w_j\\} = 0.5$,  $i \\neq j$. \n",
    "\n",
    "Compute the posterior mean and the posterior covariance matrix of ${\\bf w}$ . Store your results in the variables `wmean` and `Vw`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9ac08e32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.22602065  0.10412973  0.00210942  0.00306339]\n",
      "[[ 1.75338425e-03 -3.37670835e-04  9.75937904e-06 -9.78155808e-06]\n",
      " [-3.37670835e-04  3.90910576e-04 -8.95788025e-07  2.61584204e-06]\n",
      " [ 9.75937904e-06 -8.95788025e-07  7.35878113e-07 -2.35934261e-08]\n",
      " [-9.78155808e-06  2.61584204e-06 -2.35934261e-08  2.24170791e-06]]\n"
     ]
    }
   ],
   "source": [
    "# Write your code here.\n",
    "#<SOL>\n",
    "\n",
    "# Compute the posterior covariance matrix.\n",
    "Sigma = (np.ones((Z.shape[1], Z.shape[1])) + np.eye(Z.shape[1]))/2.0\n",
    "Vw = np.linalg.inv(Z.T @ Z/0.4 + 2 * np.linalg.inv(Sigma))\n",
    "\n",
    "# Compute the posterior mean.\n",
    "wmean = Vw @ Z.T @ sRtrain / 0.4\n",
    "#</SOL>\n",
    "\n",
    "# Print the results.\n",
    "print(wmean)\n",
    "print(Vw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f17baac5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ###########################################\n",
    "# Save results in file results.npz\n",
    "np.savez('results.npz',\n",
    "         w_full=w_full, e_full=e_full, p20=p20, emin=emin, nvar=nvar,\n",
    "         cv0=cv0, rp_opt=rp_opt, fpr=fpr, xCtrain=xCtrain, xCval=xCval,\n",
    "         wmin=wmin, wML=wML, AAE=AAE, NLL=NLL, wmean=wmean, Vw=Vw)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Hand_Digit_with_NN_professor.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
