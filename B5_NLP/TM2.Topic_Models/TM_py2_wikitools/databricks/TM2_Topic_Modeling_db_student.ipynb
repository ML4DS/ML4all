{"cells":[{"cell_type":"markdown","source":["# Topic Modelling\n\nAuthor: Jesús Cid Sueiro\n\nDate: 2017/04/21"],"metadata":{}},{"cell_type":"markdown","source":["In this notebook we will explore some tools for text analysis in python. To do so, first we will import the requested python libraries."],"metadata":{}},{"cell_type":"code","source":["# %matplotlib inline\nimport numpy as np\nimport matplotlib.pyplot as plt\n# import pylab\n\n# Required imports\nfrom wikitools import wiki\nfrom wikitools import category\n\nimport nltk\nfrom nltk.tokenize import word_tokenize\nfrom nltk.corpus import stopwords\nfrom nltk.stem import WordNetLemmatizer\n\nimport gensim\n\nimport lda\nimport lda.datasets\n\nfrom time import time\nfrom sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\nfrom sklearn.decomposition import LatentDirichletAllocation\n\nfrom test_helper import Test"],"metadata":{"collapsed":false},"outputs":[],"execution_count":3},{"cell_type":"markdown","source":["## 1. Corpus acquisition.\n\nIn this notebook we will explore some tools for text processing and analysis and two topic modeling algorithms available from Python toolboxes.\n\nTo do so, we will explore and analyze collections of Wikipedia articles from a given category, using `wikitools`, that makes easy the capture of content from wikimedia sites.\n\n(*As a side note, there are many other available text collections to test topic modelling algorithm. In particular, the NLTK library has many examples, that can explore them using the `nltk.download()` tool*.\n\n    import nltk\n    nltk.download()\n\n*for instance, you can take the gutemberg dataset*\n\n    Mycorpus = nltk.corpus.gutenberg\n    text_name = Mycorpus.fileids()[0]\n    raw = Mycorpus.raw(text_name)\n    Words = Mycorpus.words(text_name)\n\n*Also, tools like Gensim or Sci-kit learn include text databases to work with*).\n\nIn order to use Wikipedia data, we will select a single category of articles:"],"metadata":{}},{"cell_type":"code","source":["site = wiki.Wiki(\"https://en.wikipedia.org/w/api.php\")\n# Select a category with a reasonable number of articles (>100)\n# cat = \"Economics\"\ncat = \"Pseudoscience\"\nprint cat"],"metadata":{"collapsed":false},"outputs":[],"execution_count":5},{"cell_type":"markdown","source":["You can try with any other categories. Take into account that the behavior of topic modelling algorithms may depend on the amount of documents available for the analysis. Select a category with at least 100 articles. You can browse the wikipedia category tree here, https://en.wikipedia.org/wiki/Category:Contents, for instance.\n\nWe start downloading the text collection."],"metadata":{}},{"cell_type":"code","source":["# Loading category data. This may take a while\nprint \"Loading category data. This may take a while...\"\ncat_data = category.Category(site, cat)\n\ncorpus_titles = []\ncorpus_text = []\n\nfor n, page in enumerate(cat_data.getAllMembersGen()):\n    print \"\\r Loading article {0}\".format(n + 1),\n    corpus_titles.append(page.title)\n    corpus_text.append(page.getWikiText())\n\nn_art = len(corpus_titles)\nprint \"\\nLoaded \" + str(n_art) + \" articles from category \" + cat"],"metadata":{"collapsed":false},"outputs":[],"execution_count":7},{"cell_type":"markdown","source":["Now, we have stored the whole text collection in two lists:\n\n* `corpus_titles`, which contains the titles of the selected articles\n* `corpus_text`, with the text content of the selected wikipedia articles\n\nYou can browse the content of the wikipedia articles to get some intuition about the kind of documents that will be processed."],"metadata":{}},{"cell_type":"code","source":["# n = 5\n# print corpus_titles[n]\n# print corpus_text[n]"],"metadata":{"collapsed":false},"outputs":[],"execution_count":9},{"cell_type":"markdown","source":["## 2. Corpus Processing\n\nTopic modelling algorithms process vectorized data. In order to apply them, we need to transform the raw text input data into a vector representation. To do so, we will remove irrelevant information from the text data and preserve as much relevant information as possible to capture the semantic content in the document collection.\n\nThus, we will proceed with the following steps:\n\n1. Tokenization, filtering and cleaning\n2. Homogeneization (stemming or lemmatization)\n3. Vectorization"],"metadata":{}},{"cell_type":"markdown","source":["### 2.1. Tokenization, filtering and cleaning.\n\nThe first steps consists on the following:\n\n1. Tokenization: convert text string into lists of tokens.\n2. Filtering:\n  * Removing capitalization: capital alphabetic characters will be transformed to their corresponding lowercase characters. \n  * Removing non alphanumeric tokens (e.g. punktuation signs)\n3. Cleaning: Removing stopwords, i.e., those words that are very common in language and do not carry out useful semantic content (articles, pronouns, etc)."],"metadata":{}},{"cell_type":"markdown","source":["To do so, we will need some packages from the [Natural Language Toolkit](http://www.nltk.org)."],"metadata":{}},{"cell_type":"code","source":["# You can comment this if the package is already available.\n# nltk.download(\"punkt\")\n# nltk.download(\"stopwords\")"],"metadata":{"collapsed":false},"outputs":[],"execution_count":13},{"cell_type":"code","source":["stopwords_en = stopwords.words('english')\ncorpus_clean = []\n\nfor n, art in enumerate(corpus_text): \n    print \"\\rProcessing article {0} out of {1}\".format(n + 1, n_art),\n    # This is to make sure that all characters have the appropriate encoding.\n    art = art.decode('utf-8')  \n    \n    # Tokenize each text entry. \n    # scode: tokens = <FILL IN>\n    token_list = word_tokenize(art)\n    \n    # Convert all tokens in token_list to lowercase, remove non alfanumeric tokens and stem.\n    # Store the result in a new token list, clean_tokens.\n    # scode: filtered_tokens = <FILL IN>\n    filtered_tokens = [token.lower() for token in token_list if token.isalnum()]\n\n    # Remove all tokens in the stopwords list and append the result to corpus_clean\n    # scode: clean_tokens = <FILL IN>\n    clean_tokens = [token for token in filtered_tokens if token not in stopwords_en]    \n\n    # scode: <FILL IN>\n    corpus_clean.append(clean_tokens)\nprint \"\\nLet's check the first tokens from document 0 after processing:\"\nprint corpus_clean[0][0:30]"],"metadata":{"collapsed":false},"outputs":[],"execution_count":14},{"cell_type":"code","source":["Test.assertTrue(len(corpus_clean) == n_art, 'List corpus_clean does not contain the expected number of articles')\nTest.assertTrue(len([c for c in corpus_clean[0] if c in stopwords_en])==0, 'Stopwords have not been removed')"],"metadata":{"collapsed":false},"outputs":[],"execution_count":15},{"cell_type":"markdown","source":["### 2.2. Stemming vs Lemmatization\n\nAt this point, we can choose between applying a simple stemming or ussing lemmatization. We will try both to test their differences."],"metadata":{}},{"cell_type":"markdown","source":["**Task**: Apply the `.stem()` method, from the stemmer object created in the first line, to `corpus_filtered`."],"metadata":{}},{"cell_type":"code","source":["# Select stemmer.\nstemmer = nltk.stem.SnowballStemmer('english')\ncorpus_stemmed = []\n\nfor n, token_list in enumerate(corpus_clean):\n    print \"\\rStemming article {0} out of {1}\".format(n + 1, n_art),\n    \n    # Convert all tokens in token_list to lowercase, remove non alfanumeric tokens and stem.\n    # Store the result in a new token list, clean_tokens.\n    # scode: stemmed_tokens = <FILL IN>\n    stemmed_tokens = [stemmer.stem(token) for token in token_list]\n    \n    # Add art to the stemmed corpus\n    # scode: <FILL IN>\n    corpus_stemmed.append(stemmed_tokens)\n\nprint \"\\nLet's check the first tokens from document 0 after stemming:\"\nprint corpus_stemmed[0][0:30]"],"metadata":{"collapsed":false},"outputs":[],"execution_count":18},{"cell_type":"code","source":["Test.assertTrue((len([c for c in corpus_stemmed[0] if c!=stemmer.stem(c)]) < 0.1*len(corpus_stemmed[0])), \n                'It seems that stemming has not been applied properly')"],"metadata":{"collapsed":false,"scrolled":true},"outputs":[],"execution_count":19},{"cell_type":"markdown","source":["Alternatively, we can apply lemmatization. For english texts, we can use the lemmatizer from NLTK, which is based on [WordNet](http://wordnet.princeton.edu). If you have not used wordnet before, you will likely need to download it from nltk"],"metadata":{}},{"cell_type":"code","source":["# You can comment this if the package is already available.\n# nltk.download(\"wordnet\")"],"metadata":{"collapsed":false},"outputs":[],"execution_count":21},{"cell_type":"markdown","source":["**Task**: Apply the `.lemmatize()` method, from the WordNetLemmatizer object created in the first line, to `corpus_filtered`."],"metadata":{}},{"cell_type":"code","source":["wnl = WordNetLemmatizer()\n\n# Select stemmer.\ncorpus_lemmat = []\n\nfor n, token_list in enumerate(corpus_clean):\n    print \"\\rLemmatizing article {0} out of {1}\".format(n + 1, n_art),\n    \n    # scode: lemmat_tokens = <FILL IN>\n    lemmat_tokens = [wnl.lemmatize(token) for token in token_list]\n\n    # Add art to the stemmed corpus\n    # scode: <FILL IN>\n    corpus_lemmat.append(lemmat_tokens)\n\nprint \"\\nLet's check the first tokens from document 0 after stemming:\"\nprint corpus_lemmat[0][0:30]"],"metadata":{"collapsed":false,"scrolled":true},"outputs":[],"execution_count":23},{"cell_type":"markdown","source":["One of the advantages of the lemmatizer method is that the result of lemmmatization is still a true word, which is more advisable for the presentation of text processing results and lemmatization.\n\nHowever, without using contextual information, lemmatize() does not remove grammatical differences. This is the reason why \"is\" or \"are\" are preserved and not replaced by infinitive \"be\".\n\nAs an alternative, we can apply .lemmatize(word, pos), where 'pos' is a string code specifying the part-of-speech (pos), i.e. the grammatical role of the words in its sentence. For instance, you can check the difference between `wnl.lemmatize('is')` and `wnl.lemmatize('is, pos='v')`."],"metadata":{}},{"cell_type":"markdown","source":["### 2.3. Vectorization\n\nUp to this point, we have transformed the raw text collection of articles in a list of articles, where each article is a collection of the word roots that are most relevant for semantic analysis. Now, we need to convert these data (a list of token lists) into a numerical representation (a list of vectors, or a matrix). To do so, we will start using the tools provided by the `gensim` library. \n\nAs a first step, we create a dictionary containing all tokens in our text corpus, and assigning an integer identifier to each one of them."],"metadata":{}},{"cell_type":"code","source":["# Create dictionary of tokens\nD = gensim.corpora.Dictionary(corpus_clean)\nn_tokens = len(D)\n\nprint \"The dictionary contains {0} tokens\".format(n_tokens)\nprint \"First tokens in the dictionary: \"\nfor n in range(10):\n    print str(n) + \": \" + D[n]"],"metadata":{"collapsed":false},"outputs":[],"execution_count":26},{"cell_type":"markdown","source":["In the second step, let us create a numerical version of our corpus using the `doc2bow` method. In general, `D.doc2bow(token_list)` transform any list of tokens into a list of tuples `(token_id, n)`, one per each token in `token_list`, where `token_id` is the token identifier (according to dictionary `D`) and `n` is the number of occurrences of such token in `token_list`."],"metadata":{}},{"cell_type":"markdown","source":["** Task**: Apply the `doc2bow` method from gensim dictionary `D`, to all tokens in every article in `corpus_clean`. The result must be a new list named `corpus_bow` where each element is a list of tuples `(token_id, number_of_occurrences)`."],"metadata":{}},{"cell_type":"code","source":["# Transform token lists into sparse vectors on the D-space\n# scode: corpus_bow = <FILL IN>\ncorpus_bow = [D.doc2bow(doc) for doc in corpus_clean]"],"metadata":{"collapsed":false},"outputs":[],"execution_count":29},{"cell_type":"code","source":["Test.assertTrue(len(corpus_bow)==n_art, 'corpus_bow has not the appropriate size') "],"metadata":{"collapsed":false},"outputs":[],"execution_count":30},{"cell_type":"markdown","source":["At this point, it is good to make sure to understand what has happened. In `corpus_clean` we had a list of token lists. With it, we have constructed a Dictionary, `D`, which assign an integer identifier to each token in the corpus.\nAfter that, we have transformed each article (in `corpus_clean`) in a list tuples `(id, n)`."],"metadata":{}},{"cell_type":"code","source":["print \"Original article (after cleaning): \"\nprint corpus_clean[0][0:30]\nprint \"Sparse vector representation (first 30 components):\"\nprint corpus_bow[0][0:30]\nprint \"The first component, {0} from document 0, states that token 0 ({1}) appears {2} times\".format(\n    corpus_bow[0][0], D[0], corpus_bow[0][0][1])"],"metadata":{"collapsed":false},"outputs":[],"execution_count":32},{"cell_type":"markdown","source":["Note that we can interpret each element of corpus_bow as a `sparse_vector`. For example, a list of tuples \n\n    [(0, 1), (3, 3), (5,2)] \n\nfor a dictionary of 10 elements can be represented as a vector, where any tuple `(id, n)` states that position `id` must take value `n`. The rest of positions must be zero.\n\n    [1, 0, 0, 3, 0, 2, 0, 0, 0, 0]\n\nThese sparse vectors will be the inputs to the topic modeling algorithms.\n\nNote that, at this point, we have built a Dictionary containing"],"metadata":{}},{"cell_type":"code","source":["print \"{0} tokens\".format(len(D))"],"metadata":{"collapsed":false},"outputs":[],"execution_count":34},{"cell_type":"markdown","source":["and a bow representation of a corpus with"],"metadata":{}},{"cell_type":"code","source":["print \"{0} Wikipedia articles\".format(len(corpus_bow))"],"metadata":{"collapsed":false},"outputs":[],"execution_count":36},{"cell_type":"markdown","source":["Before starting with the semantic analyisis, it is interesting to observe the token distribution for the given corpus."],"metadata":{}},{"cell_type":"code","source":["# SORTED TOKEN FREQUENCIES (I):\n# Create a \"flat\" corpus with all tuples in a single list\ncorpus_bow_flat = [item for sublist in corpus_bow for item in sublist]\n\n# Initialize a numpy array that we will use to count tokens.\n# token_count[n] should store the number of ocurrences of the n-th token, D[n]\ntoken_count = np.zeros(n_tokens)\n\n# Count the number of occurrences of each token.\nfor x in corpus_bow_flat:\n    # Update the proper element in token_count\n    # scode: <FILL IN>\n    token_count[x[0]] += x[1]\n\n# Sort by decreasing number of occurences\nids_sorted = np.argsort(- token_count)\ntf_sorted = token_count[ids_sorted]"],"metadata":{"collapsed":false},"outputs":[],"execution_count":38},{"cell_type":"markdown","source":["`ids_sorted` is a list of all token ids, sorted by decreasing number of occurrences in the whole corpus. For instance, the most frequent term is"],"metadata":{}},{"cell_type":"code","source":["print D[ids_sorted[0]]"],"metadata":{"collapsed":false},"outputs":[],"execution_count":40},{"cell_type":"markdown","source":["which appears"],"metadata":{}},{"cell_type":"code","source":["print \"{0} times in the whole corpus\".format(tf_sorted[0])"],"metadata":{"collapsed":false},"outputs":[],"execution_count":42},{"cell_type":"markdown","source":["In the following we plot the most frequent terms in the corpus."],"metadata":{}},{"cell_type":"code","source":["# SORTED TOKEN FREQUENCIES (II):\nplt.rcdefaults()\n\n# Example data\nn_bins = 25\nhot_tokens = [D[i] for i in ids_sorted[n_bins-1::-1]]\ny_pos = np.arange(len(hot_tokens))\nz = tf_sorted[n_bins-1::-1]/n_art\n\nplt.barh(y_pos, z, align='center', alpha=0.4)\nplt.yticks(y_pos, hot_tokens)\nplt.xlabel('Average number of occurrences per article')\nplt.title('Token distribution')\nplt.show()"],"metadata":{"collapsed":false},"outputs":[],"execution_count":44},{"cell_type":"code","source":["# SORTED TOKEN FREQUENCIES:\n\n# Example data\nplt.semilogy(tf_sorted)\nplt.xlabel('Average number of occurrences per article')\nplt.title('Token distribution')\nplt.show()\ndisplay()"],"metadata":{"collapsed":false,"scrolled":true},"outputs":[],"execution_count":45},{"cell_type":"markdown","source":["** Exercise**: There are usually many tokens that appear with very low frequency in the corpus. Count the number of tokens appearing only once, and what is the proportion of them in the token list."],"metadata":{}},{"cell_type":"code","source":["# scode: <WRITE YOUR CODE HERE>\n# Example data\ncold_tokens = [D[i] for i in ids_sorted if tf_sorted[i]==1]\n\nprint \"There are {0} cold tokens, which represent {1}% of the total number of tokens in the dictionary\".format(\n    len(cold_tokens), float(len(cold_tokens))/n_tokens*100)"],"metadata":{"collapsed":false},"outputs":[],"execution_count":47},{"cell_type":"markdown","source":["** Exercise**: Represent graphically those 20 tokens that appear in the highest number of articles. Note that you can use the code above (headed by `# SORTED TOKEN FREQUENCIES`) with a very minor modification."],"metadata":{}},{"cell_type":"code","source":["# scode: <WRITE YOUR CODE HERE>\n\n# SORTED TOKEN FREQUENCIES (I):\n# Count the number of occurrences of each token.\ntoken_count2 = np.zeros(n_tokens)\nfor x in corpus_bow_flat:\n    token_count2[x[0]] += (x[1]>0)\n\n# Sort by decreasing number of occurences\nids_sorted2 = np.argsort(- token_count2)\ntf_sorted2 = token_count2[ids_sorted2]\n\n# SORTED TOKEN FREQUENCIES (II):\n# Example data\nn_bins = 25\nhot_tokens2 = [D[i] for i in ids_sorted2[n_bins-1::-1]]\ny_pos2 = np.arange(len(hot_tokens2))\nz2 = tf_sorted2[n_bins-1::-1]/n_art\n\nplt.figure()\nplt.barh(y_pos2, z2, align='center', alpha=0.4)\nplt.yticks(y_pos2, hot_tokens2)\nplt.xlabel('Average number of occurrences per article')\nplt.title('Token distribution')\nplt.show()\ndisplay()"],"metadata":{"collapsed":false},"outputs":[],"execution_count":49},{"cell_type":"markdown","source":["## 3. Semantic Analysis\n\nThe dictionary `D` and the Bag of Words in `corpus_bow` are the key inputs to the topic model algorithms. In this section we will explore two algorithms:\n\n1. Latent Semantic Indexing (LSI)\n2. Latent Dirichlet Allocation (LDA)\n\nThe topic model algorithms in `gensim` assume that input documents are parameterized using the tf-idf model. This can be done using"],"metadata":{}},{"cell_type":"code","source":["tfidf = gensim.models.TfidfModel(corpus_bow)"],"metadata":{"collapsed":false},"outputs":[],"execution_count":51},{"cell_type":"markdown","source":["From now on, tfidf can be used to convert any vector from the old representation (bow integer counts) to the new one (TfIdf real-valued weights):"],"metadata":{}},{"cell_type":"code","source":["doc_bow = [(0, 1), (1, 1)]\ntfidf[doc_bow]"],"metadata":{"collapsed":false},"outputs":[],"execution_count":53},{"cell_type":"markdown","source":["Or to apply a transformation to a whole corpus"],"metadata":{}},{"cell_type":"code","source":["corpus_tfidf = tfidf[corpus_bow]\nprint corpus_tfidf[0][0:5]"],"metadata":{"collapsed":false},"outputs":[],"execution_count":55},{"cell_type":"markdown","source":["### 3.1. Latent Semantic Indexing (LSI)"],"metadata":{}},{"cell_type":"markdown","source":["Now we are ready to apply a topic modeling algorithm. Latent Semantic Indexing is provided by `LsiModel`."],"metadata":{}},{"cell_type":"markdown","source":["**Task**: Generate a LSI model with 5 topics for `corpus_tfidf` and dictionary `D`. You can check de sintaxis for [gensim.models.LsiModel](https://radimrehurek.com/gensim/models/lsimodel.html)."],"metadata":{}},{"cell_type":"code","source":["# Initialize an LSI transformation\nn_topics = 5\n\n# scode: lsi = <FILL IN>"],"metadata":{"collapsed":false},"outputs":[],"execution_count":59},{"cell_type":"markdown","source":["From LSI, we can check both the topic-tokens matrix and the document-topics matrix."],"metadata":{}},{"cell_type":"markdown","source":["Now we can check the topics generated by LSI. An intuitive visualization is provided by the `show_topics` method."],"metadata":{}},{"cell_type":"code","source":["lsi.show_topics(num_topics=-1, num_words=10, log=False, formatted=True)"],"metadata":{"collapsed":false},"outputs":[],"execution_count":62},{"cell_type":"markdown","source":["However, a more useful representation of topics is as a list of tuples `(token, value)`. This is provided by the `show_topic` method."],"metadata":{}},{"cell_type":"markdown","source":["**Task**: Represent the columns of the topic-token matrix as a series of bar diagrams (one per topic) with the top 25 tokens of each topic."],"metadata":{}},{"cell_type":"code","source":["# SORTED TOKEN FREQUENCIES (II):\nplt.rcdefaults()\n\nn_bins = 25\n    \n# Example data\ny_pos = range(n_bins-1, -1, -1)\n\n# pylab.rcParams['figure.figsize'] = 16, 8  # Set figure size\nplt.figure(figsize=(16, 8))\n\nfor i in range(n_topics):\n    \n    ### Plot top 25 tokens for topic i\n    # Read i-thtopic\n    # topic_i =  <FILL IN>\n    # tokens =  <FILL IN>\n    # weights =  <FILL IN>\n\n    # Plot\n    plt.subplot(1, n_topics, i+1)\n    plt.barh(# scode: <FILL IN> \n    plt.yticks(y_pos, tokens)\n    plt.xlabel('Top {0} topic weights'.format(n_bins))\n    plt.title('Topic {0}'.format(i))\n    \nplt.show()\ndisplay()"],"metadata":{"collapsed":false},"outputs":[],"execution_count":65},{"cell_type":"markdown","source":["LSI approximates any document as a linear combination of the topic vectors. We can compute the topic weights for any input corpus entered as input to the `lsi` model."],"metadata":{}},{"cell_type":"code","source":["# On real corpora, target dimensionality of\n# 200–500 is recommended as a “golden standard”\n# Create a double wrapper over the original \n# corpus bow  tfidf  fold-in-lsi\ncorpus_lsi = lsi[corpus_tfidf]\nprint corpus_lsi[0]"],"metadata":{"collapsed":false},"outputs":[],"execution_count":67},{"cell_type":"markdown","source":["**Task**: Find the document with the largest positive weight for topic 0. Compare the document and the topic."],"metadata":{}},{"cell_type":"code","source":["# Extract weights from corpus_lsi\n# scode weight0 = <FILL IN>\nweight0 = [doc[0][1] for doc in corpus_lsi]\n\n# Locate the maximum positive weight\nnmax = np.argmax(weight0)\nprint nmax\nprint weight0[nmax]\nprint corpus_lsi[nmax]\n\n# Get topic 0\n# scode: topic_0 = <FILL IN>\n\n# Compute a list of tuples (token, wordcount) for all tokens in topic_0, where wordcount is the number of \n# occurences of the token in the article.\n# scode: token_counts = <FILL IN>\n\nprint \"Topic 0 is:\"\nprint topic_0\nprint \"Token counts:\"\nprint token_counts"],"metadata":{"collapsed":false},"outputs":[],"execution_count":69},{"cell_type":"markdown","source":["### 3.2. Latent Dirichlet Allocation (LDA)"],"metadata":{}},{"cell_type":"markdown","source":["There are several implementations of the LDA topic model in python:\n\n* Python library `lda`.\n* Gensim module: `gensim.models.ldamodel.LdaModel`\n* Sci-kit Learn module: `sklearn.decomposition`"],"metadata":{}},{"cell_type":"markdown","source":["#### 3.2.1. LDA using Gensim"],"metadata":{}},{"cell_type":"markdown","source":["The use of the LDA module in `gensim` is similar to LSI. Furthermore, it assumes that a `tf-idf` parametrization is used as an input, which is not in complete agreement with the theoretical model, which assumes documents represented as vectors of token-counts.\n\nTo use LDA in gensim, we must first create a lda model object."],"metadata":{}},{"cell_type":"code","source":["ldag = gensim.models.ldamodel.LdaModel(\n    corpus=corpus_tfidf, id2word=D, num_topics=10, update_every=1, passes=10)"],"metadata":{"collapsed":false},"outputs":[],"execution_count":74},{"cell_type":"code","source":["ldag.print_topics()"],"metadata":{"collapsed":false},"outputs":[],"execution_count":75},{"cell_type":"markdown","source":["#### 3.2.2. LDA using python lda library"],"metadata":{}},{"cell_type":"markdown","source":["An alternative to gensim for LDA is the lda library from python. It requires a doc-frequency matrix as input"],"metadata":{}},{"cell_type":"code","source":["# For testing LDA, you can use the reuters dataset\n# X = lda.datasets.load_reuters()\n# vocab = lda.datasets.load_reuters_vocab()\n# titles = lda.datasets.load_reuters_titles()\nX = np.int32(np.zeros((n_art, n_tokens)))\nfor n, art in enumerate(corpus_bow):\n    for t in art:\n        X[n, t[0]] = t[1]\nprint X.shape\nprint X.sum()\n\nvocab = D.values()\ntitles = corpus_titles\n"],"metadata":{"collapsed":false},"outputs":[],"execution_count":78},{"cell_type":"code","source":["# Default parameters:\n# model = lda.LDA(n_topics, n_iter=2000, alpha=0.1, eta=0.01, random_state=None, refresh=10)\nmodel = lda.LDA(n_topics=10, n_iter=1500, random_state=1)\nmodel.fit(X)  # model.fit_transform(X) is also available\ntopic_word = model.topic_word_  # model.components_ also works\n\n# Show topics...\nn_top_words = 8\nfor i, topic_dist in enumerate(topic_word):\n    topic_words = np.array(vocab)[np.argsort(topic_dist)][:-(n_top_words+1):-1]\n    print('Topic {}: {}'.format(i, ' '.join(topic_words)))"],"metadata":{"collapsed":false,"scrolled":true},"outputs":[],"execution_count":79},{"cell_type":"markdown","source":["Document-topic distribution"],"metadata":{}},{"cell_type":"code","source":["doc_topic = model.doc_topic_\nfor i in range(10):\n    print(\"{} (top topic: {})\".format(titles[i], doc_topic[i].argmax()))"],"metadata":{"collapsed":false},"outputs":[],"execution_count":81},{"cell_type":"code","source":["# This is to apply the model to a new doc(s)\n# doc_topic_test = model.transform(X_test)\n# for title, topics in zip(titles_test, doc_topic_test):\n#    print(\"{} (top topic: {})\".format(title, topics.argmax()))"],"metadata":{"collapsed":false},"outputs":[],"execution_count":82},{"cell_type":"markdown","source":["It allows incremental updates"],"metadata":{}},{"cell_type":"markdown","source":["#### 3.2.2. LDA using Sci-kit Learn"],"metadata":{}},{"cell_type":"markdown","source":["The input matrix to the `sklearn` implementation of LDA contains the token-counts for all documents in the corpus.\n`sklearn` contains a powerfull `CountVectorizer` method that can be used to construct the input matrix from the `corpus_bow`. \n\nFirst, we will define an auxiliary function to print the top tokens in the model, that has been taken from the `sklearn` documentation."],"metadata":{}},{"cell_type":"code","source":["# Adapted from an example in sklearn site \n# http://scikit-learn.org/dev/auto_examples/applications/topics_extraction_with_nmf_lda.html\n\n# You can try also with the dataset provided by sklearn in \n# from sklearn.datasets import fetch_20newsgroups\n# dataset = fetch_20newsgroups(shuffle=True, random_state=1,\n#                              remove=('headers', 'footers', 'quotes'))\n\ndef print_top_words(model, feature_names, n_top_words):\n    for topic_idx, topic in enumerate(model.components_):\n        print(\"Topic #%d:\" % topic_idx)\n        print(\" \".join([feature_names[i]\n                        for i in topic.argsort()[:-n_top_words - 1:-1]]))\n    print()"],"metadata":{"collapsed":false},"outputs":[],"execution_count":86},{"cell_type":"markdown","source":["Now, we need a dataset to feed the Count_Vectorizer object, by joining all tokens in `corpus_clean` in a single string, using a space ' ' as separator."],"metadata":{}},{"cell_type":"code","source":["print(\"Loading dataset...\")\n# scode: data_samples = <FILL IN>\n\nprint 'Document 0:'\nprint data_samples[0][0:200], '...'"],"metadata":{"collapsed":false},"outputs":[],"execution_count":88},{"cell_type":"markdown","source":["Now we are ready to compute the token counts."],"metadata":{}},{"cell_type":"code","source":["# Use tf (raw term count) features for LDA.\nprint(\"Extracting tf features for LDA...\")\nn_features = 1000\nn_samples = 2000\ntf_vectorizer = CountVectorizer(max_df=0.95, min_df=2,\n                                max_features=n_features,\n                                stop_words='english')\n\nt0 = time()\ntf = tf_vectorizer.fit_transform(data_samples)\nprint(\"done in %0.3fs.\" % (time() - t0))\nprint tf[0][0][0]"],"metadata":{"collapsed":false,"scrolled":true},"outputs":[],"execution_count":90},{"cell_type":"markdown","source":["Now we can apply the LDA algorithm. \n\n**Task**: Create an LDA object with the following parameters: \n    n_topics=n_topics, max_iter=5,\n    learning_method='online',\n    learning_offset=50.,\n    random_state=0"],"metadata":{}},{"cell_type":"code","source":["print(\"Fitting LDA models with tf features, \"\n      \"n_samples=%d and n_features=%d...\"\n      % (n_samples, n_features))\n# scode: lda = <FILL IN>\n\n"],"metadata":{"collapsed":false},"outputs":[],"execution_count":92},{"cell_type":"markdown","source":["**Task**: Fit model `lda` with the token frequencies computed by `tf_vectorizer`."],"metadata":{}},{"cell_type":"code","source":["t0 = # scode: <FILL IN>\ncorpus_lda = # scode: <FILL IN>\nprint corpus_lda[0]/np.sum(corpus_lda[0])\nprint(\"done in %0.3fs.\" % (time() - t0))"],"metadata":{"collapsed":false},"outputs":[],"execution_count":94},{"cell_type":"code","source":["print(\"\\nTopics in LDA model:\")\ntf_feature_names = # scode: <FILL IN>\nprint_top_words(lda, tf_feature_names, n_top_words)"],"metadata":{"collapsed":false},"outputs":[],"execution_count":95},{"cell_type":"code","source":["topics = lda.components_\ntopic_probs = [t/np.sum(t) for t in topics]\n# print topic_probs[0]\nprint -np.sort(-topic_probs[0])\n"],"metadata":{"collapsed":false},"outputs":[],"execution_count":96},{"cell_type":"markdown","source":["**Exercise**: Represent graphically the topic distributions for the top 25 tokens with highest probability for each topic."],"metadata":{}},{"cell_type":"code","source":[""],"metadata":{},"outputs":[],"execution_count":98},{"cell_type":"markdown","source":["**Exercise**: Explore the influence of the concentration parameters, $alpha$ (`doc_topic_prior` in `sklearn`) and $eta$(`topic_word_prior`). In particular observe how do topic and document distributions change as these parameters increase."],"metadata":{}},{"cell_type":"code","source":[""],"metadata":{},"outputs":[],"execution_count":100},{"cell_type":"markdown","source":["** Exercise**: The token dictionary and the token distribution have shown that:\n\n1. Some tokens, despite being very frequent in the corpus, have no semantic relevance for topic modeling. Unfortunately, they were not present in the stopword list, and have not been elliminated before the analysis.\n\n2. A large portion of tokens appear only once and, thus, they are not statistically relevant for the inference engine of the topic models.\n\nRevise the entire corpus be removing from the corpus all these sets of terms."],"metadata":{}},{"cell_type":"code","source":[""],"metadata":{},"outputs":[],"execution_count":102},{"cell_type":"markdown","source":["** Exercise**: Note that we have not used the terms in the article titles, though the can be expected to containg relevant words for the topic modeling. Include the title words in the analyisis. In order to give them a special relevante, insert them in the corpus several time, so as to make their words more significant."],"metadata":{}},{"cell_type":"code","source":[""],"metadata":{},"outputs":[],"execution_count":104},{"cell_type":"markdown","source":["** Exercise**: The topic modelling algorithms we have tested in this notebook are non-supervised. This makes them difficult to evaluate objectivelly. In order to test if LDA captures real topics, construct a dataset as the mixture of wikipedia articles from 4 different categories, and test if LDA with 4 topics identifies topics closely related to the original categories."],"metadata":{}},{"cell_type":"code","source":[""],"metadata":{},"outputs":[],"execution_count":106}],"metadata":{"language_info":{"mimetype":"text/x-python","name":"python","pygments_lexer":"ipython2","codemirror_mode":{"name":"ipython","version":2},"version":"2.7.12","nbconvert_exporter":"python","file_extension":".py"},"name":"TM2_Topic_Modeling_student","notebookId":1977758087314153,"kernelspec":{"display_name":"Python [default]","language":"python","name":"python2"},"anaconda-cloud":{}},"nbformat":4,"nbformat_minor":0}
