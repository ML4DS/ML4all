{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Master Telefónica Big Data & Analytics\n",
    "# **Prueba de Evaluación del  Tema 4:** \n",
    "## **Topic Modelling.**\n",
    "Date: 2016/04/10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para realizar esta prueba es necesario tener actualizada la máquina virtual con la versión más reciente de MLlib.\n",
    "\n",
    "Para la actualización, debe seguir los pasos que se indican a continuación:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pasos para actualizar MLlib:\n",
    "\n",
    "1. Entrar en la vm como root:\n",
    "\n",
    "    `vagrant ssh`\n",
    "\n",
    "    `sudo bash` \n",
    "\n",
    "    Ir a `/usr/local/bin`\n",
    "\n",
    "2. Descargar la última versión de spark desde la vm mediante \n",
    "\n",
    "    `wget http://www-eu.apache.org/dist/spark/spark-1.6.1/spark-1.6.1-bin-hadoop2.6.tgz`\n",
    "\n",
    "3. Desempaquetar: \n",
    "\n",
    "    `tar xvf spark-1.6.1-bin-hadoop2.6.tgz` (y borrar el tgz)\n",
    "\n",
    "4. Lo siguiente es un parche, pero suficiente para que funcione:\n",
    "\n",
    "    Guardar copia de `spark-1.3: mv spark-1.3.1-bin-hadoop2.6/ spark-1.3.1-bin-hadoop2.6_old`\n",
    "\n",
    "    Crear enlace a `spark-1.6: ln -s spark-1.6.1-bin-hadoop2.6/ spark-1.3.1-bin-hadoop2.6`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Librerías\n",
    "\n",
    "Puede utilizar este espacio para importar todas las librerías que necesite para realizar el examen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import nltk\n",
    "\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import pylab\n",
    "\n",
    "# import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "#from test_helper import Test\n",
    "\n",
    "import collections\n",
    "\n",
    "from pyspark.mllib.clustering import LDA, LDAModel\n",
    "from pyspark.mllib.linalg import Vectors\n",
    "\n",
    "# import gensim\n",
    "# import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Adquisición de un corpus.\n",
    "\n",
    "Descargue el contenido del corpus `reuters` de `nltk`.\n",
    "\n",
    "    import nltk\n",
    "    nltk.download()\n",
    "\n",
    "Selecciona el identificador `reuters`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#nltk.download()\n",
    "mycorpus = nltk.corpus.reuters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para evitar problemas de sobrecarga de memoria, o de tiempo de procesado, puede reducir el tamaño el corpus, modificando el valor de la variable n_docs a continuación."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 7769 files\n"
     ]
    }
   ],
   "source": [
    "n_docs = 500000\n",
    "\n",
    "filenames = mycorpus.fileids()\n",
    "fn_train = [f for f in filenames if f[0:5]=='train']\n",
    "\n",
    "corpus_text = [mycorpus.raw(f) for f in fn_train]\n",
    "\n",
    "# Reduced dataset:\n",
    "n_docs = min(n_docs, len(corpus_text))\n",
    "corpus_text = [corpus_text[n] for n in range(n_docs)]\n",
    "\n",
    "print 'Loaded {0} files'.format(len(corpus_text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A continuación cargaremos los datos en un RDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'sc' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-c0cddf5967d6>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mcorpusRDD\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparallelize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcorpus_text\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m4\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;32mprint\u001b[0m \u001b[1;34m\"\\nRDD created with {0} elements\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcorpusRDD\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcount\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'sc' is not defined"
     ]
    }
   ],
   "source": [
    "corpusRDD = sc.parallelize(corpus_text, 4)\n",
    "print \"\\nRDD created with {0} elements\".format(corpusRDD.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Ejercicios"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Ejercicio 1**: Preprocesamiento de datos.\n",
    "\n",
    "Prepare los datos para aplicar un algoritmo de modelado de tópicos en `pyspark`. Para ello, aplique los pasos siguientes:\n",
    "\n",
    "1. *Tokenización*: convierta cada texto a utf-8, y transforme la cadena en una lista de tokens.\n",
    "2. Homogeneización: pase todas las palabras a minúsculas y elimine todos los tokens no alfanuméricos.\n",
    "3. Limpieza: Elimine todas las stopwords utilizando el fichero de stopwords disponible en NLTK para el idioma inglés.\n",
    "\n",
    "Guarde el resultado en la variable `corpus_tokensRDD`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def getTokenList(doc, stopwords_en):\n",
    "    \n",
    "    # scode: tokens = <FILL IN>   # Tokenize docs\n",
    "    tokens = word_tokenize(doc.decode('utf-8'))    \n",
    "\n",
    "    # scode: tokens = <FILL IN>   # Remove non-alphanumeric tokens and normalize to lowercase\n",
    "    tokens = [t.lower() for t in tokens if t.isalnum()]\n",
    "    # scode: tokens = <FILL IN>   # Remove stopwords\n",
    "    tokens = [t for t in tokens if t not in stopwords_en]\n",
    "\n",
    "    return tokens\n",
    "\n",
    "stopwords_en = stopwords.words('english')\n",
    "corpus_tokensRDD = (corpusRDD\n",
    "                   .map(lambda x: getTokenList(x, stopwords_en))\n",
    "                   .cache())   \n",
    "\n",
    "# print \"\\n Let's check tokens after cleaning:\"\n",
    "print corpus_tokensRDD.take(1)[0][0:30]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Ejercicio 2**: Stemming\n",
    "\n",
    "Aplique un procedimiento de *stemming* al corpus, utilizando el `SnowballStemmer` de NLTK. Guarde el resultado en `corpus_stemRDD`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Select stemmer.\n",
    "stemmer = nltk.stem.SnowballStemmer('english')\n",
    "\n",
    "# scode: corpus_stemRDD = <FILL IN>\n",
    "corpus_stemRDD = corpus_tokensRDD.map(lambda x: [stemmer.stem(token) for token in x])\n",
    "\n",
    "print \"\\nLet's check the first tokens from document 0 after stemming:\"\n",
    "print corpus_stemRDD.take(1)[0][0:30]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Ejercicio 3**: Vectorización\n",
    "\n",
    "En este punto cada documento del corpus es una lista de tokens. \n",
    "\n",
    "Calcule un nuevo RDD que contenga, para cada documento, una lista de tuplas. La clave (*key*) de cada lista será un token y su valor el número de repeticiones del mismo en el documento. \n",
    "\n",
    "Imprima una muestra de 20 tuplas uno de los documentos del corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# corpus_wcRDD = <FILL IN>\n",
    "corpus_wcRDD = (corpus_stemRDD\n",
    "                .map(collections.Counter)\n",
    "                .map(lambda x: [(t, x[t]) for t in x]))\n",
    "\n",
    "print corpus_wcRDD.take(1)[0][0:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Ejercicio 4**: Cálculo del diccionario de tokens\n",
    "\n",
    "Construya, a partir de `corpus_wcRDD`, un nuevo diccionario con todos los tokens del corpus. El resultado será un diccionario python de nombre `wcDict`, cuyas entradas serán los tokens y sus valores el número de repeticiones del token en todo el corpus.\n",
    "\n",
    "   `wcDict = {token1: valor1, token2, valor2, ...}`\n",
    "   \n",
    "Imprima el número de repeticiones del token `interpret`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# scode: wcRDD = < FILL IN >\n",
    "wcRDD = (corpus_wcRDD\n",
    "         .flatMap(lambda x: x)\n",
    "         .reduceByKey(lambda x, y: x + y))\n",
    "\n",
    "wcDict = dict(wcRDD.collect())\n",
    "\n",
    "print wcDict['interpret']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Ejercicio 5**: Número de tokens.\n",
    "\n",
    "Determine el número total de tokens en el diccionario. Imprima el resultado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print wcRDD.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Ejercicio 6**: Términos demasiado frecuentes: \n",
    "\n",
    "Determine los 5 tokens más frecuentes del corpus. Imprima el resultado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print wcRDD.takeOrdered(5, lambda x: -x[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Ejercicio 7**: Número de documentos del token más frecuente.\n",
    "\n",
    "Determine en qué porcentaje de documentos aparece el token más frecuente.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tokenmasf = 'said'\n",
    "ndocs = corpus_stemRDD.filter(lambda x: tokenmasf in x).count()\n",
    "\n",
    "print 'El numerod de documentos es {0}, es decir, el {1} % del total de documentos'.format(\n",
    "    ndocs, float(ndocs)/corpus_stemRDD.count()*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Ejercicio 8**: Filtrado de términos.\n",
    "\n",
    "Elimine del corpus los dós términos más frecuentes. Guarde el resultado en un nuevo RDD denominado corpus_wcRDD2, con la misma estructura que corpus_wcRDD (es decir, cada documento una lista de tuplas)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "corpus_wcRDD2 = corpus_wcRDD.map(lambda x: [tupla for tupla in x if tupla[0] \n",
    "                                 not in ['said', 'mln']])\n",
    "\n",
    "print corpus_wcRDD2.take(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Ejercicio 9**: Lista de tokens y diccionario inverso.\n",
    "\n",
    "Determine la lista de topicos de todo el corpus, y construya un dictionario inverso, cuyas entradas sean los números consecutivos de 0 al número total de tokens, y sus salidas cada uno de los tokens, es decir\n",
    "\n",
    "    invD = {0: token0, 1: token1, 2: token2, ...}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# scode: wcRDD = < FILL IN >\n",
    "wcRDD2 = (corpus_wcRDD2\n",
    "         .flatMap(lambda x: x)\n",
    "         .reduceByKey(lambda x, y: x + y)\n",
    "         .sortBy(lambda x: -x[1]))\n",
    "\n",
    "# Token Dictionary:\n",
    "n_tokens = wcRDD2.count()\n",
    "TD = wcRDD2.takeOrdered(n_tokens, lambda x: -x[1])\n",
    "\n",
    "D = map(lambda x: x[0], TD)\n",
    "token_count = map(lambda x: x[1], TD)   \n",
    "\n",
    "# Compute inverse dictionary\n",
    "invD = dict(zip(D, xrange(n_tokens)))\n",
    "\n",
    "print invD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Ejercicio 10**: Algoritmo LDA.\n",
    "\n",
    "Para aplicar el algoritmo LDA, es necesario reemplazar las tuplas `(token, valor)` de `wcRDD` por tuplas del tipo `(token_id, value)`, sustituyendo cada token por un identificador entero.\n",
    "\n",
    "El código siguiente se encarga de completar este proceso:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Compute RDD replacing tokens by token_ids\n",
    "corpus_sparseRDD = corpus_wcRDD2.map(lambda x: [(invD[t[0]], t[1]) for t in x])\n",
    "\n",
    "# Convert list of tuplas into Vectors.sparse object.\n",
    "corpus_sparseRDD = corpus_sparseRDD.map(lambda x: Vectors.sparse(n_tokens, x))\n",
    "corpus4lda = corpus_sparseRDD.zipWithIndex().map(lambda x: [x[1], x[0]]).cache()\n",
    "print corpus4lda.take(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aplique el algoritmo LDA con 4 tópicos sobre el corpus obtenido en `corpus4lda`, para un valor de `topicConcentration = 2.0` y `docConcentration = 3.0`. (Tenga en cuenta que estos parámetros de entrada deben de ser tipo float)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print \"Training LDA: this might take a while...\"\n",
    "start = time.time()\n",
    "n_topics = 4\n",
    "ldaModel = LDA.train(corpus4lda, k=n_topics, topicConcentration=2.0, docConcentration=3.0)\n",
    "print \"Modelo LDA entrenado en: {0} segundos\".format(time.time()-start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Ejercicio 11**: Tokens principales.\n",
    "\n",
    "Imprima los dos tokens de mayor peso de cada tópico. (Debe imprimir el texto del token, no su índice). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "n_topics = 4\n",
    "ldatopics = ldaModel.describeTopics(maxTermsPerTopic=2)\n",
    "ldatopicnames = map(lambda x: x[0], ldatopics)\n",
    "print ldatopicnames\n",
    "for i in range(n_topics):\n",
    "    print \"Topic {0}: {1}, {2}\".format(i, D[ldatopicnames[i][0]], D[ldatopicnames[i][1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print ldatopics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### **Ejercicio 12**: Pesos de un token.\n",
    "\n",
    "Imprima el peso del token `bank` en cada tópico."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Output topics. Each is a distribution over words (matching word count vectors)\n",
    "iBank = invD['bank']\n",
    "topicMatrix = ldaModel.topicsMatrix()\n",
    "print topicMatrix[iBank]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Test 13**: Indique cuáles de las siguientes afirmaciones se puede asegurar que son verdaderas:\n",
    "\n",
    "1. En LSI, cada documento se asigna a un sólo tópico.\n",
    "2. De acuerdo con el modelo LDA, todos los tokens de un documento han sido generados por el mismo tópico\n",
    "3. LSI descompone la matriz de datos de entrada en el producto de 3 matrices cuadradas.\n",
    "4. Si el rango de la matriz de entrada a un modelo LSI es igual al número de tópicos. La descomposición SVD del modelo LSI es exacta (no es una aproximación).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "FFFV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Test 14**: Indique cuáles de las siguientes afirmaciones se puede asegurar que son verdaderas:\n",
    "\n",
    "1. En un modelo LDA, la distribución de Dirichlet se utiliza para generar distribuciones de probabilidad de tokens.\n",
    "2. Si una palabra aparece en pocos documentos del corpus, su IDF es mayor.\n",
    "3. El resultado de la lematización de una palabra es una palabra\n",
    "4. El resultado del stemming de una palabra es una palabra\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "VVVF"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
