{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Topic Modelling with MLlib\n",
    "\n",
    "Author: Jesús Cid Sueiro\n",
    "\n",
    "Date: 2016/04/10\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook we will explore the utilitis for Topic Modelling available on MLlib."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'wikitools'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-9385c520ef78>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m# Required imports\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mwikitools\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mwiki\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mwikitools\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcategory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'wikitools'"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pylab\n",
    "\n",
    "# Required imports\n",
    "from wikitools import wiki\n",
    "from wikitools import category\n",
    "\n",
    "# import nltk\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "from test_helper import Test\n",
    "\n",
    "import collections\n",
    "\n",
    "from pyspark.mllib.clustering import LDA, LDAModel\n",
    "from pyspark.mllib.linalg import Vectors\n",
    "\n",
    "# import gensim\n",
    "# import numpy as np\n",
    "# import lda\n",
    "# import lda.datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Corpus acquisition.\n",
    "\n",
    "In this notebook we will explore some tools for text processing and analysis and two topic modeling algorithms available from Python toolboxes.\n",
    "\n",
    "To do so, we will explore and analyze collections of Wikipedia articles from a given category, using `wikitools`, that makes easy the capture of content from wikimedia sites.\n",
    "\n",
    "(*As a side note, there are many other available text collections to test topic modelling algorithm. In particular, the NLTK library has many examples, that can explore them using the `nltk.download()` tool*.\n",
    "\n",
    "    import nltk\n",
    "    nltk.download()\n",
    "\n",
    "*for instance, you can take the gutemberg dataset*\n",
    "\n",
    "    Mycorpus = nltk.corpus.gutenberg\n",
    "    text_name = Mycorpus.fileids()[0]\n",
    "    raw = Mycorpus.raw(text_name)\n",
    "    Words = Mycorpus.words(text_name)\n",
    "\n",
    "*Also, tools like Gensim or Sci-kit learn include text databases to work with*).\n",
    "\n",
    "In order to use Wikipedia data, we will select a single category of articles:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "site = wiki.Wiki(\"https://en.wikipedia.org/w/api.php\")\n",
    "# Select a category with a reasonable number of articles (>100)\n",
    "cat = \"Economics\"\n",
    "# cat = \"Pseudoscience\"\n",
    "print cat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can try with any other categories. Take into account that the behavior of topic modelling algorithms may depend on the amount of documents available for the analysis. Select a category with at least 100 articles. You can browse the wikipedia category tree here, https://en.wikipedia.org/wiki/Category:Contents, for instance.\n",
    "\n",
    "We start downloading the text collection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading category data. This may take a while\n",
    "print \"Loading category data. This may take a while...\"\n",
    "cat_data = category.Category(site, cat)\n",
    "\n",
    "corpus_titles = []\n",
    "corpus_text = []\n",
    "\n",
    "for n, page in enumerate(cat_data.getAllMembersGen()):\n",
    "    print \"\\r Loading article {0}\".format(n + 1),\n",
    "    corpus_titles.append(page.title)\n",
    "    corpus_text.append(page.getWikiText())\n",
    "\n",
    "n_art = len(corpus_titles)\n",
    "print \"\\nLoaded \" + str(n_art) + \" articles from category \" + cat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we have stored the whole text collection in two lists:\n",
    "\n",
    "* `corpus_titles`, which contains the titles of the selected articles\n",
    "* `corpus_text`, with the text content of the selected wikipedia articles\n",
    "\n",
    "You can browse the content of the wikipedia articles to get some intuition about the kind of documents that will be processed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# n = 5\n",
    "# print corpus_titles[n]\n",
    "# print corpus_text[n]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we will load the text collection into an RDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpusRDD = sc.parallelize(corpus_text, 4)\n",
    "print \"\\nRDD created with {0} elements\".format(corpusRDD.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Test.assertTrue(corpusRDD.count() >= 100, \n",
    "                \"Your corpus_tokens has less than 100 articles. Consider using a larger dataset\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Corpus Processing\n",
    "\n",
    "Topic modelling algorithms process vectorized data. In order to apply them, we need to transform the raw text input data into a vector representation. To do so, we will remove irrelevant information from the text data and preserve as much relevant information as possible to capture the semantic content in the document collection.\n",
    "\n",
    "Thus, we will proceed with the following steps:\n",
    "\n",
    "1. Tokenization\n",
    "2. Homogeneization\n",
    "3. Cleaning\n",
    "4. Vectorization\n",
    "\n",
    "The first three steps are independent for each document, so they can be parallelized."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1. Tokenization, Homogeneization and Cleaning.\n",
    "\n",
    "For the first steps, we will use some of the powerfull methods available from the [Natural Language Toolkit](http://www.nltk.org). In order to use the `word_tokenize` method from nltk, you might need to get the appropriate libraries using `nltk.download()`. You must select option \"d) Download\", and identifier \"punkt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can comment this if the package is already available.\n",
    "# Select option \"d) Download\", and identifier \"punkt\"\n",
    "# nltk.download()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also, we need to load a list of english stopwords. Select now identifier \"stopwords\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can comment this if the package is already available.\n",
    "# Select option \"d) Download\", and identifier \"stopwords\"\n",
    "# nltk.download()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can check the stopword list. This is a standard python list of strings. We could modify it by removing words or adding new ones if required."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "stopwords_en = stopwords.words('english')\n",
    "\n",
    "print \"The stopword list contains {0} elements: \".format(len(stopwords_en))\n",
    "print stopwords_en"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task**: Create a method `getTokenList` with two inputs: a document (string) and a stopword list, and completes the first three steps of the corpus processing, as follows:\n",
    "\n",
    "1. Tokenization: convert string to `utf-8` and transform the string into a list of tokens, using `word_tokenize` from `nltk.tokenize`.\n",
    "2. Homogeneization: transform capital letters to lowercase and remove non alphanumeric tokens.\n",
    "3. Cleaning: remove stopwords\n",
    "\n",
    "Return the result of cleaning (a list of tokens)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getTokenList(doc, stopwords_en):\n",
    "    \n",
    "    # scode: tokens = <FILL IN>   # Tokenize docs\n",
    "    tokens = word_tokenize(doc.decode('utf-8'))    \n",
    "\n",
    "    # scode: tokens = <FILL IN>   # Remove non-alphanumeric tokens and normalize to lowercase\n",
    "    tokens = [t.lower() for t in tokens if t.isalnum()]\n",
    "    # scode: tokens = <FILL IN>   # Remove stopwords\n",
    "    tokens = [t for t in tokens if t not in stopwords_en]\n",
    "\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Test.assertEquals(getTokenList('The rain in spain stays mainly in the plane', stopwords_en), \n",
    "                  [u'rain', u'spain', u'stays', u'mainly', u'plane'],\n",
    "                  'getTokenList does not return the expected results')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task**: Apply `getTokenList` to all documents in the corpus and save the result in a `corpus_tokensRDD`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scode: corpus_tokensRDD = <FILL IN>\n",
    "corpus_tokensRDD = (corpusRDD\n",
    "                   .map(lambda x: getTokenList(x, stopwords_en))\n",
    "                   .cache())   \n",
    "\n",
    "# print \"\\n Let's check tokens after cleaning:\"\n",
    "print corpus_tokensRDD.take(1)[0][0:30]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Test.assertEquals(corpus_tokensRDD.count(), n_art, \n",
    "                  \"The number of documents in the original set does not correspond to the size of corpus_tokensRDD\")\n",
    "Test.assertTrue(all([c==c.lower() for c in corpus_tokensRDD.take(1)[0]]), 'Capital letters have not been removed')\n",
    "Test.assertTrue(all([c.isalnum() for c in corpus_tokensRDD.take(1)[0]]), \n",
    "                'Non alphanumeric characters have not been removed')\n",
    "Test.assertTrue(len([c for c in corpus_tokensRDD.take(1)[0] if c in stopwords_en])==0, \n",
    "                'Stopwords have not been removed')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2. Stemming / Lemmatization\n",
    "\n",
    "Now we will apply stemming and lemmatization to `corpus_tokensRDD`. We will test our topic models over the resulting RDDs, to test their differences."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task**: Apply stemming to all documents `corpus_tokensRDD` and save the result in a new RDD, `corpus_stemmedRDD`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select stemmer.\n",
    "stemmer = nltk.stem.SnowballStemmer('english')\n",
    "\n",
    "# scode: corpus_stemRDD = <FILL IN>\n",
    "corpus_stemRDD = corpus_tokensRDD.map(lambda x: [stemmer.stem(token) for token in x])\n",
    "\n",
    "print \"\\nLet's check the first tokens from document 0 after stemming:\"\n",
    "print corpus_stemRDD.take(1)[0][0:30]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "Test.assertTrue((len([c for c in corpus_stemRDD.take(1)[0] if c!=stemmer.stem(c)]) \n",
    "                 < 0.1*len(corpus_stemRDD.take(1)[0])), \n",
    "                'It seems that stemming has not been applied properly')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternatively, we can apply lemmatization. For english texts, we can use the lemmatizer from NLTK, which is based on [WordNet](http://wordnet.princeton.edu). If you have not used wordnet before, you will likely need to download it from nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can comment this if the package is already available.\n",
    "# Select option \"d) Download\", and identifier \"wordnet\"\n",
    "# nltk.download()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task**: Lemmatize all documents `corpus_tokensRDD` using the .lemmatize() method, from the WordNetLemmatizer object created in the first line and save the result in a new RDD, `corpus_lemRDD`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "wnl = WordNetLemmatizer()\n",
    "\n",
    "# scode: corpus_lemmatRDD = <FILL IN>\n",
    "corpus_lemmatRDD = (corpus_tokensRDD\n",
    "                    .map(lambda x: [wnl.lemmatize(token) for token in x]))\n",
    "\n",
    "print \"\\nLet's check the first tokens from document 0 after stemming:\"\n",
    "print corpus_lemmatRDD.take(1)[0][0:30]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of the advantages of the lemmatizer method is that the result of lemmatization is still a true word, which is more advisable for the presentation of text processing results and lemmatization.\n",
    "\n",
    "However, without using contextual information, lemmatize() does not remove grammatical differences. This is the reason why \"is\" or \"are\" are preserved and not replaced by infinitive \"be\".\n",
    "\n",
    "As an alternative, we can apply `.lemmatize(word, pos)`, where 'pos' is a string code specifying the part-of-speech (pos), i.e. the grammatical role of the words in its sentence. For instance, you can check the difference between `wnl.lemmatize('is')` and `wnl.lemmatize('is, pos='v')`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4. Vectorization\n",
    "\n",
    "Up to this point, we have transformed the raw text collection of articles in a list of articles, where each article is a collection of the word roots that are most relevant for semantic analysis. Now, we need to convert these data (a list of token lists) into a numerical representation (a list of vectors, or a matrix). \n",
    "\n",
    "#### 2.4.1. Word Count\n",
    "\n",
    "As a first step, we compute the word count for every document in the corpus."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task**: Compute a new RDD from `corpus_stemRDD` where each element is a list of tuples related to a document. The key of each tuple is a token, and its value the number of occurrences of this token in the document. To do so, you can use method `Counter` from `collections`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# corpus_wcRDD = <FILL IN>\n",
    "corpus_wcRDD = (corpus_stemRDD\n",
    "                .map(collections.Counter)\n",
    "                .map(lambda x: [(t, x[t]) for t in x]))\n",
    "\n",
    "print corpus_wcRDD.take(1)[0][0:30]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Test.assertTrue(corpus_wcRDD.count() == n_art, 'List corpus_clean does not contain the expected number of articles')\n",
    "Test.assertTrue(corpus_wcRDD.flatMap(lambda x: x).map(lambda x: x[1]).sum()== corpus_stemRDD.map(len).sum(), \n",
    "                'The total token count in the output RDD is not consistent with the total number of input tokens')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point, we have got a representation of documents as list of tuples `(token, word_count)` in `corpus_wcRDD`. From this RDD, we can compute a dictionary containing all tokens in the corpus as keys, and their respective number of occurrences as values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task**: Using `corpus_wcRDD` compute a new RDD of `(key, value)` pairs, where keys are the tokens in the whole corpus and their respective values are the total number of occurences in the corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# scode: wcRDD = < FILL IN >\n",
    "wcRDD = (corpus_wcRDD\n",
    "         .flatMap(lambda x: x)\n",
    "         .reduceByKey(lambda x, y: x + y))\n",
    "\n",
    "print wcRDD.take(30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task**: Take all tuples in `wcRDD` in decreasing order of the number of token counts in variable `TD` and compute two lists: \n",
    "\n",
    "1. `token_count`: a list of token counts, in decreasing order.\n",
    "2. `D`: A list of tokens, in the same order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Token Dictionary:\n",
    "n_tokens = wcRDD.count()\n",
    "# scode: TD = wcRDD.<FILL IN>\n",
    "TD = wcRDD.takeOrdered(n_tokens, lambda x: -x[1])\n",
    "\n",
    "# scode: D = <FIll IN>    # Extract tokens from TD\n",
    "D = map(lambda x: x[0], TD)\n",
    "# scode: token_count = <FILL IN>   # Extract token counts from TD\n",
    "token_count = map(lambda x: x[1], TD)   \n",
    "\n",
    "# ALTERNATIVELY:\n",
    "TD_RDD = wcRDD.sortBy(lambda x: -x[1])\n",
    "D_RDD = TD_RDD.map(lambda x: x[0])\n",
    "token_countRDD = TD_RDD.map(lambda x: x[1])\n",
    "\n",
    "print TD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can visualize the token distribution using `D` and `token_count`, for the most frequent terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# SORTED TOKEN FREQUENCIES (II):\n",
    "# plt.rcdefaults()\n",
    "\n",
    "# Example data\n",
    "n_bins = 25\n",
    "y_pos = range(n_bins-1, -1, -1)\n",
    "hot_tokens = D[0:n_bins]\n",
    "z = [float(t)/n_art for t in token_count[0:n_bins]]\n",
    "\n",
    "plt.barh(y_pos, z, align='center', alpha=0.4)\n",
    "plt.yticks(y_pos, hot_tokens)\n",
    "plt.xlabel('Average number of occurrences per article')\n",
    "plt.title('Token distribution')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Latent Dirichlet Allocation\n",
    "\n",
    "In order to apply the LDA algorithm, we need to represent the input documents in the format required by MLlib. More specifically. The input data should be an RDD where each element is a tuple\n",
    "\n",
    "    (doc_id, vector)\n",
    "\n",
    "where `doc_id` is an integer document identifier, and `vector` can be a sparse or dense vector from class `Vectors`. We will use sparse vectors, which are more adequate for large vocabularies. \n",
    "\n",
    "To compute the sparse vectors, we must first transform the lists of tuples `(token, value)` in `wcRDD` into a lists of `(token_id, value)`, pairs, thus replacing each token by a numerical identifier.\n",
    "\n",
    "We will proceed in two steps:\n",
    "\n",
    "1. Compute an inverse dictionary, `invD`, transforming tokens into numbers.\n",
    "2. Apply the inverse dictionary to compute a new RDD from `wcRDD` replacing each token by its `token_id`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[** Task**: complete the two steps outlined above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# INDICE INVERTIDO: EJEMPLO:\n",
    "# D = ['token1', 'token2', 'token3', 'token4']\n",
    "# D[1] = 'token2'\n",
    "# invD = {'token1': 0, 'token2': 1, 'token3': 2, 'token4': 3}\n",
    "# invD['token2'] = 1\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Compute inverse dictionary\n",
    "# scode: invD = <FILL IN>\n",
    "invD = dict(zip(D, xrange(n_tokens)))\n",
    "\n",
    "### ALTERNATIVELY:\n",
    "# invD_RDD = D_RDD.zipWithIndex()      ### Tuples (token, index)\n",
    "\n",
    "# Compute RDD replacing tokens by token_ids\n",
    "# scode: corpus_sparseRDD = <FILL IN>\n",
    "corpus_sparseRDD = corpus_wcRDD.map(lambda x: [(invD[t[0]], t[1]) for t in x])\n",
    "\n",
    "# Convert list of tuplas into Vectors.sparse object.\n",
    "corpus_sparseRDD = corpus_sparseRDD.map(lambda x: Vectors.sparse(n_tokens, x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "The only remaining step consists on adding an identifier to each document of the corpus.\n",
    "\n",
    "**Task**: Apply method `zipWithIndex` to `corpus_sparseRDD` in order to add consecutive integer identifiers to all documents in the corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "corpus4lda = corpus_sparseRDD.zipWithIndex().map(lambda x: [x[1], x[0]]).cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's all. We can already call to the lda algorithm.'\n",
    "\n",
    "**Task**: Train an LDA model with 3 topics and the corpus obtained in `corpus4lda`. Check the [LDA documentation](http://spark.apache.org/docs/latest/mllib-clustering.html#latent-dirichlet-allocation-lda) to find the appropriate command."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print \"Training LDA: this might take a while...\"\n",
    "# scode: ldaModel = LDA.<FILL IN>\n",
    "ldaModel = LDA.train(corpus4lda, k=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The whole topics matrix can be computed using the `.topicsMatrix()` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output topics. Each is a distribution over words (matching word count vectors)\n",
    "print(\"Learned topics (as distributions over vocab of \" + str(ldaModel.vocabSize()) + \" words):\")\n",
    "topics = ldaModel.topicsMatrix()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternatively, we can use the `.describeTopics` method that returns the most relevan terms for each topic, and it is more useful for a graphical plot.\n",
    "\n",
    "**Task**: Represent the 25 most relevant terms for each topic using bar plots."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "n_bins = 25\n",
    "    \n",
    "# Example data\n",
    "y_pos = range(n_bins-1, -1, -1)\n",
    "\n",
    "pylab.rcParams['figure.figsize'] = 16, 8  # Set figure size\n",
    "for i in range(3):\n",
    "    \n",
    "    topic = ldaModel.describeTopics(maxTermsPerTopic=n_bins)[i]\n",
    "    tokens = [D[n] for n in topic[0]]\n",
    "    weights = topic[1]\n",
    "\n",
    "    plt.subplot(1, 3, i+1)\n",
    "    plt.barh(y_pos, weights, align='center', alpha=0.4)\n",
    "    plt.yticks(y_pos, tokens)\n",
    "    plt.xlabel('Average number of occurrences per article')\n",
    "    plt.title('Token distribution')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise**: Explore the influence of the `topicConcentration` parameter. Show in barplots the most relevant tokens for each topic for large values of this parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unfortunately, we cannot capture the document distributions over topics, in the current version of pySpark mllib (1.6)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Latent Semantic Indexing\n",
    "\n",
    "LSI is not specifically available in MLlib, There are methods to compute the SVD decomposition of a matrix, which is the core transformation for LSI, but, unfortunately, SVD decomposition is available in Java and Scala, but not in python. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code, taken from Stackoverflow, can be used to compute the SVD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.mllib.common import callMLlibFunc, JavaModelWrapper\n",
    "from pyspark.mllib.linalg.distributed import RowMatrix\n",
    "\n",
    "class SVD(JavaModelWrapper):\n",
    "    \"\"\"Wrapper around the SVD scala case class\"\"\"\n",
    "    @property\n",
    "    def U(self):\n",
    "        \"\"\" Returns a RowMatrix whose columns are the left singular vectors of the SVD if computeU was set to be True.\"\"\"\n",
    "        u = self.call(\"U\")\n",
    "        if u is not None:\n",
    "            return RowMatrix(u)\n",
    "\n",
    "    @property\n",
    "    def s(self):\n",
    "        \"\"\"Returns a DenseVector with singular values in descending order.\"\"\"\n",
    "        return self.call(\"s\")\n",
    "\n",
    "    @property\n",
    "    def V(self):\n",
    "        \"\"\" Returns a DenseMatrix whose columns are the right singular vectors of the SVD.\"\"\"\n",
    "        return self.call(\"V\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def computeSVD(row_matrix, k, computeU=False, rCond=1e-9):\n",
    "    \"\"\"\n",
    "    Computes the singular value decomposition of the RowMatrix.\n",
    "    The given row matrix A of dimension (m X n) is decomposed into U * s * V'T where\n",
    "    * s: DenseVector consisting of square root of the eigenvalues (singular values) in descending order.\n",
    "    * U: (m X k) (left singular vectors) is a RowMatrix whose columns are the eigenvectors of (A X A')\n",
    "    * v: (n X k) (right singular vectors) is a Matrix whose columns are the eigenvectors of (A' X A)\n",
    "    :param k: number of singular values to keep. We might return less than k if there are numerically zero singular values.\n",
    "    :param computeU: Whether of not to compute U. If set to be True, then U is computed by A * V * sigma^-1\n",
    "    :param rCond: the reciprocal condition number. All singular values smaller than rCond * sigma(0) are treated as zero, where sigma(0) is the largest singular value.\n",
    "    :returns: SVD object\n",
    "    \"\"\"\n",
    "    java_model = row_matrix._java_matrix_wrapper.call(\"computeSVD\", int(k), computeU, float(rCond))\n",
    "    return SVD(java_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import *\n",
    "from pyspark.mllib.linalg import Vectors\n",
    "\n",
    "data = [(Vectors.dense([0.0, 1.0, 0.0, 7.0, 0.0]),), (Vectors.dense([2.0, 0.0, 3.0, 4.0, 5.0]),), (Vectors.dense([4.0, 0.0, 0.0, 6.0, 7.0]),)]\n",
    "df = sqlContext.createDataFrame(data,[\"features\"])\n",
    "\n",
    "pca_extracted = PCA(k=2, inputCol=\"features\", outputCol=\"pca_features\")\n",
    "\n",
    "model = pca_extracted.fit(df)\n",
    "features = model.transform(df) # this create a DataFrame with the regular features and pca_features\n",
    "\n",
    "# We can now extract the pca_features to prepare our RowMatrix.\n",
    "pca_features = features.select(\"pca_features\").rdd.map(lambda row : row[0])\n",
    "mat = RowMatrix(pca_features)\n",
    "\n",
    "# Once the RowMatrix is ready we can compute our Singular Value Decomposition\n",
    "svd = computeSVD(mat,2,True)\n",
    "\n",
    "print svd.s\n",
    "# DenseVector([9.491, 4.6253])\n",
    "print svd.U.rows.collect()\n",
    "# [DenseVector([0.1129, -0.909]), DenseVector([0.463, 0.4055]), DenseVector([0.8792, -0.0968])]\n",
    "print svd.V\n",
    "# DenseMatrix(2, 2, [-0.8025, -0.5967, -0.5967, 0.8025], 0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task**: Adapt the code above to compute the LSI topic model of `corpus4lda`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
