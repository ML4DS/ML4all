{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Parametric Model-Based regression\n",
    "\n",
    "    Notebook version: 1.4 (Sep 27, 2024)\n",
    "\n",
    "    Author: Jesús Cid-Sueiro (jesus.cid@uc3m.es)\n",
    "            Jerónimo Arenas García (jarenas@tsc.uc3m.es)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "    Changes: v.1.0 - First version, expanding some cells from the Bayesian Regression \n",
    "                     notebook\n",
    "             v.1.1 - Python 3 version.\n",
    "             v.1.2 - Revised presentation. \n",
    "             v.1.3 - Updated index notation\n",
    "             v.1.4 - Revised presentation order. Use of PolynomialFeatures. @ instead of dot. Defining NLL\n",
    "    \n",
    "    Pending changes: * Include regression on the stock data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Import some libraries that will be necessary for working with data and displaying plots\n",
    "\n",
    "# To visualize plots in the notebook\n",
    "%matplotlib inline \n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import scipy.io       # To read matlab files\n",
    "import pylab\n",
    "\n",
    "from sklearn.preprocessing import PolynomialFeatures\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### A quick note on the mathematical notation\n",
    "\n",
    "In this notebook we will make extensive use of probability distributions. In general, we will use capital leters\n",
    "${\\bf X}$, $S$, $E$ ..., to denote random variables, and lower-case letters ${\\bf x}$, $s$, $\\epsilon$ ..., to denote the values they can take. \n",
    "\n",
    "In general, we will use letter $p$ for probability density functions (pdf). When necessary, we will use, capital subindices to make the random variable explicit. For instance, $p_{{\\bf X}, S}({\\bf x}, s)$ would be the joint pdf of random variables ${\\bf X}$ and $S$ at values ${\\bf x}$ and $s$, respectively. \n",
    "\n",
    "However, to avoid a notation overload, we will omit subindices when they are clear from the context. For instance, we will use $p({\\bf x}, s)$ instead of $p_{{\\bf X}, S}({\\bf x}, s)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 1. Model-based parametric regression\n",
    "\n",
    "### 1.1. The regression problem\n",
    "\n",
    "Given an observation vector ${\\bf x}$, the goal of the regression problem is to find a function $f({\\bf x})$ providing *good* predictions about some unknown variable $s$. To do so, we assume that a set of *labelled* training examples, $\\{{\\bf x}_k, s_k\\}_{k=0}^{K-1}$ is available. \n",
    "\n",
    "The predictor function should make good predictions for new observations ${\\bf x}$ not used during training. In practice, this is tested using a second set (the *test set*) of labelled samples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 1.2. The underlying model assumption\n",
    "\n",
    "Many regression algorithms are grounded on the idea that all samples from the training set have been generated \n",
    "independently by some common stochastic process.\n",
    "\n",
    "<center>\n",
    "<img src=\"figs/data_model.png\" width=180>\n",
    "</center>\n",
    "\n",
    "If $p({\\bf x}, s)$ were known, we could apply estimation theory to estimate $s$ for a given ${\\bf x}$ using $p$. For instance, we could apply any of the following classical estimates:\n",
    "\n",
    "   * Maximum A Posterior (MAP): $$\\hat{s}_{\\text{MAP}} = \\arg\\max_s p(s| {\\bf x})$$\n",
    "   * Minimum Mean Square Error (MSE): $$\\hat{s}_{\\text{MSE}} = \\mathbb{E}\\{S |{\\bf x}\\} = \\int s \\, p(s| {\\bf x}) \\, ds $$\n",
    "\n",
    "Note that, since these estimators depend on $p(s |{\\bf x})$, knowing the posterior distribution of the target variable is enough, and we do not need to know the joint distribution $p({\\bf x}, s)$.\n",
    "\n",
    "More importantly, note that **if we knew the underlying model, we would not need the data** in ${\\cal D}$ to make predictions on new data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Exercise 1:\n",
    "\n",
    "Assume the target variable $s$ is a scaled noisy version of the input variable $x$: \n",
    "$$\n",
    "s = 2 x + \\epsilon\n",
    "$$\n",
    "where $\\epsilon$ is Gaussian a noise variable with zero mean and unit variance, which does not depend on $x$.\n",
    "\n",
    "  1. Compute the target model $p(s| x)$\n",
    "  2. Compute prediction $\\hat{s}_\\text{MAP}$ for an arbitrary input $x$\n",
    "  3. Compute prediction $\\hat{s}_\\text{MSE}$ for an arbitrary input $x$\n",
    "  4. Compute prediction $\\hat{s}_\\text{MSE}$ for input $x=4$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Solution:\n",
    "\n",
    "[comment]: # (<SOL>)\n",
    "[comment]: # (</SOL>)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 1.3. Model-based regression\n",
    "\n",
    "In practice, the underlying model is usually unknown. \n",
    "\n",
    "Model based-regression methods exploit the idea of using the training data to estimate the posterior distribution $p(s|{\\bf x})$ and then apply estimation theory to make predictions.\n",
    "\n",
    "<center>\n",
    "<img src=\"figs/ModelBasedReg.png\" width=280>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 1.4. Parametric model-based regression\n",
    "\n",
    "In some cases, we may have a partial knowledge about the underlying mode. In this notebook we will assume that $p$ belongs to a parametric family of distributions $p(s|{\\bf x},{\\bf w})$, where ${\\bf w}$ is some unknown parameter. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Exercise 2:\n",
    "\n",
    "Assume the target variable $s$ is a scaled noisy version of the input variable $x$: \n",
    "$$\n",
    "s = w x + \\epsilon\n",
    "$$\n",
    "where $\\epsilon$ is Gaussian a noise variable with zero mean and unit variance, which does not depend on $x$. Assume that $w$ is known. \n",
    "  1. Compute the target model $p(s| x, w)$\n",
    "  2. Compute prediction $\\hat{s}_\\text{MAP}$ for an arbitrary input $x$\n",
    "  3. Compute prediction $\\hat{s}_\\text{MSE}$ for an arbitrary input $x$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Solution:\n",
    "\n",
    "[comment]: # (<SOL>)\n",
    "[comment]: # (</SOL>)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "We will use the training data to estimate ${\\bf w}$\n",
    "\n",
    "<center>\n",
    "<img src=\"figs/ParametricReg.png\" width=300>\n",
    "</center>\n",
    "\n",
    "The estimation of ${\\bf w}$ from a given dataset $\\mathcal{D}$ is the goal of the following sections"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## 2. Maximum Likelihood parameter estimation.\n",
    "\n",
    "The ML (Maximum Likelihood) principle is well-known in statistics and can be stated as follows: take the value of the parameter to be estimated (in our case, ${\\bf w}$) that best explains the given observations (in our case, the training dataset $\\mathcal{D}$). Mathematically, this can be expressed as follows:\n",
    "$$\n",
    "\\hat{\\bf w}_{\\text{ML}} = \\arg \\max_{\\bf w} p(\\mathcal{D}|{\\bf w})\n",
    "$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Exercise 3:\n",
    "\n",
    "All samples in dataset ${\\cal D} = \\{(x_k, s_k), k=0,\\ldots,K-1 \\}$ \n",
    "$$\n",
    "s_k = w \\cdot x_k + \\epsilon_k\n",
    "$$\n",
    "where $\\epsilon_k$ are i.i.d. (independent and identically distributed) Gaussian noise random variables with zero mean and unit variance, which do not depend on $x_k$. \n",
    "\n",
    "Compute the ML estimate, $\\hat{w}_{\\text{ML}}$, of $w$.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Solution:\n",
    "\n",
    "[comment]: # (<SOL>)\n",
    "# </SOL>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* **4.2.** Compute the ML estimate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# wML = <FILL IN>\n",
    "\n",
    "print(\"The ML estimate is {}\".format(wML))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* **4.3.** Plot the likelihood as a function of parameter $w$ along the interval $-0.5\\le w \\le 2$, verifying that the ML estimate takes the maximum value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "sigma_eps = 1\n",
    "K = len(s)\n",
    "wGrid = np.arange(-0.5, 2, 0.01)\n",
    "\n",
    "p = []\n",
    "for w in wGrid:\n",
    "    d = s - X*w\n",
    "    # p.append(<FILL IN>)\n",
    "\n",
    "# Compute the likelihood for the ML parameter wML\n",
    "# d = <FILL IN>\n",
    "# pML = [<FILL IN>]\n",
    "\n",
    "# Plot the likelihood function and the optimal value\n",
    "plt.figure()\n",
    "plt.plot(wGrid, p)\n",
    "plt.stem([wML], pML)\n",
    "plt.xlabel('$w$')\n",
    "plt.ylabel('Likelihood function')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* **4.4.** Plot the prediction function on top of the data scatter plot "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "xgrid = np.arange(0, 1.2, 0.01)\n",
    "# sML = <FILL IN>\n",
    "\n",
    "plt.figure()\n",
    "plt.scatter(X, s)\n",
    "# plt.plot(<FILL IN>)\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('s')\n",
    "plt.axis('tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 2.1. Model assumptions\n",
    "\n",
    "In order to solve exercise 4 we have taken advantage of the statistical independence of the noise components. Some independence assumptions are required in general to compute the ML estimate in other scenarios.\n",
    "\n",
    "In order to estimate ${\\bf w}$ from the training data in a mathematicaly rigorous and compact form let us group the target variables into a vector\n",
    "$$\n",
    "{\\bf s} = \\left(s_0, \\dots, s_{K-1}\\right)^\\top\n",
    "$$\n",
    "and the input vectors into a matrix\n",
    "$$\n",
    "{\\bf X} = \\left({\\bf x}_0, \\dots, {\\bf x}_{K-1}\\right)^\\top\n",
    "$$\n",
    "\n",
    "We will make the following assumptions:\n",
    "\n",
    "   * A1. All samples in ${\\cal D}$ have been generated by the same distribution, $p({\\bf x}, s \\mid {\\bf w})$\n",
    "   * A2. Input variables ${\\bf x}$ do not depend on ${\\bf w}$. This implies that\n",
    "$$\n",
    "p({\\bf X} \\mid {\\bf w}) = p({\\bf X})\n",
    "$$\n",
    "   * A3. Targets $s_{0},\\ldots, s_{K-1}$ are statistically independent, given ${\\bf w}$ and the inputs ${\\bf x}_0,\\ldots, {\\bf x}_{K-1}$, that is:\n",
    "$$\n",
    "p({\\bf s} \\mid {\\bf X}, {\\bf w}) = \\prod_{k=0}^{K-1} p(s_k \\mid {\\bf x}_k, {\\bf w})\n",
    "$$\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Since ${\\cal D} = ({\\bf X}, {\\bf s})$, we can write\n",
    "$$p(\\mathcal{D}|{\\bf w}) \n",
    "    = p({\\bf s}, {\\bf X}|{\\bf w}) \n",
    "    = p({\\bf s} | {\\bf X}, {\\bf w}) p({\\bf X}|{\\bf w})\n",
    "$$\n",
    "Using assumption A2,\n",
    "$$\n",
    "p(\\mathcal{D}|{\\bf w}) \n",
    "    = p({\\bf s} | {\\bf X}, {\\bf w}) p({\\bf X})\n",
    "$$\n",
    "\n",
    "and, finally, using assumption A3, we can express the estimation problem as the computation of\n",
    "\n",
    "\\begin{align}\n",
    "\\hat{\\bf w}_{\\text{ML}}\n",
    "                &= \\arg \\max_{\\bf w} p({\\bf s}|{\\bf X},{\\bf w})  \\\\\n",
    "   \\qquad \\quad &= \\arg \\max_{\\bf w} \\prod_{k=0}^{K-1} p(s_k \\mid {\\bf x}_k, {\\bf w})       \\\\\n",
    "   \\qquad \\quad &= \\arg \\max_{\\bf w} \\sum_{k=0}^{K-1}\\log  p(s_k \\mid {\\bf x}_k, {\\bf w})   \\\\\n",
    "   \\qquad \\quad &= \\arg \\max_{\\bf w} {\\rm L}({\\bf w})\n",
    "\\end{align}\n",
    "where\n",
    "\\begin{align}\n",
    "{\\rm L}({\\bf w}) = \\sum_{k=0}^{K-1}\\log  p(s_k \\mid {\\bf x}_k, {\\bf w})\n",
    "\\end{align}\n",
    "is usually named the **log-likelihood** function. The logarithm transform products into sums, and exponentials (very common in distribution models) into their exponents, which usually simplifies the optimization.\n",
    "\n",
    "The ML estimate is also frequently expressed as a function of the negative log-likelihood function, that is\n",
    "\\begin{align}\n",
    "\\hat{\\bf w}_{\\text{ML}} &= \\arg \\min_{\\bf w} \\text{NLL}({\\bf w})\n",
    "\\end{align}\n",
    "where ${\\rm NLL}({\\bf w}) = -L({\\bf w})$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 2.2. Summary.\n",
    "\n",
    "Let's summarize what we need to do in order to design a regression algorithm based on ML estimation:\n",
    "\n",
    "1. Assume a parametric data model $p(s| {\\bf x},{\\bf w})$\n",
    "2. Using the data model and the i.i.d. assumption, compute $p({\\bf s}| {\\bf X},{\\bf w})$.\n",
    "3. Find an expression for ${\\bf w}_{\\text{ML}}$\n",
    "4. Assuming ${\\bf w} = {\\bf w}_{\\text{ML}}$, compute the MAP or the minimum MSE estimate of $s$ given ${\\bf x}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Exercise 5:\n",
    "\n",
    "Assume the dataset $\\mathcal{D} = \\{(x_k, s_k, k=0,\\ldots, K-1\\}$ contains i.i.d. samples from a distribution with posterior density given by\n",
    "$$\n",
    "p(s \\mid x, w) = w x \\exp(- w x s), \\qquad s\\ge0, \\,\\, x\\ge 0, \\,\\, w\\ge 0\n",
    "$$\n",
    "\n",
    "* **5.1.** For an arbitrary $w$, compute the prediction function based on the estimate $s_{MSE}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "**Solution**:\n",
    "\n",
    "<SOL>\n",
    "</SOL>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "\n",
    "* **5.2.** Determine an expression for the negative log-likelihood function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "\n",
    "**Solution**:\n",
    "<SOL>\n",
    "</SOL>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* **5.3.** Draw the negative log-likelihood function for the dataset in **Exercise 4** in the range $0.01\\le w\\le 15$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "K = len(s)\n",
    "wGrid = np.arange(0.01, 15, 0.01)\n",
    "\n",
    "nll = []\n",
    "Lx = np.sum(np.log(X))\n",
    "xs = np.dot(X,s)\n",
    "for w in wGrid:\n",
    "    # nll.append(<FILL IN>)\n",
    "\n",
    "plt.figure()\n",
    "# plt.plot(<FILL IN>)\n",
    "plt.xlabel('$w$')\n",
    "plt.ylabel('Likelihood function')\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* **5.4.** Determine the maximum likelihood parameter, $w_\\text{ML}$. \n",
    "\n",
    "(*Hint: you can maximize the log-likelihood function instead of the likelihood function in order to simplify the differentiation*)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "**Solution**:\n",
    "\n",
    "<SOL>\n",
    "</SOL>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* **5.5.** Compute $w_\\text{ML}$ for the dataset in **Exercise 4**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# wML = <FILL IN>\n",
    "print(wML)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* **5.6.** Plot the prediction function obtained in ap. 5.1 for $w = w_{\\rm ML}$, and compare it with the linear predictor in exercise 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "xgrid = np.arange(0.1, 1.2, 0.01)\n",
    "# sML = <FILL IN>\n",
    "\n",
    "plt.figure()\n",
    "plt.scatter(X, s)\n",
    "# plt.plot(<FILL IN>)\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('s')\n",
    "plt.axis('tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Subjectively, we can see that the predictor computed in exercise 6 does not fit the given data very well. This could be a false perception. If the data have been truly generated by the parametric model assumed in exercise 6 (i.e. $p(s \\mid x, w) = w x \\exp(- w x s)$, the apparent missbehavior of the estimator could be caused by the natural randomness of the data, and a greater amount of data would show a better adjustement. \n",
    "\n",
    "However, maybe the parametric model chosen for $p(s | x, w)$ is not appropriate: while the data show that $s$ tends to grow with $x$, the prediction function computed in exercise 5.1 is decreasing for all values of parameter $w$ it its domain.\n",
    "\n",
    "On the contrary, the gaussian model in exercise 4 has, at least, captured the positive correlation between x and s.\n",
    "\n",
    "In summary, the choice of the data model is important. In many applications, no parametric data model is available, and the data scientist must make a choice based on the nature of the data or any previous knowledge about the statistical behavior of the data. \n",
    "\n",
    "If no previous information is available, the data scientist can try different models, and compare using validation data and some cross validation technique.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 3. ML estimation for a Gaussian model.\n",
    "\n",
    "The Gaussian model in exercise 4 can be generalized to generate non-linear and multi-dimensional regression functions.\n",
    "\n",
    "### 3.1. Step 1: The Gaussian generative model\n",
    "\n",
    "Let us assume that the target variables $s_k$ in dataset $\\mathcal{D}$ are given by\n",
    "$$\n",
    "s_k = {\\bf w}^\\top {\\bf z}_k + \\varepsilon_k\n",
    "$$\n",
    "\n",
    "where ${\\bf z}_k$ is the result of some transformation of the inputs, ${\\bf z}_k = T({\\bf x}_k)$, and $\\varepsilon_k$ are i.i.d. instances of a Gaussian random variable with mean zero and varianze $\\sigma_\\varepsilon^2$, i.e.,\n",
    "$$\n",
    "p_E(\\varepsilon) = \\frac{1}{\\sqrt{2\\pi}\\sigma_\\varepsilon}\n",
    "                   \\exp\\left(-\\frac{\\varepsilon^2}{2\\sigma_\\varepsilon^2}\\right)\n",
    "$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Assuming that the noise variables are independent of ${\\bf x}$ and ${\\bf w}$, then, the target variable is conditionally gaussian with mean ${\\bf w}^\\top {\\bf z}_k$ and variance $\\sigma_\\varepsilon^2$,\n",
    "$$\n",
    "p(s|{\\bf x}, {\\bf w}) = p_E(s-{\\bf w}^\\top{\\bf z}) =\n",
    "    \\frac{1}{\\sqrt{2\\pi}\\sigma_\\varepsilon}\n",
    "    \\exp\\left(-\\frac{(s-{\\bf w}^\\top{\\bf z})^2}{2\\sigma_\\varepsilon^2}\\right)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### 3.2. Step 2: Likelihood and negative log-likelihood functions\n",
    "\n",
    "Now we need to compute the likelihood function $p({\\bf s}, {\\bf X} | {\\bf w})$. If the samples are i.i.d. we can write\n",
    "\\begin{align}\n",
    "p({\\bf s}| {\\bf X}, {\\bf w})\n",
    "    &= \\prod_{k=0}^{K-1} p(s_k| {\\bf x}_k, {\\bf w})   \\\\\n",
    "    &= \\prod_{k=0}^{K-1} \\frac{1}{\\sqrt{2\\pi}\\sigma_\\varepsilon}\n",
    "                        \\exp\\left(-\\frac{\\left(s_k-{\\bf w}^\\top{\\bf z}_k\\right)^2}\n",
    "                                        {2 \\sigma_{\\varepsilon}^2} \n",
    "                            \\right)   \\\\\n",
    "    &= \\left(\\frac{1}{\\sqrt{2\\pi}\\sigma_\\varepsilon}\\right)^K\n",
    "       \\exp\\left(-\\frac{1}{2\\sigma_\\varepsilon^2}\\sum_{k=1}^K \\left(s_k-{\\bf w}^\\top{\\bf z}_k\\right)^2\\right)\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Finally, grouping variables ${\\bf z}_k$ in\n",
    "$${\\bf Z} = \\left({\\bf z}_0, \\dots, {\\bf z}_{K-1}\\right)^\\top$$\n",
    "we get\n",
    "$$\n",
    "p({\\bf s}| {\\bf X}, {\\bf w})\n",
    "    = \\left(\\frac{1}{\\sqrt{2\\pi}\\sigma_\\varepsilon}\\right)^K\n",
    "      \\exp\\left(-\\frac{1}{2\\sigma_\\varepsilon^2}\\|{\\bf s}-{\\bf Z}{\\bf w}\\|^2\\right)\n",
    "$$\n",
    "\n",
    "Therefore\n",
    "$$\n",
    "{\\rm NLL}({\\bf w})\n",
    "    = K\\log\\left(\\sqrt{2\\pi}\\sigma_\\varepsilon\\right)\n",
    "      + \\frac{1}{2\\sigma_\\varepsilon^2}\\|{\\bf s}-{\\bf Z}{\\bf w}\\|^2\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### 3.3. Step 3: ML estimation.\n",
    "\n",
    "The <b>maximum likelihood</b> solution is then given by:\n",
    "$$\n",
    "{\\bf w}_{ML} = \\arg \\min_{\\bf w} {\\rm NLL}({\\bf w}) = \\arg \\min_{\\bf w} \\|{\\bf s} - {\\bf Z}{\\bf w}\\|^2\n",
    "$$\n",
    "Note that $\\|{\\bf s} - {\\bf Z}{\\bf w}\\|^2$ is the sum or the squared prediction errors (Sum of Squared Errors, SSE) for all samples in the dataset. For this reason, this is also called the **_Least Squares_** (LS) solution.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "The LS solution can be easily computed by differentiation,\n",
    "$$\n",
    "\\nabla_{\\bf w} \\|{\\bf s} - {\\bf Z}{\\bf w}\\|^2\\Bigg|_{{\\bf w} = {\\bf w}_\\text{ML}}\n",
    "= - 2 {\\bf Z}^\\top{\\bf s} + 2 {\\bf Z}^\\top{\\bf Z} {\\bf w}_{\\text{ML}}\n",
    "= {\\bf 0}\n",
    "$$\n",
    "and it is equal to\n",
    "$$\n",
    "{\\bf w}_\\text{ML} = ({\\bf Z}^\\top{\\bf Z})^{-1}{\\bf Z}^\\top{\\bf s}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### 3.4. Step 4: Prediction function.\n",
    "\n",
    "The last step consists on computing an estimate of $s$ by assuming that the true value of the weight parameters is ${\\bf w}_\\text{ML}$. In particular, the minimum MSE estimate is\n",
    "$$\n",
    "\\hat{s}_\\text{MSE} = \\mathbb{E}\\{s|{\\bf x},{\\bf w}_\\text{ML}\\}\n",
    "$$\n",
    "\n",
    "Knowing that, given ${\\bf x}$ and ${\\bf w}$, $s$ is normally distributed with mean ${\\bf w}^\\top {\\bf z}$ we can write\n",
    "$$\n",
    "\\hat{s}_\\text{MSE} = {\\bf w}_\\text{ML}^\\top {\\bf z}\n",
    "$$\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Exercise 6:\n",
    "\n",
    "Assume that the targets in the one-dimensional dataset in exercise 4 have been generated by the polynomial Gaussian model \n",
    "$$\n",
    "s = w_0 + w_1 x + w_2 x^2 + \\epsilon\n",
    "$$\n",
    "(i.e., with ${\\bf z} = T(x) = (1, x, x^2)^\\intercal$) with noise standard deviation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "sigma_eps = 0.3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* **6.1.** Compute the ML estimate for the given datasets. To generate the data matriz ${\\bf Z}$, you might like to use the <a href=https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.PolynomialFeatures.html>`PolynomialFeatures`</a> class from `sklearn`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Compute the extended input matrix Z\n",
    "# g = <FILL IN>\n",
    "# Create the nonlinear transformation object from sklearn\n",
    "poly = PolynomialFeatures(g)\n",
    "\n",
    "# Apply the transformation to the input data using the fit_transform method\n",
    "# Note that we need to reshape the input data X to a 2D array\n",
    "# Z = <FILL IN>\n",
    "\n",
    "# Compute the ML estimate using linalg.lstsq from Numpy.\n",
    "# wML = <FILL IN>\n",
    "print(wML)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* **6.2.** Compute the value of the log-likelihood function for ${\\bf w}={\\bf w}_\\text{ML}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "K = len(s)\n",
    "\n",
    "# Compute the likelihood for the ML parameter wML\n",
    "# d = <FILL IN>\n",
    "\n",
    "# LwML = [<FILL IN>]\n",
    "\n",
    "print(LwML)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* **6.3.** Plot the prediction function over the data scatter plot "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "xgrid = np.arange(0, 1.2, 0.01)\n",
    "nx = len(xgrid)\n",
    "# Apply the polynomial transformation to the grid data in x\n",
    "# Z = <FILL IN>\n",
    "\n",
    "# sML = <FILL IN>\n",
    "\n",
    "plt.figure()\n",
    "plt.scatter(X, s)\n",
    "# plt.plot(<FILL IN>)\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('s')\n",
    "plt.ylim([-0.5, 2])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "You might like to explore the prediction function for higher polynomial degrees, to get a betted adjustment of the data. The higher the degree, the better the approximation, but the higher the risk of overfitting. Once again, in order to select the appropriate degree, we would need to apply a cross-validation procedure."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  },
  "toc-autonumbering": false,
  "toc-showcode": true,
  "toc-showmarkdowntxt": false,
  "toc-showtags": false
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
