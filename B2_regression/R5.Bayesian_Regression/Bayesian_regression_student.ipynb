{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Bayesian Parametric Regression\n",
    "\n",
    "    Notebook version: 1.6 (Sep 29, 2024)\n",
    "\n",
    "    Author: Jerónimo Arenas García (jarenas@tsc.uc3m.es)\n",
    "            Jesús Cid-Sueiro (jesus.cid@uc3m.es)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "    Changes: v.1.0 - First version\n",
    "             v.1.1 - ML Model selection included\n",
    "             v.1.2 - Some typos corrected\n",
    "             v.1.3 - Rewriting text, reorganizing content, some exercises.\n",
    "             v.1.4 - Revised introduction\n",
    "             v.1.5 - Revised notation. Solved exercise 5\n",
    "             v.1.6 - Eq. citation and numbering, revised Secs. 5-6, simplified code, predictive mean and variance defined, shaded exercises.\n",
    "    \n",
    "    Pending changes: * Include regression on the stock data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Import some libraries that will be necessary for working with data and displaying plots\n",
    "\n",
    "# To visualize plots in the notebook\n",
    "%matplotlib inline \n",
    "from IPython import display\n",
    " \n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import scipy.io       # To read matlab files\n",
    "import pylab\n",
    "import time\n",
    "\n",
    "from sklearn.preprocessing import PolynomialFeatures\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A quick note on the mathematical notation\n",
    "\n",
    "In this notebook we will make extensive use of probability distributions.  In general, we will use letter $p$ for probability density functions (pdf). When necessary, we will use, capital subindices to make the random variable explicit. For instance, $p_{{\\bf X}, S}({\\bf x}, s)$ would be the joint pdf of random variables ${\\bf X}$ and $S$ at values ${\\bf x}$ and $s$, respectively. \n",
    "\n",
    "However, to avoid a notation overload, we will omit subindices when they are clear from the context. For instance, we will use $p({\\bf x}, s)$ instead of $p_{{\\bf X}, S}({\\bf x}, s)$.\n",
    "\n",
    "Finally, we will use notation ${\\bf U} \\sim {\\cal N}({\\bf m}, {\\bf V})$ to express that ${\\bf U}$ is a Gaussian random variable with mean ${\\bf m}$ and variance matrix ${\\bf V}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 1. Model-based parametric regression\n",
    "\n",
    "### 1.1. The regression problem.\n",
    "\n",
    "Given an observation vector ${\\bf x}$, the goal of the regression problem is to find a function $f({\\bf x})$ providing *good* predictions about some unknown variable $s$. To do so, we assume that a set of *labelled* training examples, $\\{{\\bf x}_k, s_k\\}_{k=0}^{K-1}$ is available. \n",
    "\n",
    "The predictor function should make good predictions for new observations ${\\bf x}$ not used during training. In practice, this is tested using a second set (the *test set*) of labelled samples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### 1.2. Model-based parametric regression\n",
    "\n",
    "**Model-based regression** methods assume that all data in the training and test dataset have been generated by some stochastic process. In **parametric regression**, we assume that the probability distribution generating the data has a known parametric form, but the values of some parameters are unknown. \n",
    "\n",
    "In particular, in this notebook we will assume the target variables in all pairs $({\\bf x}_k, s_k)$ from the training and test sets have been generated independently from some posterior distribution $p(s| {\\bf x}, {\\bf w})$, were ${\\bf w}$ is some unknown parameter. The training dataset is used to estimate ${\\bf w}$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<img src=\"figs/ParametricReg.png\" width=300>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### 1.3. Model assumptions\n",
    "\n",
    "In order to estimate ${\\bf w}$ from the training data in a mathematicaly rigorous and compact form let us group the target variables into a vector\n",
    "$$\n",
    "{\\bf s} = \\left(s_0, \\dots, s_{K-1}\\right)^\\top\n",
    "$$\n",
    "and the input vectors into a matrix\n",
    "$$\n",
    "{\\bf X} = \\left({\\bf x}_0, \\dots, {\\bf x}_{K-1}\\right)^\\top\n",
    "$$\n",
    "\n",
    "We will make the following **assumptions**:\n",
    "\n",
    "   * A1. All samples in ${\\cal D}$ have been generated by the same distribution, $p({\\bf x}, s \\mid {\\bf w})$\n",
    "   * A2. Input variables ${\\bf x}$ do not depend on ${\\bf w}$. This implies that\n",
    "$$\n",
    "p({\\bf X} \\mid {\\bf w}) = p({\\bf X})\n",
    "$$\n",
    "   * A3. Targets $s_0, \\dots, s_{K-1}$ are statistically independent, given ${\\bf w}$ and the inputs ${\\bf x}_0,\\ldots, {\\bf x}_{K-1}$, that is:\n",
    "$$\n",
    "p({\\bf s} \\mid {\\bf X}, {\\bf w}) = \\prod_{k=0}^{K-1} p(s_k \\mid {\\bf x}_k, {\\bf w})\n",
    "$$\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 2. Bayesian inference.\n",
    "\n",
    "### 2.1. The Bayesian approach\n",
    "\n",
    "The main idea of **Bayesian inference** is the following: assume we want to estimate some unknown variable $U$ given an observed variable $O$. If $U$ and $O$ are random variables, we can describe the relation between $U$ and $O$ through the following functions:\n",
    "\n",
    "   * **Prior distribution**: $p_U(u)$. It describes our uncertainty on the true value of $U$ before observing $O$.\n",
    "   * **Likelihood function**: $p_{O \\mid U}(o \\mid u)$. It describes how the value of the observation is generated for a given value of $U$.\n",
    "   * **Posterior distribution**: $p_{U|O}(u \\mid o)$. It describes our uncertainty on the true value of $U$ once the true value of $O$ is observed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The major component of the Bayesian inference is the posterior distribution. All Bayesian estimates are computed as some of its central statistics (e.g. the mean, the median or the mode), for instance\n",
    "\n",
    "   * **Maximum A Posteriori (MAP) estimate**: $\\qquad{\\widehat{u}}_{\\text{MAP}} = \\arg\\max_u p_{U \\mid O}(u \\mid o)$\n",
    "   * **Minimum Mean Square Error (MSE) estimate**: $\\qquad\\widehat{u}_{\\text{MSE}} = \\mathbb{E}\\{U \\mid O=o\\}$\n",
    "\n",
    "The choice between the MAP or the MSE estimate may depend on practical or computational considerations. From a theoretical point of view, $\\widehat{u}_{\\text{MSE}}$ has some nice properties: it minimizes $\\mathbb{E}\\{(U-\\widehat{u})^2\\}$ among all possible estimates, $\\widehat{u}$, so it is a natural choice. However, it involves the computation of an integral, which may not have a closed-form solution. In such cases, the MAP estimate can be a better choice.\n",
    "\n",
    "The prior and the likelihood function are auxiliary distributions: if the posterior distribution is unknown, it can be computed from them using the Bayes rule:\n",
    "\\begin{equation*}\n",
    "p_{U|O}(u \\mid o) = \\frac{p_{O|U}(o \\mid u) \\cdot p_{U}(u)}{p_{O}(o)}\n",
    "\\end{equation*}\n",
    "\n",
    "In the next two sections we show that the Bayesian approach can be applied to both the prediction and the estimation problems."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 2.2. Bayesian prediction under a known model\n",
    "\n",
    "Assuming that the model parameters ${\\bf w}$ are known, we can apply the Bayesian approach to predict ${\\bf s}$ for an input ${\\bf x}$. In that case, we can take\n",
    "\n",
    "   * Unknown variable: ${\\bf s}$, and\n",
    "   * Observations: ${\\bf x}$\n",
    "\n",
    "an the Bayesian estimates become:\n",
    "\n",
    "* Maximum A Posterior (MAP): \n",
    "<a id=\"eq1\"></a>\n",
    "\\begin{align}\\tag{1}\n",
    "\\qquad\\widehat{s}_{\\text{MAP}} = \\arg\\max_s p(s| {\\bf x}, {\\bf w})\n",
    "\\end{align}\n",
    "* Minimum Mean Square Error (MSE): \n",
    "<a id=\"eq2\"></a>\n",
    "\\begin{align}\\tag{2}\n",
    "\\qquad\\widehat{s}_{\\text{MSE}} = \\mathbb{E}\\{S |{\\bf x}, {\\bf w}\\}\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<div style=\"background-color:  Gainsboro; padding: 10px; padding-left: 40px; width: calc(100% - 60px)\">\n",
    "\n",
    "#### Exercise 1:\n",
    "\n",
    "Assuming \n",
    "$$p(s\\mid x, w) = \\frac{s}{w x^2} \\exp\\left({-\\frac{s^2}{2 w x^2}}\\right), \\qquad s \\geq 0,\n",
    "$$\n",
    "compute the MAP and MSE predictions of $s$ given $x$ and $w$.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Solution:\n",
    "<SOL>\n",
    "</SOL>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### 2.2.1. The Gaussian case\n",
    "\n",
    "The Gaussian model is one of the most widely used in Bayesian regression, mainly because it can be applied to arbitrarily non-linear and multidimensional regression problems. It is based on the assumption that the target variable and the observation are related through the stochastic equation\n",
    "<a id=\"eq3\"></a>\n",
    "$$\\tag{3} s = {\\bf w}^\\top{\\bf z} + \\varepsilon$$\n",
    "where ${\\bf z}=T({\\bf x})$ is a known, possibly nonlinear transformation of the input variables, and $\\varepsilon$ is a zero-mean Gaussian random variable\n",
    "$$ \\varepsilon \\sim {\\cal N}(0, \\sigma_\\varepsilon^2)\n",
    "$$\n",
    "that is independent of ${\\bf w}$ and ${\\bf x}$. This is equivalent to claim that\n",
    "<a id=\"eq4\"></a>\n",
    "$$p(s|{\\bf x}, {\\bf w}) = \n",
    "    \\frac{1}{\\sqrt{2\\pi}\\sigma_\\varepsilon}\n",
    "    \\exp\\left(-\\frac{(s-{\\bf w}^\\top{\\bf z})^2}{2\\sigma_\\varepsilon^2}\\right)\n",
    "\\tag{4}\n",
    "$$\n",
    "\n",
    "For a Gaussian distribution (and for any unimodal symetric distributions) the mean and the mode are the same and, thus, using Eqs. [(1)](#eq1) and [(2)](#eq2),\n",
    "<a id=\"eq5\"></a>\n",
    "$$\\tag{5}\n",
    "\\widehat{s}_\\text{MAP} = \\widehat{s}_\\text{MSE} = {\\bf w}^\\top{\\bf z}\n",
    "$$\n",
    "Such expression includes a linear regression model, where ${\\bf z} = [1; {\\bf x}]$, as well as any other non-linear model as long as it is <i>\"linear in the parameters\"</i>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "### 2.3. Bayesian Inference for Parameter Estimation\n",
    "\n",
    "In a similar way, we can apply Bayesian inference to estimate the model parameters ${\\bf w}$ from a given dataset, $\\cal{D}$. In that case\n",
    "\n",
    "   * the unknown variable is ${\\bf w}$, and\n",
    "   * the observation is $\\cal{D} \\equiv \\{{\\bf X}, {\\bf s}\\}$\n",
    "\n",
    "so that\n",
    "\n",
    "* Maximum A Posterior (MAP): \n",
    "<a id=\"eq6\"></a>\n",
    "$$\\tag{6} \\widehat{\\bf w}_{\\text{MAP}} = \\arg\\max_{\\bf w} p({\\bf w}| {\\cal D})$$\n",
    "* Minimum Mean Square Error (MSE): \n",
    "<a id=\"eq7\"></a>\n",
    "$$\\tag{7} \\widehat{\\bf w}_{\\text{MSE}} = \\mathbb{E}\\{{\\bf W} | {\\cal D}\\}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 3. Bayesian parameter estimation\n",
    "\n",
    "NOTE: Since the training data inputs are known, all probability density functions and expectations in the remainder of this notebook will be conditioned on the data matrix, ${\\bf X}$. To simplify the mathematical notation, from now on we will remove ${\\bf X}$ from all conditions. For instance, we will write  $p({\\bf s}|{\\bf w})$ instead of  $p({\\bf s}|{\\bf w}, {\\bf X})$, etc. Keep in mind that, in any case, all probabilities and expectations may depend on ${\\bf X}$ implicitely.\n",
    "\n",
    "Summarizing, the steps to design a Bayesian parametric regresion algorithm are the following:\n",
    "\n",
    "1. Assume a parametric data model $p(s| {\\bf x},{\\bf w})$ and a prior distribution $p({\\bf w})$.\n",
    "2. Using the data model and the i.i.d. assumption, compute $p({\\bf s}|{\\bf w})$.\n",
    "3. Applying the bayes rule, compute the posterior distribution $p({\\bf w}|{\\bf s})$.\n",
    "4. Compute the MAP or the MSE estimate of ${\\bf w}$ given ${\\bf x}$.\n",
    "5. Compute predictions using the selected estimate.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 3.1. Bayesian Inference and Maximum Likelihood.\n",
    "\n",
    "Applying the Bayes rule the MAP estimate in Eq. [(6)](#eq6) can be alternatively expressed as\n",
    "<a id=\"eq8\"></a>\n",
    "\\begin{align}\n",
    "\\widehat{\\bf w}_{\\text{MAP}}\n",
    "   &= \\arg\\max_{\\bf w} \\frac{p({\\cal D}| {\\bf w}) \\cdot p({\\bf w})}{p({\\cal D})} \\nonumber \\\\\n",
    "   &= \\arg\\max_{\\bf w} \\left\\{p({\\cal D}| {\\bf w}) \\cdot p({\\bf w})\\right\\}\n",
    "   \\tag{8}\n",
    " \\end{align}\n",
    "\n",
    "By comparison, the ML (Maximum Likelihood) estimate has the form:\n",
    "$$\n",
    "\\widehat{\\bf w}_{\\text{ML}} = \\arg \\max_{\\bf w} p(\\mathcal{D}|{\\bf w})\n",
    "$$\n",
    "\n",
    "This shows that the MAP estimate takes into account the prior distribution on the unknown parameter.\n",
    "\n",
    "Another advantage of the Bayesian approach is that it provides not only a point estimate of the unknown parameter, but a whole funtion, the posterior distribution, which encompasses our belief on the unknown parameter given the data. For instance, we can take second order statistics like the variance of the posterior distributions to measure the uncertainty on the true value of the parameter around the mean.\n",
    "\n",
    "The MAP estimate can also be expressed as a function of the negative-log likelihood. Applying the minus logarithm over Eq. [(8)](#eq8), we get\n",
    "\\begin{align*}\n",
    "\\widehat{\\bf w}_{\\text{MAP}} \n",
    "   &= \\arg\\min_{\\bf w} \\left\\{\\text{NLL}(w) - \\log\\left(p({\\bf w})\\right)\\right\\}  \\nonumber\\\\\n",
    "   &= \\arg\\min_{\\bf w} \\left\\{- \\sum_{k=0}^{K-1}\\log(p(s_k|x_k, {\\bf w})) - \\log\\left(p({\\bf w})\\right)\\right\\}\n",
    "\\end{align*}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 3.2. The prior distribution\n",
    "\n",
    "Since each value of ${\\bf w}$ determines a regression function, by stating a prior distribution over the weights we state also a prior distribution over the space of regression functions.\n",
    "\n",
    "For instance, assume that the data likelihood follows the Gaussian model in Eq. [(3)](#eq3), with $T(x) = (1, x, x^2, x^3)$, i.e. the regression functions have the form\n",
    "$$\n",
    "w_0 + w_1 x + w_2 x^2 + w_3 x^3\n",
    "$$\n",
    "\n",
    "Each value of ${\\bf w}$ determines a specific polynomial of degree 3. Thus, the prior distribution over ${\\bf w}$ describes which polynomials are more likely before observing the data.\n",
    "\n",
    "For instance, assume a Gaussian prior with zero mean and variance ${\\bf V}_p$, i.e.,\n",
    "$$\n",
    "p({\\bf w}) = \\frac{1}{(2\\pi)^{D/2} |{\\bf V}_p|^{1/2}} \n",
    "              \\exp \\left(-\\frac{1}{2} {\\bf w}^\\intercal {\\bf V}_{p}^{-1}{\\bf w} \\right)\n",
    "$$\n",
    "where $D$ is the dimension of ${\\bf w}$. To abbreviate, we will also express this as\n",
    "$${\\bf w} \\sim {\\cal N}\\left({\\bf 0},{\\bf V}_{p} \\right)$$\n",
    "The following code samples ${\\bf w}$ according to this distribution for ${\\bf V}_p = 0.002 \\, {\\bf I}$, and plots the resulting polynomial over the scatter plot of an arbitrary dataset.\n",
    "\n",
    "You can check the effect of modifying the variance of the prior distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "n_grid = 200\n",
    "degree = 3\n",
    "nplots = 200\n",
    "\n",
    "# Prior distribution parameters\n",
    "mean_w = np.zeros((degree + 1,))\n",
    "v_p = 0.2     ### Try increasing this value\n",
    "var_w = v_p * np.eye(degree+1)\n",
    "\n",
    "xmin = -1\n",
    "xmax = 1\n",
    "X_grid = np.linspace(xmin, xmax, n_grid)\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "\n",
    "for k in range(nplots):\n",
    "    \n",
    "    #Draw weigths fromt the prior distribution\n",
    "    w_iter = np.random.multivariate_normal(mean_w, var_w)\n",
    "    S_grid_iter = np.polyval(w_iter, X_grid)\n",
    "    ax.plot(X_grid, S_grid_iter, 'g-', lw=0.5)\n",
    "\n",
    "ax.set_xlim(xmin, xmax)\n",
    "ax.set_ylim(-1, 1)\n",
    "ax.set_xlabel('$x$')\n",
    "ax.set_ylabel('$s$')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The data observation will modify our belief about the true data model according to the posterior distribution. In the following we will analyze this in a Gaussian case."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 4. Bayesian regression for a Gaussian model.\n",
    "\n",
    "We will apply the above steps to derive a Bayesian regression algorithm for a Gaussian model.\n",
    "\n",
    "### 4.1. Step 1: The Gaussian model.\n",
    "\n",
    "Let us assume that the likelihood function is given by the Gaussian model in Eq. [(3)](#eq3)\n",
    "\n",
    "$$\n",
    "s~|~{\\bf w} \\sim {\\cal N}\\left({\\bf z}^\\top{\\bf w}, \\sigma_\\varepsilon^2 \\right)\n",
    "$$\n",
    "\n",
    "Assume, also, that the prior is Gaussian\n",
    "\n",
    "$$\n",
    "{\\bf w} \\sim {\\cal N}\\left({\\bf 0},{\\bf V}_{p} \\right)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### 4.2. Step 2: Complete data likelihood\n",
    "\n",
    "Using the assumptions A1, A2 and A3 (see Notebook about ML regression), it can be shown that\n",
    "\n",
    "$$\n",
    "{\\bf s}~|~{\\bf w} \\sim {\\cal N}\\left({\\bf Z}{\\bf w},\\sigma_\\varepsilon^2 {\\bf I} \\right)\n",
    "$$\n",
    "that is\n",
    "$$\n",
    "p({\\bf s}| {\\bf w})\n",
    "    = \\frac{1}{\\left(\\sqrt{2\\pi}\\sigma_\\varepsilon\\right)^K}\n",
    "      \\exp\\left(-\\frac{1}{2\\sigma_\\varepsilon^2}\\|{\\bf s}-{\\bf Z}{\\bf w}\\|^2\\right)\n",
    "$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### 4.3. Step 3: Posterior weight distribution\n",
    "\n",
    "The posterior distribution of the weights can be computed using the Bayes rule\n",
    "\n",
    "$$p({\\bf w}|{\\bf s}) = \\frac{p({\\bf s}|{\\bf w})~p({\\bf w})}{p({\\bf s})}$$\n",
    "\n",
    "Since both $p({\\bf s}|{\\bf w})$ and $p({\\bf w})$ follow a Gaussian distribution, we know also that the joint distribution and the posterior distribution of ${\\bf w}$ given ${\\bf s}$ are also Gaussian. Therefore,\n",
    "\n",
    "$${\\bf w}~|~{\\bf s} \\sim {\\cal N}\\left({\\bf w}_\\text{MSE}, {\\bf V}_{\\bf w}\\right)$$\n",
    "\n",
    "It can be shown that mean and the covariance matrix of the distribution are:\n",
    "<a id=\"eq9\"></a>\n",
    "$$\\tag{9} \\boxed{{\\bf V_w} = \\left[\\frac{1}{\\sigma_\\varepsilon^2} {\\bf Z}^{\\top}{\\bf Z} \n",
    "                           + {\\bf V}_p^{-1}\\right]^{-1}}$$\n",
    "<a id=\"eq10\"></a>\n",
    "$$\\tag{10} \\boxed{{\\bf w}_\\text{MSE} = {\\sigma_\\varepsilon^{-2}} {\\bf V}_{\\bf w} {\\bf Z}^\\top {\\bf s}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<div style=\"background-color:  Gainsboro; padding: 10px; padding-left: 40px; width: calc(100% - 60px)\">\n",
    "\n",
    "#### Exercise 2: \n",
    "\n",
    "Consider the dataset with one-dimensional inputs given by\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# True data parameters\n",
    "w_true = 3\n",
    "std_n = 0.4\n",
    "\n",
    "# Generate the whole dataset\n",
    "n_max = 64\n",
    "X_tr = 3 * np.random.random((n_max,1)) - 0.5\n",
    "S_tr =  w_true * X_tr + std_n * np.random.randn(n_max,1)\n",
    "\n",
    "# Plot data\n",
    "plt.figure()\n",
    "plt.plot(X_tr, S_tr, 'b.')\n",
    "plt.xlabel('$x$')\n",
    "plt.ylabel('$s$')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<div style=\"background-color:  Gainsboro; padding: 10px; padding-left: 40px; width: calc(100% - 60px)\">\n",
    "\n",
    "Fit a Bayesian linear regression model assuming $z= x$ and\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Model parameters\n",
    "sigma_eps = 0.4\n",
    "mean_w = 1           # Since w is a scalar, mean_w is a scalar, too\n",
    "sigma_p = 1e6\n",
    "Var_p = sigma_p**2   # Since w is a scalar, Var_p is a scalar, too"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<div style=\"background-color:  Gainsboro; padding: 10px; padding-left: 40px; width: calc(100% - 60px)\">\n",
    "\n",
    "(note that parameter $w$ is a scalar). \n",
    "\n",
    "To do so, compute the posterior weight distribution using the first $k$ samples in the complete dataset, for $k = 1,2,4,8,\\ldots 128$. Draw all these posteriors along with the prior distribution in the same plot.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# No. of points to analyze\n",
    "n_points = [1, 2, 4, 8, 16, 32, 64, 128]\n",
    "\n",
    "# Prepare plots\n",
    "w_grid = np.linspace(2, 4, 5000)   # Sample the w axis\n",
    "plt.figure()\n",
    "\n",
    "# Compute the prior distribution over the grid points in w_grid\n",
    "# p = <FILL IN>\n",
    "plt.plot(w_grid, p,'g-')\n",
    "\n",
    "for k in n_points:\n",
    "\n",
    "    # Select the first k samples\n",
    "    Zk = X_tr[:k, :]\n",
    "    Sk = S_tr[:k]\n",
    "\n",
    "    # Parameters of the posterior distribution\n",
    "    # 1. Compute the posterior variance.\n",
    "    #    (Make sure that the resulting variable, Var_w, is a scalar.)\n",
    "    # Var_w = <FILL IN>\n",
    "    \n",
    "    # 2. Compute the posterior mean.\n",
    "    #    (Make sure that the resulting variable, w_MSE, is a scalar)\n",
    "    # w_MSE = <FILL IN>\n",
    "\n",
    "    # Compute the posterior distribution over the grid points in w_grid\n",
    "    sigma_w = np.sqrt(Var_w)     # First we take a scalar standard deviation\n",
    "    # p = <FILL IN>\n",
    "    \n",
    "    plt.plot(w_grid, p,'g-')\n",
    "    plt.fill_between(w_grid, 0, p, alpha=0.5, edgecolor='#1B2ACC', facecolor='#089FFF',\n",
    "                     linewidth=0.8, antialiased=True)\n",
    "    plt.title(f'Posterior distribution after {k} samples')\n",
    "    plt.xlim(w_grid[0], w_grid[-1])\n",
    "    plt.ylim(0, 12)\n",
    "    plt.xlabel('$w$')\n",
    "    plt.ylabel('$p(w|s)$')\n",
    "\n",
    "    display.clear_output(wait=True)\n",
    "    display.display(plt.gcf())\n",
    "    time.sleep(2.0)\n",
    "\n",
    "# Remove the temporary plots and fix the last one\n",
    "display.clear_output(wait=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<div style=\"background-color:  Gainsboro; padding: 10px; padding-left: 40px; width: calc(100% - 60px)\">\n",
    "\n",
    "#### Exercise 3: \n",
    "\n",
    "Note that, in the example above, the model assumptions are correct: the target variables have been generated by a linear model with noise standard deviation `sigma_n` which is exactly equal to the value assumed by the model, stored in variable `sigma_eps`. Check what happens if we take `sigma_eps=4*sigma_n` or `sigma_eps=sigma_n/4`. \n",
    "\n",
    "* Does the algorithm fail in that cases?\n",
    "* What differences can you observe with respect to the ideal case `sigma_eps=sigma_n`?\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### 4.4. Step 4: Weight estimation.\n",
    "\n",
    "Since the posterior weight distribution is Gaussian, both the MAP and the MSE estimates are equal to the posterior mean in Eq. [(10)](#eq10):\n",
    "\n",
    "$$\\widehat{\\bf w}_\\text{MAP} = \\widehat{\\bf w}_\\text{MSE} = {\\sigma_\\varepsilon^{-2}} {\\bf V}_{\\bf w} {\\bf Z}^\\top {\\bf s}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### 4.5. Step 5: Prediction\n",
    "\n",
    "Using the MSE estimate, as in Eq. [(5)](#eq5), the final predictions are given by\n",
    "$$\n",
    "\\widehat{s}_\\text{MSE} =  \\widehat{\\bf w}_\\text{MSE}^\\top{\\bf z}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<div style=\"background-color:  Gainsboro; padding: 10px; padding-left: 40px; width: calc(100% - 60px)\">\n",
    "\n",
    "#### Exercise 4:\n",
    "\n",
    "Plot the minimum MSE predictions of $s$ for inputs $x$ in the interval [-1, 3].\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Make grid on points (two points suffices to plot a line)\n",
    "# x = <FILL IN>\n",
    "# Compute the predicted values for the points in the grid\n",
    "# s_pred = <FILL IN>\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(X_tr, S_tr,'b.')\n",
    "plt.plot(x, s_pred)\n",
    "plt.xlabel('$x$')\n",
    "plt.ylabel('$s$')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. The predictive mean.\n",
    "\n",
    "Up to here, we have followed a Bayesian approach in two steps:\n",
    "\n",
    "  1. **Learning**: use $p({\\bf w}\\mid \\cal D)$ to compute the MSE estimate, ${\\bf w}_\\text{MSE}$.\n",
    "  2. **Prediction**: use $p(s \\mid {\\bf x}, {\\bf w})$ to compute prediction $\\hat{s}_\\text{MSE} = \\mathbb{E}\\{s \\mid {\\bf x}, {\\bf w}_\\text{MSE}\\}$\n",
    "\n",
    "From a purely Bayesian perspective, if the goal is to predict $s$ (the unknown variable) for an input ${\\bf x}$ and from the data in ${\\cal D}$ (the observarions) we can use $p(s \\mid {\\bf x}, {\\cal D})$ to compute estimate\n",
    "\n",
    "\\begin{align*}\n",
    "\\hat{s}^* &= \\mathbb{E}\\{s | {\\bf x}, {\\cal D} \\}    \\nonumber\\\\\n",
    "          &= \\int \\mathbb{E}\\{s | {\\bf x}, {\\bf w}, {\\cal D} \\} p({\\bf w} \\mid {\\cal D}) d{\\bf w}   \\nonumber\\\\\n",
    "          &= \\int \\mathbb{E}\\{s | {\\bf x}, {\\bf w} \\} p({\\bf w} \\mid {\\cal D}) d{\\bf w}   \\tag{10}\n",
    "\\end{align*}\n",
    "where we have applied the total expectation theorem in the second equality, and the independence assumption in the third one. The Bayesian estimate $\\hat{s}^*$ is known as the **predictive mean**. \n",
    "\n",
    "From Eq. [(10)](#eq10), we can interpret the predictive mean as the aggregation of all possible estimates of the target variable (one per each ${\\bf w}$), weighted by the posterior parameter distribution. That is, the purely Bayesian estimate integrates all possible predictors for a given model into a single one.\n",
    "\n",
    "In general, $\\hat{s}_\\text{MSE}$ and $\\hat{s}^*$ are different estimations, and $\\hat{s}^*$ may be hard to compute. Fortunatelly, for the Gaussian model, they are equivalent, which can be shown by noting that \n",
    "\\begin{align*}\n",
    "\\hat{s}^* &= \\int_{\\bf w} {\\bf w}^\\top{\\bf z} p({\\bf w} \\mid {\\cal D}) d{\\bf w}  \\\\\n",
    "          &= \\left(\\int_{\\bf w} {\\bf w} p({\\bf w} \\mid {\\cal D}) d{\\bf w}\\right)^\\top{\\bf z}  \\\\\n",
    "          &= {\\bf w}_\\text{MSE}^\\top{\\bf z} = \\hat{s}_\\text{MSE}  \\\\\n",
    "\\end{align*}\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## 6. Posterior distribution of the target variable\n",
    "\n",
    "One of the powerfull advantages of the Bayesian regression models is that they provide no only a prediction of the target variable, but a posterior distribution, which may be very informative of the quality of the predictions.\n",
    "\n",
    "For the Gaussian model, the computation of the posterior distribution is straightforward. Eq. [(3)](#eq3) shows that the target variable is the sum of two components:\n",
    "$$s = f + \\varepsilon\n",
    "$$\n",
    "were $f = {\\bf w}^\\top{\\bf z}$. The prediction error is a consequence of the randomness of the noise (which cannot be predicted, since it is independent on the date) and the uncertainty about ${\\bf w}$, which can be reduced using data.\n",
    "\n",
    "Since $f$ is a linear combination of random variables, it is a Gaussian random variable, with mean and variance\n",
    "\\begin{align*}\n",
    "\\mathbb{E}\\{f \\mid {\\bf x}, {\\cal D}\\} \n",
    "   &= \\mathbb{E}\\{s \\mid {\\bf x}, {\\cal D}\\}  \n",
    "    =  \\widehat{\\bf w}_\\text{MSE} ^\\top {\\bf z}     \\\\\n",
    "%   &=  {\\sigma_\\varepsilon^{-2}} {{\\bf z}}^\\top {\\bf V}_{\\bf w} {\\bf Z}^\\top {\\bf s}   \\\\\n",
    "\\text{var}(f \\mid {\\bf x}, {\\cal D}) \n",
    "   &= \\mathbb{E}\\{(f-{\\bf w}_\\text{MSE}^\\top{\\bf z})^2 \\mid {\\bf x}, {\\cal D}\\}   \\\\\n",
    "   &= \\mathbb{E}\\{(({\\bf w}-{\\bf w}_\\text{MSE})^\\top{\\bf z})^2 \\mid {\\bf x}, {\\cal D}\\}  \\\\\n",
    "              &= {\\bf z}^\\top {\\bf V}_{\\bf w} {{\\bf z}}\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Therefore, \n",
    "$$\n",
    "f^*~|~{\\bf s}, {\\bf x} \n",
    "   \\sim {\\cal N}\\left(\\widehat{\\bf w}_\\text{MSE} ^\\top {\\bf z}, ~~ \n",
    "                      {\\bf z}^\\top {\\bf V}_{\\bf w} {\\bf z} \\right)\n",
    "$$\n",
    "and, since $s = f + \\varepsilon$, which is the sum of independent variables the posterior distribution of the target variable  \n",
    "$$\n",
    "s ~|~{\\bf s}, {\\bf z}^* \n",
    "   \\sim {\\cal N}\\left(\\widehat{\\bf w}_\\text{MSE} ^\\top {\\bf z}, ~~\n",
    "                      {\\bf z}^\\top {\\bf V}_{\\bf w} {\\bf z} + \\sigma_\\varepsilon^2\\right)\n",
    "$$\n",
    "\n",
    "The variance of the posterior distribution,\n",
    "$$\n",
    "v_{s|{\\bf x}} = {\\bf z}^\\top {\\bf V}_{\\bf w} {\\bf z} + \\sigma_\\varepsilon^2\n",
    "$$\n",
    "is known as the **predictive variance**, and it accounts for the variance of the target variable for each observation. Note that the noise variance cannot be reduced, but more data reduce the variance of the parameter estimates, and thus the predictive variance, too."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Example: random sinusoid\n",
    "\n",
    "For the remainder of this notebook, we will illustrate the behavior of the Bayesian regression models by using a dataset of noisy samples from a sinusoidal signal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# Configurable parameters\n",
    "n_points = 20\n",
    "frec = 3\n",
    "std_n = 0.2     # Noise standard deviation\n",
    "n_grid = 200\n",
    "\n",
    "# Initialize random generator, for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# Data generation\n",
    "X_tr = 3 * np.random.random((n_points,1)) - 0.5\n",
    "S_tr = - np.cos(frec*X_tr) + std_n * np.random.randn(n_points, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The figure shows the sampled points, and the underlying function (in the dotted curve)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# Signal\n",
    "xmin = np.min(X_tr) - 0.1\n",
    "xmax = np.max(X_tr) + 0.1\n",
    "X_grid = np.linspace(xmin, xmax, n_grid)\n",
    "S_grid = - np.cos(frec*X_grid) #Noise free for the true model\n",
    "\n",
    "# Plot data\n",
    "plt.figure()\n",
    "plt.plot(X_tr, S_tr,'b.',markersize=10)\n",
    "# Plot noise-free function\n",
    "plt.plot(X_grid, S_grid, 'b:', label='Noise-free signal')    \n",
    "\n",
    "# we set some limits for the plot\n",
    "ymin = np.min(S_tr) - 1\n",
    "ymax = np.max(S_tr) + 1\n",
    "\n",
    "# Set axes\n",
    "plt.xlim(xmin, xmax), plt.xlabel('$x$')\n",
    "plt.ylim(ymin, ymax), plt.ylabel('$s$')\n",
    "plt.legend(loc='best')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Let us assume that the cosine form of the noise-free signal is unknown, and we assume a polynomial model with a high degree. The following code plots the ML estimate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# Configurable parameters\n",
    "degree = 18\n",
    "\n",
    "# Compute matrix with training input data using sklearn\n",
    "poly = PolynomialFeatures(degree=degree)\n",
    "\n",
    "# Transform the training data\n",
    "Z_tr = poly.fit_transform(X_tr)\n",
    "# Compute the ML parameters\n",
    "wML = np.linalg.lstsq(Z_tr, S_tr, rcond=1e-20)[0]\n",
    "\n",
    "# Transform the samples in the grid.\n",
    "X_grid_matrix = np.array([X_grid]).T     # Convert to 1-column matrix\n",
    "Z_grid = poly.transform(X_grid_matrix)   # Transform the matrix\n",
    "\n",
    "# Make predictions for the samples in the grid\n",
    "# (and convert to a 1D array, as X_grid, for plotting)\n",
    "S_grid_ML = (Z_grid @ wML).T[0]\n",
    "\n",
    "# Plot data\n",
    "fig = plt.figure()\n",
    "plt.plot(X_tr, S_tr, 'b.', markersize=10)\n",
    "\n",
    "# Plot noise-free function\n",
    "plt.plot(X_grid, S_grid, 'b:', label='Noise-free signal')    \n",
    "# Plot ML regression function\n",
    "plt.plot(X_grid, S_grid_ML, 'm-', label='ML regression')    \n",
    "\n",
    "# Set axis\n",
    "plt.xlim(xmin, xmax), plt.xlabel('$x$')\n",
    "plt.ylim(ymin, ymax), plt.ylabel('$s$')\n",
    "plt.legend(loc='best')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "When the degree of the polynomial is close to the size of the dataset, the ML regresion overfits the training data, and would produce a large prediction errors over an independent test set.\n",
    "\n",
    "The following fragment of code computes the posterior weight distribution, draws random vectors from $p({\\bf w}|{\\bf s})$, and plots the corresponding regression curves along with the training points. Compare these curves with those extracted from the prior distribution of ${\\bf w}$ and with the ML solution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# Configurable parameters\n",
    "nplots = 40\n",
    "# Noise standard deviation\n",
    "sigma_eps = 0.2\n",
    "# Prior distribution parameters\n",
    "mean_w = np.zeros((degree + 1,))\n",
    "sigma_p = 2\n",
    "\n",
    "# Compute posterior distribution parameters\n",
    "Vp = sigma_p**2 * np.eye(degree + 1)\n",
    "inv_Vp = 1 / sigma_p**2 * np.eye(degree + 1)\n",
    "Vw = np.linalg.inv(inv_Vp + Z_tr.T @ Z_tr / (sigma_eps**2))\n",
    "# This is to ensure that Var_w is symmetric, and does not deviate because of \n",
    "# numerical errors\n",
    "Vw = (Vw + Vw.T) / 2\n",
    "# Compute the posterior mean, and convert to 1D array\n",
    "wMSE = Vw @ Z_tr.T @ S_tr / (sigma_eps**2)\n",
    "wMSE = wMSE.T[0]\n",
    "\n",
    "# Plot data\n",
    "plt.figure()\n",
    "plt.plot(X_tr, S_tr,'b.',markersize=10)\n",
    "# Plot noise-free function\n",
    "plt.plot(X_grid, S_grid, 'b:', label='Noise-free signal')    \n",
    "# Plot ML regression function\n",
    "plt.plot(X_grid, S_grid_ML, 'm-', label='LS regression')    \n",
    "\n",
    "for k in range(nplots):    \n",
    "    # Draw weights from the posterior distribution\n",
    "    w_iter = np.random.multivariate_normal(wMSE, Vw)\n",
    "\n",
    "    # Compute and plot predictions for the samples in the grid\n",
    "    S_grid_iter = Z_grid @ w_iter\n",
    "    plt.plot(X_grid, S_grid_iter, 'g-', lw=0.5)\n",
    "\n",
    "# Set axis\n",
    "plt.xlim(xmin, xmax), plt.xlabel('$x$')\n",
    "plt.ylim(ymin, ymax), plt.ylabel('$s$')\n",
    "plt.legend(loc='best')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "As an alternative to the\n",
    "The following fragment of code computes the posterior weight distribution, draws random vectors from $p({\\bf w}|{\\bf s})$, and plots the corresponding regression curves along with the training points. Compare these curves with those extracted from the prior distribution of ${\\bf w}$ and with the LS solution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Compute the posterior mean\n",
    "S_grid_MSE = Z_grid @ wMSE\n",
    "\n",
    "# Compute standard deviations of the posterior distribution for the samples in \n",
    "# the grid\n",
    "Vsx = np.sum(Z_grid * (Z_grid @ Vw), axis=1)\n",
    "std_x = np.sqrt(Vsx)\n",
    "\n",
    "# Plot data\n",
    "plt.figure(figsize=(9, 6))\n",
    "plt.plot(X_tr, S_tr, 'b.', markersize=10)\n",
    "# Plot true function\n",
    "plt.plot(X_grid, S_grid, 'b:', label='Noise-free signal')    \n",
    "# Plot LS regression function\n",
    "plt.plot(X_grid, S_grid_ML, 'm-', label='LS regression')    \n",
    "\n",
    "# Plot predictive mean for the Bayesian Inference\n",
    "plt.plot(X_grid, S_grid_MSE,'g-',label='Predictive mean, BI')\n",
    "# Plot confidence intervals for the Bayesian Inference\n",
    "plt.fill_between(X_grid, S_grid_MSE-std_x, S_grid_MSE+std_x,\n",
    "    alpha=0.4, edgecolor='#1B2ACC', facecolor='#089FFF',\n",
    "    linewidth=2, antialiased=True)\n",
    "\n",
    "# Set axis\n",
    "plt.xlim(xmin, xmax), plt.xlabel('$x$')\n",
    "plt.ylim(ymin, ymax), plt.ylabel('$s$')\n",
    "plt.title('Predicting the target variable')\n",
    "plt.legend(loc='best')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Not only do we obtain a better predictive model, but we also have confidence intervals (error bars) for the predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<div style=\"background-color:  Gainsboro; padding: 10px; padding-left: 40px; width: calc(100% - 60px)\">\n",
    "\n",
    "#### Exercise 5:\n",
    "\n",
    "Assume the dataset ${\\cal{D}} = \\left\\{ x_k, s_k \\right\\}_{k=0}^{K-1}$ containing $K$ i.i.d. samples from a distribution\n",
    "$$p(s|x,w) = w x \\exp(-w x s), \\qquad s>0,\\quad x> 0,\\quad w> 0$$\n",
    "\n",
    "We model also our uncertainty about the value of $w$ assuming a prior distribution for $w$ following a Gamma distribution with parameters $\\alpha>0$ and $\\beta>0$.\n",
    "$$\n",
    "w \\sim \\text{Gamma}\\left(\\alpha, \\beta \\right) \n",
    "    = \\frac{\\beta^\\alpha}{\\Gamma(\\alpha)} w^{\\alpha-1} \\exp\\left(-\\beta w\\right), \\qquad w>0\n",
    "$$\n",
    "\n",
    "Note that the mean and the mode of a Gamma distribution can be calculated in closed-form as\n",
    "$$\n",
    "\\mathbb{E}\\left\\{w\\right\\}=\\frac{\\alpha}{\\beta}; \\qquad\n",
    "$$\n",
    "$$\n",
    "\\text{mode}\\{w\\} = \\arg\\max_w p(w) = \\frac{\\alpha-1}{\\beta}\n",
    "$$\n",
    "\n",
    "**1.** Determine an expression for the likelihood function.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Solution:\n",
    "\n",
    "[comment]: # (<SOL>)\n",
    "[comment]: # (</SOL>)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<div style=\"background-color:  Gainsboro; padding: 10px; padding-left: 40px; width: calc(100% - 60px)\">\n",
    "\n",
    "**2.** Determine the maximum likelihood coefficient,  $\\widehat{w}_{\\text{ML}}$.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "#### Solution:\n",
    "\n",
    "[comment]: # (<SOL>)\n",
    "[comment]: # (</SOL>)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<div style=\"background-color:  Gainsboro; padding: 10px; padding-left: 40px; width: calc(100% - 60px)\">\n",
    "\n",
    "**3.** Obtain the posterior distribution $p(w|{\\bf s})$. Note that you do not need to calculate $p({\\bf s})$ since the posterior distribution can be readily identified as another Gamma distribution.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "#### Solution:\n",
    "\n",
    "[comment]: # (<SOL>)\n",
    "[comment]: # (</SOL>)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<div style=\"background-color:  Gainsboro; padding: 10px; padding-left: 40px; width: calc(100% - 60px)\">\n",
    "\n",
    "**4.** Determine the MSE and MAP a posteriori estimators of $w$: $w_\\text{MSE}=\\mathbb{E}\\left\\{w|{\\bf s}\\right\\}$ and $w_\\text{MAP} = \\max_w p(w|{\\bf s})$.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "#### Solution:\n",
    "\n",
    "[comment]: # (<SOL>)\n",
    "[comment]: # (</SOL>)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<div style=\"background-color:  Gainsboro; padding: 10px; padding-left: 40px; width: calc(100% - 60px)\">\n",
    "\n",
    "**5.** Compute the following estimators of $S$:\n",
    "\n",
    "$\\qquad\\widehat{s}_1 = \\mathbb{E}\\{s|w_\\text{ML},x\\}$\n",
    "\n",
    "$\\qquad\\widehat{s}_2 = \\mathbb{E}\\{s|w_\\text{MSE},x\\}$\n",
    "\n",
    "$\\qquad\\widehat{s}_3 = \\mathbb{E}\\{s|w_\\text{MAP},x\\}$\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "#### Solution:\n",
    "\n",
    "[comment]: # (<SOL>)\n",
    "[comment]: # (</SOL>)\n",
    "    \n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 7. Maximum evidence model selection\n",
    "\n",
    "We have already addressed with Bayesian Inference the following two issues:\n",
    "\n",
    "   - For a given degree, how do we choose the weights?\n",
    "   \n",
    "   - Should we focus on just one model, or can we use several models at once?\n",
    "   \n",
    "However, we still needed some assumptions: a parametric model (i.e., polynomial function and <i>a priori</i> degree selection) and several parameters needed to be adjusted.\n",
    "\n",
    "Though we can recur to cross-validation, Bayesian inference opens the door to other strategies. \n",
    "\n",
    "   - We could argue that rather than keeping single selections of these parameters, we could use simultaneously several sets of parameters (and/or several parametric forms), and average them in a probabilistic way ... (like we did with the models)\n",
    "   \n",
    "   - We will follow a simpler strategy, selecting just the most likely set of parameters according to an ML criterion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 7.1 Model evidence\n",
    "\n",
    "The evidence of a model is defined as\n",
    "\n",
    "$$L = p({\\bf s}~|~{\\cal M})$$\n",
    "\n",
    "where ${\\cal M}$ denotes the model itself and any free parameters it may have. For instance, for the polynomial model we have assumed so far, ${\\cal M}$ would represent the degree of the polynomia, the variance of the additive noise, and the <i>a priori</i> covariance matrix of the weights\n",
    "\n",
    "Applying the Theorem of Total probability, we can compute the evidence of the model as\n",
    "\n",
    "$$L = \\int p({\\bf s}~|~{\\bf f},{\\cal M}) p({\\bf f}~|~{\\cal M}) d{\\bf f} $$\n",
    "\n",
    "For the linear model $f({\\bf x}) = {\\bf w}^\\top{\\bf z}$, the evidence can be computed as\n",
    "\n",
    "$$L = \\int p({\\bf s}~|~{\\bf w},{\\cal M}) p({\\bf w}~|~{\\cal M}) d{\\bf w} $$\n",
    "\n",
    "It is important to notice that these probability density functions are exactly the ones we computed on the previous section. We are just making explicit that they depend on a particular model and the selection of its parameters. Therefore:\n",
    "\n",
    "   - $p({\\bf s}~|~{\\bf w},{\\cal M})$ is the likelihood of ${\\bf w}$\n",
    "   \n",
    "   - $p({\\bf w}~|~{\\cal M})$ is the <i>a priori</i> distribution of the weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 7.2 Model selection via evidence maximization\n",
    "\n",
    "   - As we have already mentioned, we could propose a prior distribution for the model parameters, $p({\\cal M})$, and use it to infer the posterior. However, this can be very involved (usually no closed-form expressions can be derived)\n",
    "   \n",
    "   - Alternatively, maximizing the evidence is normally good enough\n",
    "   \n",
    "   $${\\cal M}_\\text{ML} = \\arg\\max_{\\cal M} p(s~|~{\\cal M})$$\n",
    "   \n",
    "Note that we are using the subscript 'ML' because the evidence can also be referred to as the likelihood of the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 7.3 Example: Selection of the degree of the polynomia\n",
    "\n",
    "For the previous example we had (we consider a spherical Gaussian for the weights):\n",
    "\n",
    "   - ${\\bf s}~|~{\\bf w},{\\cal M}~\\sim~{\\cal N}\\left({\\bf Z}{\\bf w},~\\sigma_\\varepsilon^2 {\\bf I} \\right)$\n",
    "   \n",
    "   - ${\\bf w}~|~{\\cal M}~\\sim~{\\cal N}\\left({\\bf 0},~\\sigma_p^2 {\\bf I} \\right)$\n",
    "   \n",
    "In this case, $p({\\bf s}~|~{\\cal M})$ follows also a Gaussian distribution, and it can be shown that\n",
    "\n",
    "   - $L = p({\\bf s}~|~{\\cal M}) = {\\cal N}\\left({\\bf 0},\\sigma_p^2 {\\bf Z} {\\bf Z}^\\top+\\sigma_\\varepsilon^2 {\\bf I} \\right)$\n",
    "   \n",
    "If we just pursue the maximization of $L$, this is equivalent to maximizing the log of the evidence\n",
    "\n",
    "$$\\log(L) = -\\frac{M}{2} \\log(2\\pi) -{\\frac{1}{2}}\\log\\mid\\sigma_p^2 {\\bf Z} {\\bf Z}^\\top+\\sigma_\\varepsilon^2 {\\bf I}\\mid - \\frac{1}{2} {\\bf s}^\\top \\left(\\sigma_p^2 {\\bf Z} {\\bf Z}^\\top+\\sigma_\\varepsilon^2 {\\bf I}\\right)^{-1} {\\bf s}$$\n",
    "\n",
    "where $M$ denotes the length of vector ${\\bf z}$ (the degree of the polynomia minus 1).\n",
    "   \n",
    "The following fragment of code evaluates the evidence of the model as a function of the degree of the polynomia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Prior distribution parameters\n",
    "max_degree = n_points - 1\n",
    "\n",
    "# Evaluate the posterior evidence\n",
    "logE = []\n",
    "for deg in range(max_degree):\n",
    "    Z_iter = Z_tr[:,:deg+1]\n",
    "    ZZ = Z_iter @ Z_iter.T\n",
    "    logE_iter = - (deg + 1) * np.log(2 * np.pi) / 2   \\\n",
    "                - np.log(np.linalg.det((sigma_p**2) * ZZ + (sigma_eps**2)*np.eye(n_points))) / 2   \\\n",
    "                - S_tr.T @ np.linalg.inv((sigma_p**2) * ZZ + (sigma_eps**2)*np.eye(n_points)) @ S_tr / 2\n",
    "    logE.append(logE_iter[0, 0])\n",
    "\n",
    "plt.plot(np.array(range(max_degree))+1, logE, ':o')\n",
    "plt.xlabel('Polynomial degree')\n",
    "plt.ylabel('log evidence')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "degree = np.argmax(logE)   \n",
    "print(f'The degree of the polynomial is {degree}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's train the model for the maximum degree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "inv_Vp = np.eye(degree+1) / sigma_p**2\n",
    "\n",
    "# Select transformed matrix up to the selected degree\n",
    "Z = Z_tr[:, :(degree+1)]\n",
    "\n",
    "# Compute posterior distribution parameters\n",
    "Vw = np.linalg.inv(Z.T @ Z /(sigma_eps**2) + inv_Vp)\n",
    "wMSE = (Vw @ Z.T @ S_tr / (sigma_eps**2)).T[0]\n",
    "\n",
    "# Compute maximum likelihood solution for the selected degree\n",
    "wML = np.linalg.lstsq(Z, S_tr, rcond=1e-20)[0]\n",
    "\n",
    "# Compute ML and MSE predictions for the grid inputs\n",
    "Z_grid_degree = Z_grid[:, :(degree+1)]\n",
    "S_grid_ML = Z_grid_degree @ wML\n",
    "S_grid_MSE = Z_grid_degree @ wMSE\n",
    "\n",
    "# Compute standard deviations of the posterior distribution for the samples in \n",
    "# the grid\n",
    "Vsx = np.sum(Z_grid_degree * (Z_grid_degree @ Vw), axis=1)\n",
    "std_x = np.sqrt(Vsx)\n",
    "\n",
    "# Plot data\n",
    "fig = plt.figure()\n",
    "plt.plot(X_tr, S_tr,'b.',markersize=10)\n",
    "# Plot true function\n",
    "plt.plot(X_grid, S_grid, 'b:', label='Noise-free signal')    \n",
    "plt.plot(X_grid, S_grid_MSE,'g-',label='Predictive mean, BI')\n",
    "\n",
    "plt.fill_between(X_grid, S_grid_MSE - std_x, S_grid_MSE + std_x,\n",
    "    alpha=0.2, edgecolor='#1B2ACC', facecolor='#089FFF',\n",
    "    linewidth=1, linestyle='dashdot', antialiased=True)\n",
    "\n",
    "#We plot also the least square solution\n",
    "plt.plot(X_grid, S_grid_ML,'m-',label='ML regression')\n",
    "    \n",
    "plt.xlim(xmin, xmax), plt.xlabel('$x$')\n",
    "plt.ylim(ymin, ymax), plt.ylabel('$s$')\n",
    "plt.legend(loc='best')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "We can check, that now the model also seems quite appropriate for ML regression, but keep in mind that selection of such parameter was itself carried out using Bayesian inference."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
