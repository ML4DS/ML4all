{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "# Introduction to Regression.\n",
    "\n",
    "    Author: Jerónimo Arenas García (jarenas@tsc.uc3m.es)\n",
    "            Jesús Cid Sueiro (jcid@tsc.uc3m.es)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "    Notebook version: 1.2 (Sep 14, 2024)\n",
    "\n",
    "    Changes: v.1.0 - First version. Extracted from regression_intro_knn v.1.0.\n",
    "             v.1.1 - (JCS) Compatibility with python 2 and python 3\n",
    "             v.1.2 - (JCS) Removed compatiblity with python 2. Improved text. Concept of generalization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Import some libraries that will be necessary for working with data and displaying plots\n",
    "\n",
    "# To visualize plots in the notebook\n",
    "%matplotlib inline \n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd   # To read data tables from csv files\n",
    "\n",
    "# For plots and graphical results\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D   \n",
    "import pylab\n",
    "\n",
    "# That's default image size for this interactive session\n",
    "pylab.rcParams['figure.figsize'] = 9, 6  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "## 1. The regression problem\n",
    "\n",
    "The goal of regression methods is to predict the value of some *target* variable $S$ from the observation of variables that we will collect in a single vector $\\bf X$).\n",
    "\n",
    "Regression problems arise in situations where the value of the target variable is not easily accessible, but we can measure other dependent variables, from which we can try to predict $S$.  \n",
    "\n",
    "<p align=\"center\">\n",
    "<img src=\"figs/block_diagram.png\" width=400>\n",
    "</p>\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The only information available to estimate the relation between the inputs and the target is a *dataset* $\\mathcal D$ containing several observations of all variables.\n",
    "\n",
    "$$\\mathcal{D} = \\{{\\bf x}_{k}, s_{k}\\}_{k=0}^{K-1}$$\n",
    "\n",
    "The dataset $\\mathcal{D}$ must be used to find a function $f$ that, for any observation vector ${\\bf x}$, computes an output $\\hat{s} = f({\\bf x})$ that is a good predition of the true value of the target, $s$.\n",
    "\n",
    "<p align=\"center\">\n",
    "  <img src=\"figs/predictor.png\" width=300>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 2. Examples of regression problems.\n",
    "\n",
    "The <a href=http://scikit-learn.org/>scikit-learn</a> package contains several <a href=https://scikit-learn.org/stable/datasets.html> datasets</a> related to regression problems. \n",
    "\n",
    "* **Boston dataset**: the target variable contains housing values in different suburbs of Boston. The goal is to predict these values based on several social, economic and demographic variables taken frome theses suburbs. (Dataset no longer available in sklearn).\n",
    "* <a href=http://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_diabetes.html#sklearn.datasets.load_diabetes /> Diabetes dataset</a>.\n",
    "* **California housing**.\n",
    "\n",
    "\n",
    "We can load these datasets as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Boston dataset\n",
    "data_url = \"http://lib.stat.cmu.edu/datasets/boston\"\n",
    "raw_df = pd.read_csv(data_url, sep=\"\\\\s+\", skiprows=22, header=None)\n",
    "X = np.hstack([raw_df.values[::2, :], raw_df.values[1::2, :2]])\n",
    "S = raw_df.values[1::2, 2]\n",
    "\n",
    "n_samples = X.shape[0]  # Number of observations\n",
    "n_vars = X.shape[1]     # Number of variables (including input and target)\n",
    "\n",
    "feature_names = [f'feature $x_{{{i}}}$' for i in range(n_vars)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Alternative dataset 1: Diabetes\n",
    "\n",
    "# from sklearn import datasets\n",
    "# Load the dataset. Select it by uncommenting the appropriate line\n",
    "# D_all = datasets.load_diabetes()\n",
    "\n",
    "# Extract data and data parameters.\n",
    "# X = D_all.data         # Complete data matrix (including input and target variables)\n",
    "# S = D_all.target      # Target variables\n",
    "# feature_names = [f'feature {i}' for i in range(n_vars)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Alternative dataset 2: California Housing\n",
    "# from sklearn.datasets import fetch_california_housing\n",
    "# housing = fetch_california_housing()\n",
    "\n",
    "# Extract data and data parameters.\n",
    "# X = housing.data         # Complete data matrix (including input and target variables)\n",
    "# S = housing.target      # Target variables\n",
    "# info = housing.DESCR\n",
    "# feature_names = housing.feature_names\n",
    "# print(info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_samples = X.shape[0] # Number of observations\n",
    "n_vars = X.shape[1]    # Number of variables (including input and target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "This dataset contains "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "print(n_samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "observations of the target variable and"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "print(n_vars)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "input variables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 3. Scatter plots\n",
    "\n",
    "### 3.1. 2D scatter plots\n",
    "\n",
    "When the instances of the dataset are multidimensional, they cannot be visualized directly, but we can get a first rough idea about the regression task if we plot the target variable versus one of the input variables. These representations are known as <i>scatter plots</i>\n",
    "\n",
    "Python methods `plot` and `scatter` from the `matplotlib` package can be used for these graphical representations. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# Select a dataset\n",
    "nrows = 4\n",
    "ncols = 1 + (X.shape[1]-1) // nrows\n",
    "\n",
    "# Some adjustment for the subplot.\n",
    "pylab.subplots_adjust(hspace=0.3)\n",
    "\n",
    "# Plot all variables\n",
    "for idx in range(X.shape[1]):\n",
    "    ax = plt.subplot(nrows, ncols, idx+1)\n",
    "    ax.scatter(X[:,idx], S, s=5)    # <-- This is the key command\n",
    "    ax.get_xaxis().set_ticks([])\n",
    "    ax.get_yaxis().set_ticks([])\n",
    "    plt.xlabel(f'{feature_names[idx]}')\n",
    "    plt.ylabel('$s$')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 3.2. 3D Plots\n",
    "\n",
    "With the addition of a third coordinate, `plot` and `scatter` can be used for 3D plotting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "##### Exercise 1:\n",
    "\n",
    "Select the `diabetes` dataset. Visualize the target versus components 2 and 5. (You can get more info about the <a href=http://matplotlib.org/api/pyplot_api.html#matplotlib.pyplot.scatter>scatter</a> command and an <a href=http://matplotlib.org/examples/mplot3d/scatter3d_demo.html>example of use</a> in the <a href=http://matplotlib.org/index.html> matplotlib</a> documentation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# <SOL>\n",
    "# </SOL>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "## 4. Evaluating a regression task\n",
    "\n",
    "In order to evaluate the performance of a given predictor, we need to quantify the quality of predictions. This is usually done by means of a loss function $l(s,\\hat{s})$. Two common losses are\n",
    "\n",
    "   * Square error: $l(s, \\hat{s}) = (s - \\hat{s})^2$\n",
    "   * Absolute error: $l(s, \\hat{s}) = |s - \\hat{s}|$\n",
    "\n",
    "Note that both the square and absolute errors are functions of the estimation error $e = s-{\\hat s}$. However, this is not necessarily the case. As an example, imagine a situation in which we would like to introduce a penalty which increases with the magnitude of the estimated variable. For such case, the following cost would better fit our needs: $l(s,{\\hat s}) = s^2 \\left(s-{\\hat s}\\right)^2$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# In this section we will plot together the square and absolute errors\n",
    "grid = np.linspace(-3,3,num=100)\n",
    "plt.plot(grid, grid**2, 'b-', label='Square error')\n",
    "plt.plot(grid, np.absolute(grid), 'r--', label='Absolute error')\n",
    "plt.xlabel('Error')\n",
    "plt.ylabel('Cost')\n",
    "plt.legend(loc='best')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The overal prediction performance is computed as the average of the loss computed over a set of samples:\n",
    "\n",
    "$${\\bar R} = \\frac{1}{K}\\sum_{k=0}^{K-1} l\\left(s_k, \\hat{s}_k\\right)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "##### Exercise 2:\n",
    "\n",
    "The dataset in file `'datasets/x01.csv'`, taken from <a href=\"http://people.sc.fsu.edu/~jburkardt/datasets/regression/x01.txt\">here</a> records the average weight of the brain and body for a number of mammal species.\n",
    "* Represent a scatter plot of the targe variable versus the one-dimensional input.\n",
    "* Plot, over the same plot, the prediction function given by $S = 1.2 X$.\n",
    "* Represent the prediction function in logarithmic scale, using <a href=\"https://matplotlib.org/stable/api/_as_gen/matplotlib.pyplot.loglog.html\">`loglog`</a> instead of `plot`.\n",
    "* Compute the square error rate for the given dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Load dataset in arrays X and S\n",
    "df = pd.read_csv('datasets/x01.csv', sep=',', header=None)\n",
    "X = df.values[:,0]\n",
    "S = df.values[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# <SOL>\n",
    "# </SOL>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.testing.assert_almost_equal(R, 153781.943889, decimal=4)\n",
    "print(\"No error. Test passed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 4.1. Training and Test Data\n",
    "\n",
    "The primary objective of a regression algorithm is to obtain a predictor that performs well on new, unseen inputs. This characteristic is typically referred to as **generalization**.\n",
    "\n",
    "To *evaluate* the generalization capabilities of a regression algorithm, we need data that was not used during the predictor's design. To achieve this, the original dataset is commonly split into at least two disjoint sets:\n",
    "\n",
    "- **Training set**, $\\cal{D}_{\\text{train}}$: Used by the regression algorithm to determine the predictor $f$.\n",
    "- **Test set**, $\\cal{D}_{\\text{test}}$: Used to assess the generalization performance of the regression algorithm on new data.\n",
    "\n",
    "An effective regression algorithm leverages $\\cal{D}_{\\text{train}}$ to obtain a predictor that minimizes the average loss on $\\cal{D}_{\\text{test}}$:\n",
    "$$\n",
    "{\\bar R}_{\\text{test}} = \\frac{1}{K_{\\text{test}}} \n",
    "\\sum_{  ({\\bf x},s) \\in \\mathcal{D}_{\\text{test}}} l(s, f({\\bf x}))\n",
    "$$\n",
    "where $K_{\\text{test}}$ is the size of the test set.\n",
    "\n",
    "The original dataset is typically partitioned into training and test sets at random. This ensures that the statistical distribution is consistent between the two sets, which is crucial for maintaining the generalization ability of the regression algorithm. Otherwise, the generalization cannot be guaranteed.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 5. Parametric and non-parametric regression models\n",
    "\n",
    "Generally speaking, we can distinguish two approaches when designing a regression model:\n",
    "\n",
    "   - Parametric approach: In this case, the estimation function is given <i>a priori</i> a parametric form, and the goal of the design is to find the most appropriate values of the parameters according to a certain goal\n",
    "   \n",
    "   For instance, we could assume a linear expression\n",
    "   $${\\hat s} = f({\\bf x}) = {\\bf w}^\\top {\\bf x}$$\n",
    "   and adjust the parameter vector in order to minimize the average of the quadratic error over the training data. This is known as least-squares regression, and we will study it in a future session.\n",
    "   \n",
    "   - Non-parametric approach: In this case, the analytical shape of the regression model is not assumed <i>a priori</i>.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
