{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Text Analysis\n",
    "\n",
    "\n",
    "Version 1.0\n",
    "\n",
    "Date: Nov 23, 2017\n",
    "\n",
    "Authors: \n",
    "\n",
    "   * Jerónimo Arenas-García (jeronimo.arenas@uc3m.es)\n",
    "   * Jesús Cid-Sueiro (jcid@tsc.uc3m.es)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Common imports \n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import pylab\n",
    "\n",
    "import numpy as np\n",
    "# import pandas as pd\n",
    "# import os\n",
    "from os.path import isfile, join\n",
    "# import scipy.io as sio\n",
    "# import scipy\n",
    "import zipfile as zp\n",
    "# import shutil\n",
    "# import difflib\n",
    "\n",
    "import gensim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Corpus acquisition\n",
    "\n",
    "In this block we will work with collections of text documents. The objectives will be:\n",
    "\n",
    "   * Find the most important topics in the collection and assign documents to topics\n",
    "   * Analyze the structure of the collection by means of graph analysis\n",
    "   \n",
    "We will work with a collection of research projects funded by the US National Science Foundation, that you can find under the `./data` directory. These files are publicly available from the NSF website.\n",
    "\n",
    "(*As a side note, there are many other available text collections to work with. In particular, the NLTK library has many examples, that you can explore using the `nltk.download()` tool*.\n",
    "\n",
    "    import nltk\n",
    "    nltk.download()\n",
    "\n",
    "*for instance, you can take the gutemberg dataset*\n",
    "\n",
    "    Mycorpus = nltk.corpus.gutenberg\n",
    "    text_name = Mycorpus.fileids()[0]\n",
    "    raw = Mycorpus.raw(text_name)\n",
    "    Words = Mycorpus.words(text_name)\n",
    "\n",
    "*Also, tools like Gensim or Sci-kit learn include text databases to work with*).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1. Exploring file structure\n",
    "\n",
    "NSF project information is provided in XML files. Projects are yearly grouped in `.zip` files, and each project is saved in a different XML file. To explore the structure of such files, we will use the file `160057.xml`. Parsing XML files in python is rather easy using the `ElementTree` module. \n",
    "\n",
    "To introduce some common functions to work with XML files we will follow <a href=http://docs.python.org/3.4/library/xml.etree.elementtree.html#module-xml.etree.ElementTree>this tutorial</a>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.1.1. File format\n",
    "\n",
    "To start with, you can have a look at the contents of the example file. We are interested on the following information of each project:\n",
    "\n",
    "   * Project identifier\n",
    "   * Project Title\n",
    "   * Project Abstract\n",
    "   * Budget\n",
    "   * Starting Year (we will ignore project duration)\n",
    "   * Institution (name, zipcode, and state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "xmlfile = '../data/1600057.xml'\n",
    "\n",
    "with open(xmlfile,'r') as fin:\n",
    "    print(fin.read())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.1.2. Parsing XML\n",
    "\n",
    "XML is an inherently hierarchical data format, and the most natural way to represent it is with a tree. The `ElementTree` module has two classes for this purpose:\n",
    "\n",
    "   * `ElementTree` represents the whole XML document as a tree\n",
    "   * `Element` represents a single node in this tree\n",
    "\n",
    "We can import XML data by reading an XML file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "import xml.etree.ElementTree as ET\n",
    "tree = ET.parse(xmlfile)\n",
    "root = tree.getroot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "or directly reading a string:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "root = ET.fromstring(open(xmlfile,'r').read())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`fromstring()` parses XML from a string directly into an `Element`, which is the root element of the parsed tree. Other parsing functions may create an `ElementTree`, but we will not cover them here.\n",
    "\n",
    "As an `Element`, root has a tag and a dictionary of attributes:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "print(root.tag)\n",
    "print(root.attrib)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It also has children nodes over which we can iterate:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "for child in root:\n",
    "    print(child.tag, child.attrib)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Children are nested, and we can access specific child nodes by index. We can also access the text of specified elements. For instance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "for child in root[0]:\n",
    "    print(child.tag, child.attrib, child.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The presented classes and functions are all you need to solve the following exercise. However, there are many other interesting functions that can probably make it easier for you to work with XML files. For more information, please refer to the ElementTree API."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 1**: **Parsing the XML project files**. Complete the code below to implement a function that parses the XML files and provides as its output a dictionary with fields:\n",
    "\n",
    "    project_code      (string)\n",
    "    title             (string)\n",
    "    abstract          (string)\n",
    "    budget            (float)\n",
    "    year              (string)\n",
    "    institution       (tuple with elements: name, zipcode, and statecode)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def parse_xmlproject(xml_string):\n",
    "    \"\"\"This function processess the specified XML field,\n",
    "    and outputs a dictionary with the desired project information\n",
    "    \n",
    "    :xml_string: String with XML content\n",
    "    :Returns: Dictionary with indicated files\n",
    "    \"\"\"\n",
    "    \n",
    "    root = ET.fromstring(xml_string)\n",
    "    dictio = {}\n",
    "    \n",
    "    for child in root[0]:\n",
    "        if child.tag.lower() == 'awardtitle':\n",
    "            dictio['title'] = child.text\n",
    "        #<SOL>\n",
    "        elif child.tag.lower() == 'awardeffectivedate':\n",
    "            dictio['year'] = str(child.text[-4:])\n",
    "        elif child.tag.lower() == 'awardamount':\n",
    "            dictio['budget'] = float(child.text)\n",
    "        elif child.tag.lower() == 'abstractnarration':\n",
    "            dictio['abstract'] = child.text\n",
    "        elif child.tag.lower() == 'awardid':\n",
    "            dictio['project_code'] = child.text\n",
    "        #</SOL>\n",
    "        elif child.tag.lower() == 'institution':\n",
    "            #For the institution we have to access the children elements\n",
    "            #and search for the name, zipcode, and statecode only\n",
    "            name = ''\n",
    "            zipcode = ''\n",
    "            statecode = ''\n",
    "            for child2 in child:\n",
    "                if child2.tag.lower() == 'name':\n",
    "                    name = child2.text\n",
    "                elif child2.tag.lower() == 'zipcode':\n",
    "                    zipcode = child2.text\n",
    "                elif child2.tag.lower() == 'statecode':\n",
    "                    statecode = child2.text\n",
    "            dictio['institution'] = (name, zipcode, statecode)\n",
    "    \n",
    "    return dictio\n",
    "    \n",
    "parse_xmlproject(open(xmlfile,'r').read())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2. Building the dataset\n",
    "\n",
    "Now, we will use the function you just implemented, to create a database that we will use throughout this module.\n",
    "\n",
    "For simplicity, and given that the dataset is not too large, we will keep all projects in the RAM. The dataset will consist of a list containing the dictionaries associated to each of the considered projects in a time interval."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Construct an iterator (or a list) for the years you want to work with\n",
    "years = [2015, 2016]\n",
    "datafiles_path = '../data/'\n",
    "NSF_data = []\n",
    "\n",
    "for year in years:\n",
    "    \n",
    "    zpobj = zp.ZipFile(join(datafiles_path, str(year)+'.zip'))\n",
    "    for fileinzip in zpobj.namelist():\n",
    "        if fileinzip.endswith('xml'):\n",
    "            \n",
    "            #Some files seem to be incorrectly parsed\n",
    "            try:\n",
    "                project_dictio = parse_xmlproject(zpobj.read(fileinzip))\n",
    "                if project_dictio['abstract']:\n",
    "                    NSF_data.append(project_dictio)\n",
    "            except:\n",
    "                pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will extract some characteristics of the constructed dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "print('Number of projects in dataset:', len(NSF_data))\n",
    "\n",
    "####\n",
    "budget_data = list(map(lambda x: x['budget'], NSF_data))\n",
    "print('Average budget of projects in dataset:', np.mean(budget_data))\n",
    "\n",
    "####\n",
    "insti_data = list(map(lambda x: x['institution'], NSF_data))\n",
    "print('Number of unique institutions in dataset:', len(set(insti_data)))\n",
    "\n",
    "####\n",
    "counts = dict()\n",
    "for project in NSF_data:\n",
    "    counts[project['year']] = counts.get(project['year'],0) + 1\n",
    "\n",
    "print('Breakdown of projects by starting year:')\n",
    "for el in counts:\n",
    "    print(el, ':', counts[el])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the rest of this notebook, we will work with the abstracts only. The list of all abstract will be the corpus w will work with."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 2**: Generate a list of abstracts from `NSF_data`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# corpus_raw = <FILL IN>\n",
    "corpus_raw = list(map(lambda x: x['abstract'], NSF_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 3**: Compute the average length of the abstracts of all projects in the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "#<SOL>\n",
    "abstractlen_data = list(map(lambda x: len(x), corpus_raw))\n",
    "print('Average length of projects abstracts (in characters):', np.mean(abstractlen_data))\n",
    "#</SOL>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Corpus Processing\n",
    "\n",
    "Topic modelling algorithms process vectorized data. In order to apply them, we need to transform the raw text input data into a vector representation. To do so, we will remove irrelevant information from the text data and preserve as much relevant information as possible to capture the semantic content in the document collection.\n",
    "\n",
    "Thus, we will proceed with the following steps:\n",
    "\n",
    "1. Tokenization\n",
    "2. Homogeneization\n",
    "3. Cleaning\n",
    "4. Vectorization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1. Tokenization\n",
    "\n",
    "For the first steps, we will use some of the powerful methods available from the [Natural Language Toolkit](http://www.nltk.org). In order to use the `word_tokenize` method from nltk, you might need to get the appropriate libraries using `nltk.download()`. You must select option \"d) Download\", and identifier \"punkt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "from nltk import download\n",
    "\n",
    "# You should comment this code fragment if the package is already available.\n",
    "# download('punkt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will create a list that contains just the abstracts in the dataset. As the order of the elements in a list is fixed, it will be later straightforward to match the processed abstracts to metadata associated to their corresponding projects."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 4**: Insert the appropriate call to `word_tokenize` in the code below, in order to get the tokens list corresponding to each document in the corpus:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "corpus_tokens = []\n",
    "ndocs = len(corpus_raw)\n",
    "\n",
    "for n, text in enumerate(corpus_raw):\n",
    "    if not n%100:\n",
    "        print('\\rTokenizing document', n, 'out of', ndocs, end='', flush=True)\n",
    "\n",
    "    # Tokenize each text entry. \n",
    "    # tokens = <FILL IN>\n",
    "    tokens = word_tokenize(text)\n",
    "    \n",
    "    # Add the new token list as a new element to corpus_tokens (that will be a list of lists)\n",
    "    # <FILL IN>\n",
    "    corpus_tokens.append(tokens)\n",
    "\n",
    "print('\\n\\n The corpus has been tokenized. Check the result for the first abstract:')\n",
    "print(corpus_raw[0])\n",
    "print(corpus_tokens[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2. Homogeneization\n",
    "\n",
    "By looking at the tokenized corpus you may verify that there are many tokens that correspond to punktuation signs and other symbols that are not relevant to analyze the semantic content. They can be removed using the stemming or lemmatization tools from `nltk`.\n",
    "\n",
    "The homogeneization process will consist of:\n",
    "\n",
    "1. **Removing capitalization**: capital alphabetic characters will be transformed to their corresponding lowercase characters.\n",
    "2. **Removing non alphanumeric tokens** (e.g. punktuation signs)\n",
    "3. **Stemming/Lemmatization**: removing word terminations to preserve the root of the words and ignore grammatical information."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 5**: Convert all tokens in `corpus_tokens` to lowercase (using the `.lower()` method) and remove non alphanumeric tokens (that you can detect with `.isalnum()` method). You can complete the following code fragment with a single line of code ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "corpus_filtered = []\n",
    "\n",
    "for n, tokens in enumerate(corpus_tokens):\n",
    "    if not n%1000:\n",
    "        print('\\rFiltering document', n, 'out of', ndocs, end='', flush=True)\n",
    "\n",
    "    #<SOL>\n",
    "    corpus_filtered.append([el.lower() for el in tokens if el.isalnum()])\n",
    "    #</SOL>\n",
    "\n",
    "print('\\n',corpus_filtered[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2.1. Stemming vs Lemmatization\n",
    "\n",
    "At this point, we can choose between applying a simple stemming or ussing lemmatization. We will try both to test their differences.\n",
    "\n",
    "The lemmatizer from NLTK is based on [WordNet](http://wordnet.princeton.edu). If you have not used wordnet before, you will likely need to download it from nltk (use the nltk.download() command)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "from nltk.stem import SnowballStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "stemmer = SnowballStemmer('english')\n",
    "wnl = WordNetLemmatizer()\n",
    "\n",
    "print('Result for the first document in the corpus applying stemming')\n",
    "print([stemmer.stem(el) for el in corpus_filtered[0]])\n",
    "\n",
    "print('Result for the first document in the corpus applying lemmatization')\n",
    "print([wnl.lemmatize(el) for el in corpus_filtered[0]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of the advantages of the lemmatizer method is that the result of lemmmatization is still a true word, which is more advisable for the presentation of text processing results and lemmatization.\n",
    "\n",
    "However, without using contextual information, lemmatize() does not remove grammatical differences. This is the reason why \"is\" or \"are\" are preserved and not replaced by infinitive \"be\".\n",
    "\n",
    "As an alternative, we can apply .lemmatize(word, pos), where 'pos' is a string code specifying the part-of-speech (pos), i.e. the grammatical role of the words in its sentence. For instance, you can check the difference between `wnl.lemmatize('is')` and `wnl.lemmatize('is, pos='v')`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 6**: Complete the following code fragment to lemmatize all documents in the corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "corpus_lemmatized = []\n",
    "\n",
    "for n, doc in enumerate(corpus_filtered):\n",
    "    if not n%200:\n",
    "        print('\\rLemmatizing document', n, 'out of', ndocs, end='', flush=True)\n",
    "\n",
    "    #<SOL>\n",
    "    corpus_lemmatized.append([wnl.lemmatize(el) for el in doc])\n",
    "    #</SOL>\n",
    "\n",
    "print('\\nResult for the first document in the dataset applying lemmatization')\n",
    "print('\\n',corpus_lemmatized[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3. Cleaning\n",
    "\n",
    "The third step consists of removing those words that are very common in language and do not carry out usefull semantic content (articles, pronouns, etc).\n",
    "\n",
    "Once again, we might need to load the stopword files using the download tools from `nltk`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 7**: In the second line below we read a list of common english stopwords. Clean `lemmatized_abstracts` by removing all tokens in the stopword list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stopwords_en = stopwords.words('english')\n",
    "\n",
    "corpus_clean = []\n",
    "\n",
    "for n, doc in enumerate(corpus_lemmatized):\n",
    "    if not n%100:\n",
    "        print('\\rCleaning document', n, 'out of', ndocs, end='', flush=True)\n",
    "        \n",
    "    # Remove all tokens in the stopwords list and append the result to corpus_clean\n",
    "    # <SOL>\n",
    "    clean_tokens = [token for token in doc if token not in stopwords_en]    \n",
    "    # </SOL>\n",
    "    corpus_clean.append(clean_tokens)\n",
    "    \n",
    "print('\\n Let us check tokens after cleaning:')\n",
    "print(corpus_clean[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4. Vectorization\n",
    "\n",
    "Up to this point, we have transformed the raw text collection in a list of documents, where each documen is a collection of the words that are most relevant for semantic analysis. Now, we need to convert these data (a list of token lists) into a numerical representation (a list of vectors, or a matrix). To do so, we will start using the tools provided by the `gensim` library. \n",
    "\n",
    "As a first step, we create a dictionary containing all tokens in our text corpus, and assigning an integer identifier to each one of them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Create dictionary of tokens\n",
    "D = gensim.corpora.Dictionary(corpus_clean)\n",
    "n_tokens = len(D)\n",
    "\n",
    "print('The dictionary contains', n_tokens, 'terms')\n",
    "print('First terms in the dictionary:')\n",
    "for n in range(10):\n",
    "    print(str(n), ':', D[n])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also filter out terms that appear in too few or too many of the documents in the dataset:\n",
    "\n",
    "** Exercise 8**: Use method `filter_extremes` over dictionary `D` to remove words appearing in less than 5 documents and, also, those appearing in more than 75% of documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# <SOL>\n",
    "no_below = 5 #Minimum number of documents to keep a term in the dictionary\n",
    "no_above = .75 #Maximum proportion of documents in which a term can appear to be kept in the dictionary\n",
    "\n",
    "D.filter_extremes(no_below=no_below, no_above=no_above, keep_n=25000)\n",
    "n_tokens = len(D)\n",
    "# </SOL>\n",
    "\n",
    "print('The dictionary contains', n_tokens, 'terms')\n",
    "print('First terms in the dictionary:')\n",
    "for n in range(10):\n",
    "    print(str(n), ':', D[n])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the second step, let us create a numerical version of our corpus using the `doc2bow` method. In general, `D.doc2bow(token_list)` transforms any list of tokens into a list of tuples `(token_id, n)`, one per each token in `token_list`, where `token_id` is the token identifier (according to dictionary `D`) and `n` is the number of occurrences of such token in `token_list`. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 9**: Apply the `doc2bow` method from gensim dictionary `D`, to all tokens in every document in `clean_abstracts`. The result must be a new list named `corpus_bow` where each element is a list of tuples `(token_id, number_of_occurrences)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# <SOL>\n",
    "corpus_bow = [D.doc2bow(doc) for doc in corpus_clean]\n",
    "# </SOL>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point, it is good to make sure to understand what has happened. In `clean_abstracts` we had a list of token lists. With it, we have constructed a Dictionary, `D`, which assigns an integer identifier to each token in the corpus.\n",
    "After that, we have transformed each article (in `clean_abstracts`) in a list tuples `(id, n)`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "print('Original document (after cleaning):')\n",
    "print(corpus_clean[0])\n",
    "print('Sparse vector representation (first 10 components):')\n",
    "print(corpus_bow[0][:10])\n",
    "print('Word counts for the first project (first 10 components):')\n",
    "print(list(map(lambda x: (D[x[0]], x[1]), corpus_bow[0][:10])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that we can interpret each element of corpus_bow as a `sparse_vector`. For example, a list of tuples \n",
    "\n",
    "    [(0, 1), (3, 3), (5,2)] \n",
    "\n",
    "for a dictionary of 10 elements can be represented as a vector, where any tuple `(id, n)` states that position `id` must take value `n`. The rest of positions must be zero.\n",
    "\n",
    "    [1, 0, 0, 3, 0, 2, 0, 0, 0, 0]\n",
    "\n",
    "These sparse vectors will be the inputs to the topic modeling algorithms.\n",
    "\n",
    "As a summary, the following variables will be relevant for the next chapters:\n",
    "\n",
    "   * `D`: A gensim dictionary. Term strings can be accessed using the numeric identifiers. For instance, `D[0]` contains the string corresponding to the first position in the BoW representation.\n",
    "   * `corpus_bow`: BoW corpus. A list containing an entry per project in the dataset, and consisting of the (sparse) BoW representation for the abstract of that project.\n",
    "   * `NSF_data`: A list containing an entry per project in the dataset, and consisting of metadata for the projects in the dataset\n",
    "   \n",
    "The way we have constructed the `corpus_bow` variable guarantees that the order is preserved, so that the projects are listed in the same order in the lists `corpus_bow` and `NSF_data`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before starting with the semantic analyisis, it is interesting to observe the token distribution for the given corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# SORTED TOKEN FREQUENCIES (I):\n",
    "# Create a \"flat\" corpus with all tuples in a single list\n",
    "corpus_bow_flat = [item for sublist in corpus_bow for item in sublist]\n",
    "\n",
    "# Initialize a numpy array that we will use to count tokens.\n",
    "# token_count[n] should store the number of ocurrences of the n-th token, D[n]\n",
    "token_count = np.zeros(n_tokens)\n",
    "\n",
    "# Count the number of occurrences of each token.\n",
    "for x in corpus_bow_flat:\n",
    "    # Update the proper element in token_count\n",
    "    # scode: <FILL IN>\n",
    "    token_count[x[0]] += x[1]\n",
    "\n",
    "# Sort by decreasing number of occurences\n",
    "ids_sorted = np.argsort(- token_count)\n",
    "tf_sorted = token_count[ids_sorted]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`ids_sorted` is a list of all token ids, sorted by decreasing number of occurrences in the whole corpus. For instance, the most frequent term is "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "print(D[ids_sorted[0]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "which appears"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "print(\"{0} times in the whole corpus\".format(tf_sorted[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following we plot the most frequent terms in the corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# SORTED TOKEN FREQUENCIES (II):\n",
    "plt.rcdefaults()\n",
    "\n",
    "# Example data\n",
    "n_art = len(NSF_data)\n",
    "n_bins = 25\n",
    "hot_tokens = [D[i] for i in ids_sorted[n_bins-1::-1]]\n",
    "y_pos = np.arange(len(hot_tokens))\n",
    "z = tf_sorted[n_bins-1::-1]/n_art\n",
    "\n",
    "plt.figure()\n",
    "plt.barh(y_pos, z, align='center', alpha=0.4)\n",
    "plt.yticks(y_pos, hot_tokens)\n",
    "plt.xlabel('Average number of occurrences per article')\n",
    "plt.title('Token distribution')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# SORTED TOKEN FREQUENCIES:\n",
    "\n",
    "# Example data\n",
    "plt.figure()\n",
    "plt.semilogy(tf_sorted)\n",
    "plt.ylabel('Total number of occurrences')\n",
    "plt.xlabel('Token rank')\n",
    "plt.title('Token occurrences')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 10**: There are usually many tokens that appear with very low frequency in the corpus. Count the number of tokens appearing only once, and what is the proportion of them in the token list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# cold_tokens = <FILL IN>\n",
    "cold_tokens = [D[i] for i in ids_sorted if tf_sorted[i]==1]\n",
    "\n",
    "print(\"There are {0} cold tokens, which represent {1}% of the total number of tokens in the dictionary\".format(\n",
    "    len(cold_tokens), float(len(cold_tokens))/n_tokens*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 11**: Represent graphically those 20 tokens that appear in the highest number of articles. Note that you can use the code above (headed by `# SORTED TOKEN FREQUENCIES`) with a very minor modification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# <SOL>\n",
    "# SORTED TOKEN FREQUENCIES (I):\n",
    "# Count the number of occurrences of each token.\n",
    "token_count2 = np.zeros(n_tokens)\n",
    "for x in corpus_bow_flat:\n",
    "    token_count2[x[0]] += (x[1]>0)\n",
    "\n",
    "# Sort by decreasing number of occurences\n",
    "ids_sorted2 = np.argsort(- token_count2)\n",
    "tf_sorted2 = token_count2[ids_sorted2]\n",
    "\n",
    "# SORTED TOKEN FREQUENCIES (II):\n",
    "# Example data\n",
    "n_bins = 25\n",
    "hot_tokens2 = [D[i] for i in ids_sorted2[n_bins-1::-1]]\n",
    "y_pos2 = np.arange(len(hot_tokens2))\n",
    "z2 = tf_sorted2[n_bins-1::-1]\n",
    "\n",
    "plt.barh(y_pos2, z2, align='center', alpha=0.4)\n",
    "plt.yticks(y_pos2, hot_tokens2)\n",
    "plt.xlabel('Number of articles')\n",
    "plt.title('Token distribution')\n",
    "plt.show()\n",
    "# </SOL>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 12**:  Count the number of tokens appearing only in a single article.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise (bigrams)**: `nltk` provides an utility to compute n-grams from a list of tokens, in `nltk.util.ngrams`. Join all tokens in `corpus_clean` in a single list and compute the bigrams. Plot the 20 most frequent bigrams in the corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5. Dictionary properties\n",
    "\n",
    "As a final comment, note that gensim dictionaries contain a method `dfs` to compute the word counts automatically. In the code below we build a list `all_counts` that contains tuples (terms, document_counts). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "all_counts = [(D[el], D.dfs[el]) for el in D.dfs]\n",
    "all_counts = sorted(all_counts, key=lambda x: x[1])"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
