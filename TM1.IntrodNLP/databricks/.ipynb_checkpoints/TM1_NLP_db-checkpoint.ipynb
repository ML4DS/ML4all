{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Text Analysis with NLTK"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "Author: Jesús Cid-Sueiro\n",
    "\n",
    "Date: 2016/04/03\n",
    "\n",
    "Last review: 2017/04/21"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# %matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "# import pylab\n",
    "\n",
    "# Required imports\n",
    "from wikitools import wiki\n",
    "from wikitools import category\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "from time import time\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "\n",
    "from test_helper import Test\n",
    "\n",
    "import gensim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Corpus acquisition.\n",
    "In this notebook we will explore some tools for text analysis available from Python toolboxes.\n",
    "\n",
    "To do so, we will explore and analyze collections of Wikipedia articles from a given category, using `wikitools`, that makes the capture of content from wikimedia sites very easy.\n",
    "\n",
    "(*As a side note, there are many other available text collections to work with. In particular, the NLTK library has many examples, that you can explore using the `nltk.download()` tool*.\n",
    "\n",
    "    import nltk\n",
    "    nltk.download()\n",
    "\n",
    "*for instance, you can take the gutemberg dataset*\n",
    "\n",
    "    Mycorpus = nltk.corpus.gutenberg\n",
    "    text_name = Mycorpus.fileids()[0]\n",
    "    raw = Mycorpus.raw(text_name)\n",
    "    Words = Mycorpus.words(text_name)\n",
    "\n",
    "*Also, tools like Gensim or Sci-kit learn include text databases to work with*).\n",
    "\n",
    "In order to use Wikipedia data, we will select a single category of articles:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "site = wiki.Wiki(\"https://en.wikipedia.org/w/api.php\")\n",
    "# Select a category with a reasonable number of articles (>100)\n",
    "# cat = \"Economics\"\n",
    "cat = \"Pseudoscience\"\n",
    "print cat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can try with any other categories, but take into account that some categories may contain very few articles. Select a category with at least 100 articles. You can browse the wikipedia category tree here, https://en.wikipedia.org/wiki/Category:Contents, for instance, and select the appropriate one.\n",
    "\n",
    "We start downloading the text collection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Loading category data. This may take a while\n",
    "print \"Loading category data. This may take a while...\"\n",
    "cat_data = category.Category(site, cat)\n",
    "\n",
    "corpus_titles = []\n",
    "corpus_text = []\n",
    "\n",
    "for n, page in enumerate(cat_data.getAllMembersGen()):\n",
    "    print \"\\rLoading article {0}\".format(n + 1),\n",
    "    corpus_titles.append(page.title)\n",
    "    corpus_text.append(page.getWikiText())\n",
    "\n",
    "n_art = len(corpus_titles)\n",
    "print \"\\nLoaded \" + str(n_art) + \" articles from category \" + cat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we have stored the whole text collection in two lists:\n",
    "\n",
    "* `corpus_titles`, which contains the titles of the selected articles\n",
    "* `corpus_text`, with the text content of the selected wikipedia articles\n",
    "\n",
    "You can browse the content of the wikipedia articles to get some intuition about the kind of documents that will be processed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "n = 5\n",
    "print corpus_titles[n]\n",
    "print corpus_text[n]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Corpus Processing\n",
    "\n",
    "Topic modelling algorithms process vectorized data. In order to apply them, we need to transform the raw text input data into a vector representation. To do so, we will remove irrelevant information from the text data and preserve as much relevant information as possible to capture the semantic content in the document collection.\n",
    "\n",
    "Thus, we will proceed with the following steps:\n",
    "\n",
    "1. Tokenization\n",
    "2. Homogeneization\n",
    "3. Cleaning\n",
    "4. Vectorization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1. Tokenization\n",
    "\n",
    "For the first steps, we will use some of the powerfull methods available from the [Natural Language Toolkit](http://www.nltk.org). In order to use the `word_tokenize` method from nltk, you might need to get the appropriate libraries using `nltk.download()`. You must select option \"d) Download\", and identifier \"punkt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# You can comment this if the package is already available.\n",
    "# Select option \"d) Download\", and identifier \"punkt\"\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task**: Insert the appropriate call to `word_tokenize` in the code below, in order to get the tokens list corresponding to each Wikipedia article:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "corpus_tokens = []\n",
    "\n",
    "for n, art in enumerate(corpus_text): \n",
    "    print \"\\rTokenizing article {0} out of {1}\".format(n + 1, n_art),\n",
    "    # This is to make sure that all characters have the appropriate encoding.\n",
    "    art = art.decode('utf-8')  \n",
    "    \n",
    "    # Tokenize each text entry. \n",
    "    # scode: tokens = <FILL IN>\n",
    "    tokens = word_tokenize(art)\n",
    "    \n",
    "    # Add the new token list as a new element to corpus_tokens (that will be a list of lists)\n",
    "    # scode: <FILL IN>\n",
    "    corpus_tokens.append(tokens)\n",
    "\n",
    "print \"\\n The corpus has been tokenized. Let's check some portion of the first article:\"\n",
    "print corpus_tokens[0][0:30]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Test.assertEquals(len(corpus_tokens), n_art, \"The number of articles has changed unexpectedly\")\n",
    "Test.assertTrue(len(corpus_tokens) >= 100, \n",
    "                \"Your corpus_tokens has less than 100 articles. Consider using a larger dataset\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2. Homogeneization\n",
    "\n",
    "By looking at the tokenized corpus you may verify that there are many tokens that correspond to punktuation signs and other symbols that are not relevant to analyze the semantic content. They can be removed using the stemming tool from `nltk`.\n",
    "\n",
    "The homogeneization process will consist of:\n",
    "\n",
    "1. Removing capitalization: capital alphabetic characters will be transformed to their corresponding lowercase characters.\n",
    "2. Removing non alphanumeric tokens (e.g. punktuation signs)\n",
    "3. Stemming/Lemmatization: removing word terminations to preserve the root of the words and ignore grammatical information."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2.1. Filtering\n",
    "\n",
    "Let us proceed with the filtering steps 1 and 2 (removing capitalization and non-alphanumeric tokens)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task**: Convert all tokens in `corpus_tokens` to lowercase (using `.lower()` method) and remove non alphanumeric tokens (that you can detect with `.isalnum()` method). You can do it in a single line of code..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "corpus_filtered = []\n",
    "\n",
    "for n, token_list in enumerate(corpus_tokens):\n",
    "    print \"\\rFiltering article {0} out of {1}\".format(n + 1, n_art),\n",
    "    \n",
    "    # Convert all tokens in token_list to lowercase, remove non alfanumeric tokens and stem.\n",
    "    # Store the result in a new token list, clean_tokens.\n",
    "    # scode: filtered_tokens = <FILL IN>\n",
    "    filtered_tokens = [token.lower() for token in token_list if token.isalnum()]\n",
    "    \n",
    "    # Add art to corpus_filtered\n",
    "    # scode: <FILL IN>\n",
    "    corpus_filtered.append(filtered_tokens)\n",
    "\n",
    "print \"\\nLet's check the first tokens from document 0 after filtering:\"\n",
    "print corpus_filtered[0][0:30]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "Test.assertTrue(all([c==c.lower() for c in corpus_filtered[23]]), 'Capital letters have not been removed')\n",
    "Test.assertTrue(all([c.isalnum() for c in corpus_filtered[13]]), 'Non alphanumeric characters have not been removed')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2.2. Stemming vs Lemmatization\n",
    "\n",
    "At this point, we can choose between applying a simple stemming or ussing lemmatization. We will try both to test their differences."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task**: Apply the `.stem()` method, from the stemmer object created in the first line, to `corpus_filtered`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Select stemmer.\n",
    "stemmer = nltk.stem.SnowballStemmer('english')\n",
    "corpus_stemmed = []\n",
    "\n",
    "for n, token_list in enumerate(corpus_filtered):\n",
    "    print \"\\rStemming article {0} out of {1}\".format(n + 1, n_art),\n",
    "    \n",
    "    # Convert all tokens in token_list to lowercase, remove non alfanumeric tokens and stem.\n",
    "    # Store the result in a new token list, clean_tokens.\n",
    "    # scode: stemmed_tokens = <FILL IN>\n",
    "    stemmed_tokens = [stemmer.stem(token) for token in token_list]\n",
    "    \n",
    "    # Add art to the stemmed corpus\n",
    "    # scode: <FILL IN>\n",
    "    corpus_stemmed.append(stemmed_tokens)\n",
    "\n",
    "print \"\\nLet's check the first tokens from document 0 after stemming:\"\n",
    "print corpus_stemmed[0][0:30]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "Test.assertTrue((len([c for c in corpus_stemmed[0] if c!=stemmer.stem(c)]) < 0.1*len(corpus_stemmed[0])), \n",
    "                'It seems that stemming has not been applied properly')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternatively, we can apply lemmatization. For english texts, we can use the lemmatizer from NLTK, which is based on [WordNet](http://wordnet.princeton.edu). If you have not used wordnet before, you will likely need to download it from nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# You can comment this if the package is already available.\n",
    "# Select option \"d) Download\", and identifier \"wordnet\"\n",
    "nltk.download(\"wordnet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task**: Apply the `.lemmatize()` method, from the WordNetLemmatizer object created in the first line, to `corpus_filtered`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "wnl = WordNetLemmatizer()\n",
    "\n",
    "# Select stemmer.\n",
    "corpus_lemmat = []\n",
    "\n",
    "for n, token_list in enumerate(corpus_filtered):\n",
    "    print \"\\rLemmatizing article {0} out of {1}\".format(n + 1, n_art),\n",
    "    \n",
    "    # scode: lemmat_tokens = <FILL IN>\n",
    "    lemmat_tokens = [wnl.lemmatize(token) for token in token_list]\n",
    "\n",
    "    # Add art to the stemmed corpus\n",
    "    # scode: <FILL IN>\n",
    "    corpus_lemmat.append(lemmat_tokens)\n",
    "\n",
    "print \"\\nLet's check the first tokens from document 0 after lemmatization:\"\n",
    "print corpus_lemmat[0][0:30]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of the advantages of the lemmatizer method is that the result of lemmmatization is still a true word, which is more advisable for the presentation of text processing results and lemmatization.\n",
    "\n",
    "However, without using contextual information, lemmatize() does not remove grammatical differences. This is the reason why \"is\" or \"are\" are preserved and not replaced by infinitive \"be\".\n",
    "\n",
    "As an alternative, we can apply .lemmatize(word, pos), where 'pos' is a string code specifying the part-of-speech (pos), i.e. the grammatical role of the words in its sentence. For instance, you can check the difference between `wnl.lemmatize('is')` and `wnl.lemmatize('is, pos='v')`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3. Cleaning\n",
    "\n",
    "The third step consists of removing those words that are very common in language and do not carry out usefull semantic content (articles, pronouns, etc).\n",
    "\n",
    "Once again, we might need to load the stopword files using the download tools from `nltk`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# You can comment this if the package is already available.\n",
    "# Select option \"d) Download\", and identifier \"stopwords\"\n",
    "nltk.download(\"stopwords\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Task**: In the second line below we read a list of common english stopwords. Clean `corpus_stemmed` by removing all tokens in the stopword list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "corpus_clean = []\n",
    "stopwords_en = stopwords.words('english')\n",
    "n = 0\n",
    "for token_list in corpus_stemmed:\n",
    "    n += 1\n",
    "    print \"\\rRemoving stopwords from article {0} out of {1}\".format(n, n_art),\n",
    "\n",
    "    # Remove all tokens in the stopwords list and append the result to corpus_clean\n",
    "    # scode: clean_tokens = <FILL IN>\n",
    "    clean_tokens = [token for token in token_list if token not in stopwords_en]    \n",
    "\n",
    "    # scode: <FILL IN>\n",
    "    corpus_clean.append(clean_tokens)\n",
    "    \n",
    "print \"\\n Let's check tokens after cleaning:\"\n",
    "print corpus_clean[0][0:30]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Test.assertTrue(len(corpus_clean) == n_art, 'List corpus_clean does not contain the expected number of articles')\n",
    "Test.assertTrue(len([c for c in corpus_clean[0] if c in stopwords_en])==0, 'Stopwords have not been removed')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4. Vectorization\n",
    "\n",
    "Up to this point, we have transformed the raw text collection of articles in a list of articles, where each article is a collection of the word roots that are most relevant for semantic analysis. Now, we need to convert these data (a list of token lists) into a numerical representation (a list of vectors, or a matrix). To do so, we will start using the tools provided by the `gensim` library. \n",
    "\n",
    "As a first step, we create a dictionary containing all tokens in our text corpus, and assigning an integer identifier to each one of them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Create dictionary of tokens\n",
    "D = gensim.corpora.Dictionary(corpus_clean)\n",
    "n_tokens = len(D)\n",
    "\n",
    "print \"The dictionary contains {0} tokens\".format(n_tokens)\n",
    "print \"First tokens in the dictionary: \"\n",
    "for n in range(10):\n",
    "    print str(n) + \": \" + D[n]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the second step, let us create a numerical version of our corpus using the `doc2bow` method. In general, `D.doc2bow(token_list)` transform any list of tokens into a list of tuples `(token_id, n)`, one per each token in `token_list`, where `token_id` is the token identifier (according to dictionary `D`) and `n` is the number of occurrences of such token in `token_list`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Task**: Apply the `doc2bow` method from gensim dictionary `D`, to all tokens in every article in `corpus_clean`. The result must be a new list named `corpus_bow` where each element is a list of tuples `(token_id, number_of_occurrences)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Transform token lists into sparse vectors on the D-space\n",
    "# scode: corpus_bow = <FILL IN>\n",
    "corpus_bow = [D.doc2bow(doc) for doc in corpus_clean]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Test.assertTrue(len(corpus_bow)==n_art, 'corpus_bow has not the appropriate size') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point, it is good to make sure to understand what has happened. In `corpus_clean` we had a list of token lists. With it, we have constructed a Dictionary, `D`, which assign an integer identifier to each token in the corpus.\n",
    "After that, we have transformed each article (in `corpus_clean`) in a list tuples `(id, n)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print \"Original article (after cleaning): \"\n",
    "print corpus_clean[0][0:30]\n",
    "print \"Sparse vector representation (first 30 components):\"\n",
    "print corpus_bow[0][0:30]\n",
    "print \"The first component, {0} from document 0, states that token 0 ({1}) appears {2} times\".format(\n",
    "    corpus_bow[0][0], D[0], corpus_bow[0][0][1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that we can interpret each element of corpus_bow as a `sparse_vector`. For example, a list of tuples \n",
    "\n",
    "    [(0, 1), (3, 3), (5,2)] \n",
    "\n",
    "for a dictionary of 10 elements can be represented as a vector, where any tuple `(id, n)` states that position `id` must take value `n`. The rest of positions must be zero.\n",
    "\n",
    "    [1, 0, 0, 3, 0, 2, 0, 0, 0, 0]\n",
    "\n",
    "These sparse vectors will be the inputs to the topic modeling algorithms.\n",
    "\n",
    "Note that, at this point, we have built a Dictionary containing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print \"{0} tokens\".format(len(D))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and a bow representation of a corpus with"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print \"{0} Wikipedia articles\".format(len(corpus_bow))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before starting with the semantic analyisis, it is interesting to observe the token distribution for the given corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# SORTED TOKEN FREQUENCIES (I):\n",
    "# Create a \"flat\" corpus with all tuples in a single list\n",
    "corpus_bow_flat = [item for sublist in corpus_bow for item in sublist]\n",
    "\n",
    "# Initialize a numpy array that we will use to count tokens.\n",
    "# token_count[n] should store the number of ocurrences of the n-th token, D[n]\n",
    "token_count = np.zeros(n_tokens)\n",
    "\n",
    "# Count the number of occurrences of each token.\n",
    "for x in corpus_bow_flat:\n",
    "    # Update the proper element in token_count\n",
    "    # scode: <FILL IN>\n",
    "    token_count[x[0]] += x[1]\n",
    "\n",
    "# Sort by decreasing number of occurences\n",
    "ids_sorted = np.argsort(- token_count)\n",
    "tf_sorted = token_count[ids_sorted]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`ids_sorted` is a list of all token ids, sorted by decreasing number of occurrences in the whole corpus. For instance, the most frequent term is"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print D[ids_sorted[0]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "which appears"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print \"{0} times in the whole corpus\".format(tf_sorted[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following we plot the most frequent terms in the corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# SORTED TOKEN FREQUENCIES (II):\n",
    "plt.rcdefaults()\n",
    "\n",
    "# Example data\n",
    "n_bins = 25\n",
    "hot_tokens = [D[i] for i in ids_sorted[n_bins-1::-1]]\n",
    "y_pos = np.arange(len(hot_tokens))\n",
    "z = tf_sorted[n_bins-1::-1]/n_art\n",
    "\n",
    "plt.barh(y_pos, z, align='center', alpha=0.4)\n",
    "plt.yticks(y_pos, hot_tokens)\n",
    "plt.xlabel('Average number of occurrences per article')\n",
    "plt.title('Token distribution')\n",
    "plt.show()\n",
    "display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# SORTED TOKEN FREQUENCIES:\n",
    "\n",
    "# Example data\n",
    "plt.semilogy(tf_sorted)\n",
    "plt.ylabel('Total number of occurrences')\n",
    "plt.xlabel('Token rank')\n",
    "plt.title('Token occurrences')\n",
    "plt.show()\n",
    "display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Exercise**: There are usually many tokens that appear with very low frequency in the corpus. Count the number of tokens appearing only once, and what is the proportion of them in the token list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# scode: <WRITE YOUR CODE HERE>\n",
    "# Example data\n",
    "cold_tokens = [D[i] for i in ids_sorted if tf_sorted[i]==1]\n",
    "\n",
    "print \"There are {0} cold tokens, which represent {1}% of the total number of tokens in the dictionary\".format(\n",
    "    len(cold_tokens), float(len(cold_tokens))/n_tokens*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Exercise**: Represent graphically those 20 tokens that appear in the highest number of articles. Note that you can use the code above (headed by `# SORTED TOKEN FREQUENCIES`) with a very minor modification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# scode: <WRITE YOUR CODE HERE>\n",
    "\n",
    "# SORTED TOKEN FREQUENCIES (I):\n",
    "# Count the number of occurrences of each token.\n",
    "token_count2 = np.zeros(n_tokens)\n",
    "for x in corpus_bow_flat:\n",
    "    token_count2[x[0]] += (x[1]>0)\n",
    "\n",
    "# Sort by decreasing number of occurences\n",
    "ids_sorted2 = np.argsort(- token_count2)\n",
    "tf_sorted2 = token_count2[ids_sorted2]\n",
    "\n",
    "# SORTED TOKEN FREQUENCIES (II):\n",
    "# Example data\n",
    "n_bins = 25\n",
    "hot_tokens2 = [D[i] for i in ids_sorted2[n_bins-1::-1]]\n",
    "y_pos2 = np.arange(len(hot_tokens2))\n",
    "z2 = tf_sorted2[n_bins-1::-1]/n_art\n",
    "\n",
    "plt.figure()\n",
    "plt.barh(y_pos2, z2, align='center', alpha=0.4)\n",
    "plt.yticks(y_pos2, hot_tokens2)\n",
    "plt.xlabel('Number of articles')\n",
    "plt.title('Token distribution')\n",
    "plt.show()\n",
    "display()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Exercise**: Count the number of tokens appearing only in a single article."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# scode: <WRITE YOUR CODE HERE>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Exercise** (*All in one*): Note that, for pedagogical reasons, we have used a different `for` loop for each text processing step creating a new `corpus_xxx` variable after each step. For very large corpus, this could cause memory problems. \n",
    "\n",
    "As a summary exercise, repeat the whole text processing, starting from corpus_text up to computing the bow, with the following modifications:\n",
    "\n",
    "1. Use a single `for` loop, avoiding the creation of any intermediate corpus variables.\n",
    "2. Use lemmatization instead of stemming.\n",
    "3. Remove all tokens appearing in only one document and less than 2 times.\n",
    "4. Save the result in a new variable `corpus_bow1`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# scode: <WRITE YOUR CODE HERE>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Exercise** (*Visualizing categories*): Repeat the previous exercise with a second wikipedia category. For instance, you can take \"communication\". \n",
    "\n",
    "1. Save the result in variable `corpus_bow2`.\n",
    "2. Determine the most frequent terms in `corpus_bow1` (`term1`) and `corpus_bow2` (`term2`).\n",
    "3. Transform each article in `corpus_bow1` and `corpus_bow2` into a 2 dimensional vector, where the first component is the frecuency of `term1` and the second component is the frequency of `term2`\n",
    "4. Draw a dispersion plot of all 2 dimensional points, using a different marker for each corpus. Could you differentiate both corpora using the selected terms only? What if the 2nd most frequent term is used?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# scode: <WRITE YOUR CODE HERE>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Exercise ** (bigrams): `nltk` provides an utility to compute n-grams from a list of tokens, in `nltk.util.ngrams`. Join all tokens in `corpus_clean` in a single list and compute the bigrams. Plot the 20 most frequent bigrams in the corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# scode: <WRITE YOUR CODE HERE>\n",
    "# Check the code below to see how ngrams works, and adapt it to solve the exercise.\n",
    "# from nltk.util import ngrams\n",
    "# sentence = 'this is a foo bar sentences and i want to ngramize it'\n",
    "# sixgrams = ngrams(sentence.split(), 2)\n",
    "# for grams in sixgrams:\n",
    "#     print grams\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  },
  "name": "TM1_NLP",
  "notebookId": 1977758087313894
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
