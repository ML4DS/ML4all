{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "# Introduction to Classification.\n",
    "\n",
    "    Notebook version: 2.3 (Oct 25, 2020)\n",
    "\n",
    "    Author: Jesús Cid Sueiro (jcid@tsc.uc3m.es)\n",
    "            Jerónimo Arenas García (jarenas@tsc.uc3m.es)\n",
    "            \n",
    "    Changes: v.1.0 - First version. Extracted from a former notebook on K-NN\n",
    "             v.2.0 - Adapted to Python 3.0 (backcompatible with Python 2.7)\n",
    "             v.2.1 - Minor corrections affecting the notation and assumptions\n",
    "             v.2.2 - Updated index notation\n",
    "             v.2.3 - Adaptation to slides conversion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# To visualize plots in the notebook\n",
    "%matplotlib inline \n",
    "\n",
    "# Import some libraries that will be necessary for working with data and displaying plots\n",
    "import csv     # To read csv files\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from scipy import spatial\n",
    "from sklearn import neighbors, datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 1. The Classification problem\n",
    "\n",
    "In a generic classification problem, we are given an **observation vector** ${\\bf x}\\in \\mathbb{R}^N$ which is known to belong to one and only one **category** or **class**, $y$, from the set ${\\mathcal Y} = \\{0, 1, \\ldots, M-1\\}$. \n",
    "\n",
    "The goal of a classifier system is to **predict** $y$ based on ${\\bf x}$.\n",
    "\n",
    "To design the classifier, we are given a collection of labelled observations ${\\mathcal D} = \\{({\\bf x}_k, y_k)\\}_{k=0}^{K-1}$ where, for each observation ${\\bf x}_k$, the value of its true category, $y_k$, is known."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### 1.1. Binary Classification\n",
    "\n",
    "We will focus in binary classification problems, where the label set is binary, ${\\mathcal Y} = \\{0, 1\\}$. \n",
    "\n",
    "Despite its simplicity, this is the most frequent case. Many multi-class classification problems are usually solved by decomposing them into a collection of binary problems."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### 1.2. The independence and identical distribution (i.i.d.) assumption.\n",
    "\n",
    "The classification algorithms, as many other machine learning algorithms, are based on two major underlying hypothesis:\n",
    "\n",
    "   - **Independence**: All samples are statistically independent.\n",
    "   - **Identical distribution**: All samples in dataset ${\\mathcal D}$ have been generated by the same distribution $p_{{\\bf X}, Y}({\\bf x}, y)$.\n",
    "   \n",
    "The i.i.d. assumption is essential to guarantee that a classifier based on ${\\mathcal D}$ has a good perfomance when applied to new input samples. \n",
    "\n",
    "The **underlying distribution is unknown** (if we knew it, we could apply classic decision theory to make optimal predictions). This is why we need the data in ${\\mathcal D}$ to design the classifier. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 2. A simple classification problem: the Iris dataset\n",
    "\n",
    "(Iris dataset presentation is based on this <a href=http://machinelearningmastery.com/tutorial-to-implement-k-nearest-neighbors-in-python-from-scratch/> Tutorial </a> by <a href=http://machinelearningmastery.com/about/> Jason Brownlee</a>) \n",
    "\n",
    "As an illustration, consider the <a href = http://archive.ics.uci.edu/ml/datasets/Iris> Iris dataset </a>, taken from the <a href=http://archive.ics.uci.edu/ml/> UCI Machine Learning repository </a>. Quoted from the dataset description:\n",
    "\n",
    "> *This is perhaps the best known database to be found in the pattern recognition literature. The data set contains 3 classes of 50 instances each, where each class refers to a type of iris plant. [...] One class is linearly separable from the other 2; the latter are NOT linearly separable from each other.* \n",
    "\n",
    "The *class* is the species, which is one of *setosa*, *versicolor* or *virginica*. Each instance contains 4 measurements of given flowers: sepal length, sepal width, petal length and petal width, all in centimeters. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# Taken from Jason Brownlee notebook.\n",
    "with open('datasets/iris.data', 'r') as csvfile:\n",
    "\tlines = csv.reader(csvfile)\n",
    "\tfor row in lines:\n",
    "\t\tprint(','.join(row))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "###  2.1. Training and test\n",
    "\n",
    "Next, we will split the data into two sets:\n",
    "\n",
    "* **Training set**, that will be used to learn the classification model\n",
    "* **Test set**, that will be used to evaluate the classification performance\n",
    "\n",
    "The data partition must be **random**, in such a way that the statistical distribution of both datasets is the same."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The code fragment below defines a function `loadDataset` that loads the data in a CSV with the provided filename, converts the flower measures (that were loaded as strings) into numbers and, finally, it splits the data into a training and test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# Adapted from a notebook by Jason Brownlee\n",
    "def loadDataset(filename, split):\n",
    "    xTrain = []\n",
    "    cTrain = []\n",
    "    xTest = []\n",
    "    cTest = []\n",
    "\n",
    "    with open(filename, 'r') as csvfile:\n",
    "        lines = csv.reader(csvfile)\n",
    "        dataset = list(lines)\n",
    "    for i in range(len(dataset)-1):\n",
    "        for y in range(4):\n",
    "            dataset[i][y] = float(dataset[i][y])\n",
    "        item = dataset[i]\n",
    "        if random.random() < split:\n",
    "            xTrain.append(item[0:-1])\n",
    "            cTrain.append(item[-1])\n",
    "        else:\n",
    "            xTest.append(item[0:-1])\n",
    "            cTest.append(item[-1])\n",
    "    return xTrain, cTrain, xTest, cTest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "We can use this function to get a data split. An expected ratio of 67/33 samples for train/test will be used. However, note that, because of the way samples are assigned to the train or test datasets, the exact number of samples in each partition will differ if you run the code several times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "xTrain_all, cTrain_all, xTest_all, cTest_all = loadDataset('./datasets/iris.data', 0.67)\n",
    "nTrain_all = len(xTrain_all)\n",
    "nTest_all = len(xTest_all)\n",
    "print('Train:', str(nTrain_all))\n",
    "print('Test:', str(nTest_all))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 2.2. Scatter plots\n",
    "\n",
    "To get some intuition about this four dimensional dataset we can plot 2-dimensional projections taking only two variables each time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "i = 2 # Try 0,1,2,3\n",
    "j = 3 # Try 0,1,2,3 with j!=i\n",
    "\n",
    "# Take coordinates for each class separately\n",
    "xiSe = [xTrain_all[n][i] for n in range(nTrain_all) if cTrain_all[n]=='Iris-setosa']\n",
    "xjSe = [xTrain_all[n][j] for n in range(nTrain_all) if cTrain_all[n]=='Iris-setosa']\n",
    "xiVe = [xTrain_all[n][i] for n in range(nTrain_all) if cTrain_all[n]=='Iris-versicolor']\n",
    "xjVe = [xTrain_all[n][j] for n in range(nTrain_all) if cTrain_all[n]=='Iris-versicolor']\n",
    "xiVi = [xTrain_all[n][i] for n in range(nTrain_all) if cTrain_all[n]=='Iris-virginica']\n",
    "xjVi = [xTrain_all[n][j] for n in range(nTrain_all) if cTrain_all[n]=='Iris-virginica']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "plt.plot(xiSe, xjSe,'bx', label='Setosa')\n",
    "plt.plot(xiVe, xjVe,'r.', label='Versicolor')\n",
    "plt.plot(xiVi, xjVi,'g+', label='Virginica')\n",
    "plt.xlabel('$x_' + str(i) + '$')\n",
    "plt.ylabel('$x_' + str(j) + '$')\n",
    "plt.legend(loc='best')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "In the following, we will design a classifier to separate classes \"Versicolor\" and \"Virginica\" using $x_0$ and $x_1$ only. To do so, we build a training set with samples from these categories, and a binary label $y^{(k)} = 1$ for samples in class \"Virginica\", and $0$ for \"Versicolor\" data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# Select two classes\n",
    "c0 = 'Iris-versicolor' \n",
    "c1 = 'Iris-virginica'\n",
    "\n",
    "# Select two coordinates\n",
    "ind = [0, 1]\n",
    "\n",
    "# Take training test\n",
    "X_tr = np.array([[xTrain_all[n][i] for i in ind] for n in range(nTrain_all) \n",
    "                  if cTrain_all[n]==c0 or cTrain_all[n]==c1])\n",
    "C_tr = [c for c in cTrain_all if c==c0 or c==c1]\n",
    "Y_tr = np.array([int(c==c1) for c in C_tr])\n",
    "n_tr = len(X_tr)\n",
    "\n",
    "# Take test set\n",
    "X_tst = np.array([[xTest_all[n][i] for i in ind] for n in range(nTest_all) \n",
    "                 if cTest_all[n]==c0 or cTest_all[n]==c1])\n",
    "C_tst = [c for c in cTest_all if c==c0 or c==c1]\n",
    "Y_tst = np.array([int(c==c1) for c in C_tst])\n",
    "n_tst = len(X_tst)\n",
    "\n",
    "# Separate components of x into different arrays (just for the plots)\n",
    "x0c0 = [X_tr[n][0] for n in range(n_tr) if Y_tr[n]==0]\n",
    "x1c0 = [X_tr[n][1] for n in range(n_tr) if Y_tr[n]==0]\n",
    "x0c1 = [X_tr[n][0] for n in range(n_tr) if Y_tr[n]==1]\n",
    "x1c1 = [X_tr[n][1] for n in range(n_tr) if Y_tr[n]==1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# Scatterplot.\n",
    "labels = {'Iris-setosa': 'Setosa', \n",
    "          'Iris-versicolor': 'Versicolor',\n",
    "          'Iris-virginica': 'Virginica'}\n",
    "plt.plot(x0c0, x1c0,'r.', label=labels[c0])\n",
    "plt.plot(x0c1, x1c1,'g+', label=labels[c1])\n",
    "plt.xlabel('$x_' + str(ind[0]) + '$')\n",
    "plt.ylabel('$x_' + str(ind[1]) + '$')\n",
    "plt.legend(loc='best')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 3. A Baseline Classifier: Maximum A Priori.\n",
    "\n",
    "For the selected data set, we have two clases and a dataset with the following class proportions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "print(f'Class 0 {c0}: {n_tr - sum(Y_tr)} samples')\n",
    "print(f'Class 1 ({c1}): {sum(Y_tr)} samples')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The maximum a priori classifier assigns any sample ${\\bf x}$ to the most frequent class in the training set. Therefore, the class prediction $y$ for any sample ${\\bf x}$ is"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "y = int(2*sum(Y_tr) > n_tr)\n",
    "print(f'y = {y} ({c1 if y==1 else c0})')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The error rate for this baseline classifier is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Training and test error arrays\n",
    "E_tr = (Y_tr != y)\n",
    "E_tst = (Y_tst != y)\n",
    "\n",
    "# Error rates\n",
    "pe_tr = float(sum(E_tr)) / n_tr\n",
    "pe_tst = float(sum(E_tst)) / n_tst\n",
    "print('Pe(train):', str(pe_tr))\n",
    "print('Pe(test):', str(pe_tst))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The error rate of the baseline classifier is a simple benchmark for classification. Since the maximum a priori decision is independent on the observation, ${\\bf x}$, any classifier based on ${\\bf x}$ should have a better (or, at least, not worse) performance than the baseline classifier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 3. Parametric vs non-parametric classification.\n",
    "\n",
    "Most classification algorithms can be fitted to one of two categories:\n",
    "\n",
    "1. **Parametric classifiers**: to classify any input sample ${\\bf x}$, the classifier applies some function $f_{\\bf w}({\\bf x})$ which depends on some parameters ${\\bf w}$. The training dataset is used to estimate ${\\bf w}$. Once the parameter has been estimated, the training data is no longer needed to classify new inputs.\n",
    "\n",
    "2. **Non-parametric classifiers**: the classifier decision for any input ${\\bf x}$ depend on the training data in a direct manner. The training data must be preserved to classify new data.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
