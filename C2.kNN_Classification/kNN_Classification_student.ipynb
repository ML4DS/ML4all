{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "# The $k$-Nearest Neighbor Classification Algorithm\n",
    "\n",
    "    Notebook version: 2.1 (Oct 19, 2018)\n",
    "\n",
    "    Author: Jesús Cid Sueiro (jcid@tsc.uc3m.es)\n",
    "            Jerónimo Arenas García (jarenas@tsc.uc3m.es)\n",
    "            \n",
    "    Changes: v.1.0 - First version\n",
    "             v.1.1 - Function loadDataset updated to work with any number of dimensions\n",
    "             v.2.0 - Compatible with Python 3 (backcompatible with Python 2.7)\n",
    "                     Added solution to Exercise 3\n",
    "             v.2.1 - Minor corrections regarding notation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "# To visualize plots in the notebook\n",
    "%matplotlib inline \n",
    "\n",
    "# Import some libraries that will be necessary for working with data and displaying plots\n",
    "import csv     # To read csv files\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from scipy import spatial\n",
    "from sklearn import neighbors, datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "## 1. The binary classification problem.\n",
    "\n",
    "In a binary classification problem, we are given an observation vector ${\\bf x}\\in \\mathbb{R}^N$ which is known to belong to one and only one *category* or *class*, $y$, in the set ${\\mathcal Y} = \\{0, 1\\}$. The goal of a classifier system is to predict the value of $y$ based on ${\\bf x}$.\n",
    "\n",
    "To design the classifier, we are given a collection of labelled observations ${\\mathcal D} = \\{({\\bf x}^{(k)}, y^{(k)})\\}_{k=0}^{K-1}$ where, for each observation ${\\bf x}^{(k)}$, the value of its true category, $y^{(k)}$, is known. All samples are outcomes of an unknown distribution $p_{{\\bf X},Y}({\\bf x}, y)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 2. The Iris dataset\n",
    "\n",
    "(Iris dataset presentation is based on this <a href=http://machinelearningmastery.com/tutorial-to-implement-k-nearest-neighbors-in-python-from-scratch/> Tutorial </a> by <a href=http://machinelearningmastery.com/about/> Jason Brownlee</a>) \n",
    "\n",
    "To illustrate the algorithms, we will consider the <a href = http://archive.ics.uci.edu/ml/datasets/Iris> Iris dataset </a>, taken from the <a href=http://archive.ics.uci.edu/ml/> UCI Machine Learning repository </a>. Quoted from the dataset description:\n",
    "\n",
    "> This is perhaps the best known database to be found in the pattern recognition literature. The data set contains 3 classes of 50 instances each, where each class refers to a type of iris plant. [...] One class is linearly separable from the other 2; the latter are NOT linearly separable from each other. \n",
    "\n",
    "The *class* is the species, which is one of *setosa*, *versicolor* or *virginica*. Each instance contains 4 measurements of given flowers: sepal length, sepal width, petal length and petal width, all in centimeters. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# Taken from Jason Brownlee notebook.\n",
    "with open('datasets/iris.data', 'r') as csvfile:\n",
    "\tlines = csv.reader(csvfile)\n",
    "\tfor row in lines:\n",
    "\t\tprint(', '.join(row))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### 2.1. Train/test split\n",
    "\n",
    "Next, we will split the data into a training dataset, that will be used to learn the classification model, and a test dataset that we can use to evaluate its the accuracy.\n",
    "\n",
    "We first need to convert the flower measures that were loaded as strings into numbers that we can work with. Next we need to split the data set **randomly** into train and datasets. A ratio of 67/33 for train/test will be used.\n",
    "\n",
    "The code fragment below defines a function `loadDataset` that loads the data in a CSV with the provided filename and splits it randomly into train and test datasets using the provided split ratio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# Adapted from a notebook by Jason Brownlee\n",
    "def loadDataset(filename, split):\n",
    "    xTrain = []\n",
    "    cTrain = []\n",
    "    xTest = []\n",
    "    cTest = []\n",
    "\n",
    "    with open(filename, 'r') as csvfile:\n",
    "        lines = csv.reader(csvfile)\n",
    "        dataset = list(lines)\n",
    "    for i in range(len(dataset)-1):\n",
    "        for y in range(4):\n",
    "            dataset[i][y] = float(dataset[i][y])\n",
    "        item = dataset[i]\n",
    "        if random.random() < split:\n",
    "            xTrain.append(item[0:-1])\n",
    "            cTrain.append(item[-1])\n",
    "        else:\n",
    "            xTest.append(item[0:-1])\n",
    "            cTest.append(item[-1])\n",
    "    return xTrain, cTrain, xTest, cTest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "We can use this function to get a data split. Note that, because of the way samples are assigned to the train or test datasets, the number of samples in each partition will differ if you run the code several times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "xTrain_all, cTrain_all, xTest_all, cTest_all = loadDataset('datasets/iris.data', 0.67)\n",
    "nTrain_all = len(xTrain_all)\n",
    "nTest_all = len(xTest_all)\n",
    "print('Train:', nTrain_all)\n",
    "print('Test:', nTest_all)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 2.2. Versicolor vs Virginica\n",
    "\n",
    "In the following, we will design a classifier to separate classes \"Versicolor\" and \"Virginica\" using $x_0$ and $x_1$ only. To do so, we build a training set with samples from these categories, and a bynary label $y^{(k)} = 1$ for samples in class \"Virginica\", and $0$ for \"Versicolor\" data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# Select two classes\n",
    "c0 = 'Iris-versicolor' \n",
    "c1 = 'Iris-virginica'\n",
    "\n",
    "# Select two coordinates\n",
    "ind = [0, 1]\n",
    "\n",
    "# Take training test\n",
    "X_tr = np.array([[xTrain_all[n][i] for i in ind] for n in range(nTrain_all) \n",
    "                  if cTrain_all[n]==c0 or cTrain_all[n]==c1])\n",
    "C_tr = [cTrain_all[n] for n in range(nTrain_all) \n",
    "          if cTrain_all[n]==c0 or cTrain_all[n]==c1]\n",
    "Y_tr = np.array([int(c==c1) for c in C_tr])\n",
    "n_tr = len(X_tr)\n",
    "\n",
    "# Take test set\n",
    "X_tst = np.array([[xTest_all[n][i] for i in ind] for n in range(nTest_all) \n",
    "                 if cTest_all[n]==c0 or cTest_all[n]==c1])\n",
    "C_tst = [cTest_all[n] for n in range(nTest_all) \n",
    "         if cTest_all[n]==c0 or cTest_all[n]==c1]\n",
    "Y_tst = np.array([int(c==c1) for c in C_tst])\n",
    "n_tst = len(X_tst)\n",
    "\n",
    "# Separate components of x into different arrays (just for the plots)\n",
    "x0c0 = [X_tr[n][0] for n in range(n_tr) if Y_tr[n]==0]\n",
    "x1c0 = [X_tr[n][1] for n in range(n_tr) if Y_tr[n]==0]\n",
    "x0c1 = [X_tr[n][0] for n in range(n_tr) if Y_tr[n]==1]\n",
    "x1c1 = [X_tr[n][1] for n in range(n_tr) if Y_tr[n]==1]\n",
    "\n",
    "# Scatterplot.\n",
    "labels = {'Iris-setosa': 'Setosa', \n",
    "          'Iris-versicolor': 'Versicolor',\n",
    "          'Iris-virginica': 'Virginica'}\n",
    "plt.plot(x0c0, x1c0,'r.', label=labels[c0])\n",
    "plt.plot(x0c1, x1c1,'g+', label=labels[c1])\n",
    "plt.xlabel('$x_' + str(ind[0]) + '$')\n",
    "plt.ylabel('$x_' + str(ind[1]) + '$')\n",
    "plt.legend(loc='best')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 3. Baseline Classifier: Maximum A Priori.\n",
    "\n",
    "For the selected data set, we have two clases and a dataset with the following class proportions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "print('Class 0 (' + c0 + '): ' + str(n_tr - sum(Y_tr)) + ' samples')\n",
    "print('Class 1 (' + c1 + '): ' + str(sum(Y_tr)) + ' samples')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "The maximum a priori classifier assigns any sample ${\\bf x}$ to the most frequent class in the training set. Therefore, the class prediction $y$ for any sample ${\\bf x}$ is"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "y = int(2*sum(Y_tr) > n_tr)\n",
    "print('y = ' + str(y) + ' (' + (c1 if y==1 else c0) + ')')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The error rate for this baseline classifier is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Training and test error arrays\n",
    "E_tr = (Y_tr != y)\n",
    "E_tst = (Y_tst != y)\n",
    "\n",
    "# Error rates\n",
    "pe_tr = float(sum(E_tr)) / n_tr\n",
    "pe_tst = float(sum(E_tst)) / n_tst\n",
    "print('Pe(train):', pe_tr)\n",
    "print('Pe(test):', pe_tst)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "The error rate of the baseline classifier is a simple benchmark for classification. Since the maximum a priori decision is independent on the observation, ${\\bf x}$, any classifier based on ${\\bf x}$ should have a better (or, at least, not worse) performance than the baseline classifier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 4. The Nearest-Neighbour Classifier (1-NN).\n",
    "\n",
    "\n",
    "The 1-NN classifier assigns any instance ${\\bf x}$ to the category of the nearest neighbor in the training set.\n",
    "$$\n",
    "d = f({\\bf x}) = y^{(n)}, {\\rm~where} \\\\\n",
    "n = \\arg \\min_k \\|{\\bf x}-{\\bf x}^{(k)}\\|\n",
    "$$\n",
    "In case of ties (i.e. if there is more than one instance at minimum distance) the class of one of them, taken arbitrarily, is assigned to ${\\bf x}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "def nn_classifier(X1,Y1,X2):\n",
    "    \"\"\" Compute the 1-NN classification for the observations contained in\n",
    "        the rows of X2, for the training set given by the rows in X1 and the\n",
    "        class labels contained in Y1.\n",
    "    \"\"\"\n",
    "    if X1.ndim == 1:\n",
    "        X1 = np.asmatrix(X1).T\n",
    "    if X2.ndim == 1:\n",
    "        X2 = np.asmatrix(X2).T\n",
    "    distances = spatial.distance.cdist(X1,X2,'euclidean')\n",
    "    neighbors = np.argsort(distances, axis=0, kind='quicksort', order=None)\n",
    "    closest = neighbors[0,:]\n",
    "    y_values = np.zeros([X2.shape[0],1])\n",
    "    for idx in range(X2.shape[0]):\n",
    "        y_values[idx] = Y1[closest[idx]]\n",
    "        \n",
    "    return y_values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Let us apply the 1-NN classifier to the given dataset. First, we will show the decision regions of the classifier. To do so, we compute the classifier output for all points in a rectangular grid from the sample space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# Create a regtangular grid.\n",
    "n_points = 200\n",
    "x_min, x_max = X_tr[:, 0].min(), X_tr[:, 0].max() \n",
    "y_min, y_max = X_tr[:, 1].min(), X_tr[:, 1].max()\n",
    "dx = x_max - x_min\n",
    "dy = y_max - y_min\n",
    "h = dy / n_points\n",
    "xx, yy = np.meshgrid(np.arange(x_min - 0.1 * dx, x_max + 0.1 * dx, h),\n",
    "                     np.arange(y_min - 0.1 * dx, y_max + 0.1 * dy, h))\n",
    "X_grid = np.array([xx.ravel(), yy.ravel()]).T\n",
    "\n",
    "# Compute the classifier output for all samples in the grid.\n",
    "Z = nn_classifier(X_tr, Y_tr, X_grid)\n",
    "\n",
    "# Put the result into a color plot\n",
    "plt.plot(x0c0, x1c0,'r.', label=labels[c0])\n",
    "plt.plot(x0c1, x1c1,'g+', label=labels[c1])\n",
    "plt.xlabel('$x_' + str(ind[0]) + '$')\n",
    "plt.ylabel('$x_' + str(ind[1]) + '$')\n",
    "plt.legend(loc='best')\n",
    "\n",
    "Z = Z.reshape(xx.shape)\n",
    "plt.contourf(xx, yy, Z)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "We can observe that the decision boudary of the 1-NN classifier is rather intricate, and it may contain small *islands* covering one or few samples from one class. Actually, the extension of these small regions usually reduces as we have more training samples, though the number of them may increase.\n",
    "\n",
    "Now we compute the error rates over the training and test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Training errors\n",
    "Z_tr = nn_classifier(X_tr, Y_tr, X_tr)\n",
    "E_tr = Z_tr.flatten()!=Y_tr\n",
    "\n",
    "# Test errors\n",
    "Z_tst = nn_classifier(X_tr, Y_tr, X_tst)\n",
    "E_tst = Z_tst.flatten()!=Y_tst\n",
    "\n",
    "# Error rates\n",
    "pe_tr = float(sum(E_tr)) / n_tr\n",
    "pe_tst = float(sum(E_tst)) / n_tst\n",
    "print('Pe(train):', pe_tr)\n",
    "print('Pe(test):', pe_tst)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The training and test error rates of the 1-NN may be significantly different. In fact, the training error may go down to zero if samples do not overlap. In the selected problem, this is not the case, because samples from different classes coincide at the same point, causing some classification errors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**Consistency of the 1-NN classifier**\n",
    "\n",
    "Despite the 1-NN usually reduces the error rate with respect to the baseline classifier, the number of errors may be too large. Errors may be attributed to diferent causes:\n",
    "\n",
    "   1. The class distributions are overlapped, because the selected features have no complete information for discriminating between the classes: this would imply that, even the best possible classifier would be prone to errors.\n",
    "   2. The training sample is small, and it is not enough to obtaing a good estimate of the optimal classifiers.\n",
    "   3. The classifier has intrinsic limitations: even though we had an infinite number of samples, the classifier performance does not approach the optimal classifiers.\n",
    "\n",
    "In general, a classifier is said to be consistent if it makes nearly optimal decisions as the number of training samples increases. Actually, it can be shown that this is the case of the 1-NN classifier if the classification problem is separable, i.e. if there exist a decision boundary with zero error probability. Unfortunately, in a non-separable case, the 1-NN classifier is not consistent. It can be shown that the error rate of the 1-NN classifier converges to an error rate which is not worse than twice the minimum attainable error rate (Bayes error rate) as the number of training samples goes to infinity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**Exercise 1**: In this exercise we test the non-consistency of the 1-NN classifier for overlapping distributions. Generate an artificial dataset for classification as follows:\n",
    "\n",
    "- Generate $N$ binary labels at random with values '0' and '1'. Store them in vector ${\\bf y}$\n",
    "- For every label $y^{(k)}$ in ${\\bf y}$:\n",
    "    - If the label is 0, take sample $x^{(k)}$ at random from a uniform distribution $U(0,2)$.\n",
    "    - If the label is 1, take sample $x^{(k)}$ at random from a uniform distribution $U(1,5)$.\n",
    "\n",
    "Take $N=1000$ for the test set. This is a large sample to get accurate error rate estimates. Also, take $N=10$, $20$, $40$, $80$,... for the training set. Compute the 1-NN classifier, and observe the test error rate as a function of $N$. \n",
    "\n",
    "Now, compute the test error rate of the classifier making decision $1$ if $x^{(k)}>1.5$, and $0$ otherwise. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# <SOL>\n",

    "# </SOL>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 5. $k$-NN classifier\n",
    "\n",
    "A simple extension of the 1-NN classifier is the $k$-NN classifier, which, for any input sample ${\\bf x}$, computes the $k$ closest neighbors in the training set, and takes the majority class in the subset. To avoid ties, in the binary classification case $k$ is usually taken as an odd number."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "def knn_classifier(X1,Y1,X2,k):\n",
    "    \"\"\" Compute the k-NN classification for the observations contained in\n",
    "        the rows of X2, for the training set given by the rows in X1 and the\n",
    "        components of S1. k is the number of neighbours.\n",
    "    \"\"\"\n",
    "    if X1.ndim == 1:\n",
    "        X1 = np.asmatrix(X1).T\n",
    "    if X2.ndim == 1:\n",
    "        X2 = np.asmatrix(X2).T\n",
    "    distances = spatial.distance.cdist(X1,X2,'euclidean')\n",
    "    neighbors = np.argsort(distances, axis=0, kind='quicksort', order=None)\n",
    "    closest = neighbors[range(k),:]\n",
    "    \n",
    "    y_values = np.zeros([X2.shape[0],1])\n",
    "    for idx in range(X2.shape[0]):\n",
    "        y_values[idx] = np.median(Y1[closest[:,idx]])\n",
    "        \n",
    "    return y_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "k = 7\n",
    "\n",
    "# Plot the decision boundary. For that, we will assign a color to each\n",
    "# point in the mesh [x_min, m_max]x[y_min, y_max].\n",
    "Z = knn_classifier(X_tr, Y_tr, X_grid, k)\n",
    "\n",
    "# Put the result into a color plot\n",
    "plt.plot(x0c0, x1c0,'r.', label=labels[c0])\n",
    "plt.plot(x0c1, x1c1,'g+', label=labels[c1])\n",
    "plt.xlabel('$x_' + str(ind[0]) + '$')\n",
    "plt.ylabel('$x_' + str(ind[1]) + '$')\n",
    "plt.legend(loc='best')\n",
    "\n",
    "Z = Z.reshape(xx.shape)\n",
    "plt.contourf(xx, yy, Z)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Plot training and test error as a function of parameter k.\n",
    "pe_tr = []\n",
    "pe_tst = []\n",
    "k_list = [2*n+1 for n in range(int(np.floor(n_tr/2)))]\n",
    "\n",
    "for k in k_list:\n",
    "\n",
    "    # Training errors\n",
    "    Z_tr = knn_classifier(X_tr, Y_tr, X_tr, k)\n",
    "    E_tr = Z_tr.flatten()!=Y_tr\n",
    "\n",
    "    # Test errors\n",
    "    Z_tst = knn_classifier(X_tr, Y_tr, X_tst, k)\n",
    "    E_tst = Z_tst.flatten()!=Y_tst\n",
    "\n",
    "    # Error rates\n",
    "    pe_tr.append(float(sum(E_tr)) / n_tr)\n",
    "    pe_tst.append(float(sum(E_tst)) / n_tst)\n",
    "\n",
    "# Put the result into a color plot\n",
    "markerline, stemlines, baseline = plt.stem(k_list, pe_tr,'r', markerfmt='s', label='Training')\n",
    "plt.plot(k_list, pe_tr,'r:')\n",
    "plt.setp(markerline, 'markerfacecolor', 'r', )\n",
    "plt.setp(baseline, 'color','r', 'linewidth', 2)\n",
    "markerline, stemlines, baseline = plt.stem(k_list, pe_tst, label='Test')\n",
    "plt.plot(k_list, pe_tst,'b:')\n",
    "plt.xlabel('$k$')\n",
    "plt.ylabel('Error rate')\n",
    "plt.legend(loc='best')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "We can analyze the influence of parameter $k$ by observing both traning and test errors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Exercise 2**: Observe the train and test error for large $k$. Could you relate the error rate of the baseline classifier with that to the $k$-NN for certain value of $k$? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "The figure above suggests that the optimal value of $k$ is"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "i = np.argmin(pe_tst)\n",
    "k_opt = k_list[i]\n",
    "print('k_opt:', k_opt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "However, using the test set to select the optimal value of the hyperparameter $k$ is not allowed. Instead, we should recur to cross validation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 2.3 Hyperparameter selection via cross-validation\n",
    "\n",
    "An inconvenient of the application of the $k$-nn method is that the selection of $k$ influences the final error of the algorithm. In the previous experiments, we noticed that the location of the minimum is not necessarily the same from the perspective of the test and training data. Ideally, we would like that the designed classification model works as well as possible on future unlabeled patterns that are not available during the training phase. This property is known as <i>generalization</i>. Fitting the training data is only pursued in the hope that we are also indirectly obtaining a model that generalizes well. In order to achieve this goal, there are some strategies that try to guarantee a correct generalization of the model. One of such approaches is known as <b>cross-validation</b>.\n",
    "\n",
    "Since using the test labels during the training phase is not allowed (they should be kept aside to simultate the future application of the classification model on unseen patterns), we need to figure out some way to improve our estimation of the hyperparameter that requires only training data. Cross-validation allows us to do so by following the following steps:\n",
    "\n",
    "   - Split the training data into several (generally non-overlapping) subsets. If we use $M$ subsets, the method is referred to as $M$-fold cross-validation. If we consider each pattern a different subset, the method is usually referred to as leave-one-out (LOO) cross-validation.\n",
    "   - Carry out the training of the system $M$ times. For each run, use a different partition as a <i>validation</i> set, and use the restating partitions as the training set. Evaluate the performance for different choices of the hyperparameter (i.e., for different values of $k$ for the $k$-NN method).\n",
    "   - Average the validation error over all partitions, and pick the hyperparameter that provided the minimum validation error.\n",
    "   - Rerun the algorithm using all the training data, keeping the value of the parameter that came out of the cross-validation process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "## This fragment of code runs k-nn with M-fold cross validation\n",
    "\n",
    "# Obtain the indices for the different folds\n",
    "n_tr = X_tr.shape[0]\n",
    "M = n_tr\n",
    "permutation = np.random.permutation(n_tr)\n",
    "\n",
    "# Initialize sets of indices\n",
    "set_indices = {n: [] for n in range(M)}\n",
    "\n",
    "# Distribute data amont M partitions\n",
    "n = 0\n",
    "for pos in range(n_tr):\n",
    "    set_indices[n].append(permutation[pos])\n",
    "    n = (n+1) % M\n",
    "\n",
    "# Now, we run the cross-validation process using the k-nn method\n",
    "k_max = 30\n",
    "k_list = [2*j+1 for j in range(int(k_max))]\n",
    "\n",
    "# Obtain the validation errors\n",
    "pe_val = 0\n",
    "for n in range(M):\n",
    "    i_val = set_indices[n]\n",
    "    i_tr = []\n",
    "    for kk in range(M):\n",
    "        if not n==kk:\n",
    "            i_tr += set_indices[kk]\n",
    "    \n",
    "    pe_val_iter = []\n",
    "    for k in k_list:\n",
    "        y_tr_iter = knn_classifier(X_tr[i_tr], Y_tr[i_tr], X_tr[i_val], k)\n",
    "        pe_val_iter.append(np.mean(Y_tr[i_val] != y_tr_iter))\n",
    "\n",
    "    pe_val = pe_val + np.asarray(pe_val_iter).T\n",
    "\n",
    "pe_val = pe_val / M\n",
    "\n",
    "# We compute now the train and test errors curves\n",
    "pe_tr = [np.mean(Y_tr != knn_classifier(X_tr, Y_tr, X_tr, k).T) for k in k_list]\n",
    "\n",
    "k_opt = k_list[np.argmin(pe_val)]\n",
    "pe_tst = np.mean(Y_tst != knn_classifier(X_tr, Y_tr, X_tst, k_opt).T)\n",
    "\n",
    "plt.plot(k_list, pe_tr,'b--o',label='Training error')\n",
    "plt.plot(k_list, pe_val.T,'g--o',label='Validation error')\n",
    "plt.stem([k_opt], [pe_tst],'r-o',label='Test error')\n",
    "plt.legend(loc='best')\n",
    "plt.title('The optimal value of $k$ is ' + str(k_opt))\n",
    "plt.xlabel('$k$')\n",
    "plt.ylabel('Error rate')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 6. Scikit-learn implementation\n",
    "\n",
    "In practice, most well-known machine learning methods are implemented and available for python. Probably, the most complete library for machine learning is <a href=http://scikit-learn.org/stable/>Scikit-learn</a>. The following piece of code uses the method\n",
    "\n",
    "    KNeighborsClassifier\n",
    "    \n",
    "available in Scikit-learn, to compute the $k$-NN classifier using the four components of the observations in the original dataset. This routine allows us to classify a particular point using a weighted average of the targets of the neighbors:\n",
    "\n",
    "   To classify point ${\\bf x}$:\n",
    "   \n",
    "   - Find $k$ closest points to ${\\bf x}$ in the training set\n",
    "   - Average the corresponding targets, weighting each value according to the distance of each point to ${\\bf x}$, so that closer points have a larger influence in the estimation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "k = 5\n",
    "\n",
    "# import some data to play with\n",
    "iris = datasets.load_iris()\n",
    "\n",
    "# Take training test\n",
    "X_tr = np.array([xTrain_all[n] for n in range(nTrain_all) \n",
    "                 if cTrain_all[n]==c0 or cTrain_all[n]==c1])\n",
    "C_tr = [cTrain_all[n] for n in range(nTrain_all) \n",
    "          if cTrain_all[n]==c0 or cTrain_all[n]==c1]\n",
    "Y_tr = np.array([int(c==c1) for c in C_tr])\n",
    "n_tr = len(X_tr)\n",
    "\n",
    "# Take test set\n",
    "X_tst = np.array([xTest_all[n] for n in range(nTest_all) \n",
    "                 if cTest_all[n]==c0 or cTest_all[n]==c1])\n",
    "C_tst = [cTest_all[n] for n in range(nTest_all) \n",
    "         if cTest_all[n]==c0 or cTest_all[n]==c1]\n",
    "Y_tst = np.array([int(c==c1) for c in C_tst])\n",
    "n_tst = len(X_tst)\n",
    "\n",
    "for weights in ['uniform', 'distance']:\n",
    "    # we create an instance of Neighbours Classifier and fit the data.\n",
    "    clf = neighbors.KNeighborsClassifier(k, weights=weights)\n",
    "    clf.fit(X_tr, Y_tr)\n",
    "    Z = clf.predict(X_tst)\n",
    "    pe_tst = np.mean(Y_tst != Z)\n",
    "    print('Test error rate with ' + weights + ' weights = ' + str(pe_tst))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<a href = http://scikit-learn.org/stable/auto_examples/neighbors/plot_classification.html> Here</a> you can find an example of the application of `KNeighborsClassifier` to the complete 3-class Iris flower classification problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## $k$-NN Classification and Probability Estimation.\n",
    "\n",
    "If a sample ${\\bf x}$ has $m$ neighbors from class 1 and $k-m$ neighbors from class $0$, we can estimate the posterior probability that an observation ${\\bf x}$ belongs to class 1 as\n",
    "$$\n",
    "\\hat P(\\{y=1\\}) = \\frac{m}{k}\n",
    "$$\n",
    "Therefore, besides computing a decision about the class of the data, we can modify the $k$-NN algorithm to obtain posterior probability estimates.\n",
    "\n",
    "Note that the above equation is equivalent\n",
    "$$\n",
    "\\hat P(\\{y=1\\}) = \\frac{\\sum_{n \\in {\\mathcal N}({\\bf x})} y^{(n)}}{k}\n",
    "$$\n",
    "where ${\\mathcal N}({\\bf x})$ is the set of indices for the samples in the neighborhood of $\\bf x$.\n",
    "\n",
    "In other words, $\\hat P(\\{y=1\\})$ is the *average* of the neighbors labels. Averages can be computed using `KNeighborsRegressor`, which is useful for regression applications.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**Exercise 3**: Plot a $k$-NN posterior probability map for the Iris flower data, for $k=15$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select two classes\n",
    "c0 = 'Iris-versicolor' \n",
    "c1 = 'Iris-virginica'\n",
    "\n",
    "# Select two coordinates\n",
    "ind = [0, 1]\n",
    "\n",
    "# Take training test\n",
    "X_tr = np.array([[xTrain_all[n][i] for i in ind] for n in range(nTrain_all) \n",
    "                  if cTrain_all[n]==c0 or cTrain_all[n]==c1])\n",
    "C_tr = [cTrain_all[n] for n in range(nTrain_all) \n",
    "          if cTrain_all[n]==c0 or cTrain_all[n]==c1]\n",
    "Y_tr = np.array([int(c==c1) for c in C_tr])\n",
    "n_tr = len(X_tr)\n",
    "\n",
    "# Take test set\n",
    "X_tst = np.array([[xTest_all[n][i] for i in ind] for n in range(nTest_all) \n",
    "                 if cTest_all[n]==c0 or cTest_all[n]==c1])\n",
    "C_tst = [cTest_all[n] for n in range(nTest_all) \n",
    "         if cTest_all[n]==c0 or cTest_all[n]==c1]\n",
    "Y_tst = np.array([int(c==c1) for c in C_tst])\n",
    "n_tst = len(X_tst)\n",
    "\n",
    "# Separate components of x into different arrays (just for the plots)\n",
    "x0c0 = [X_tr[n][0] for n in range(n_tr) if Y_tr[n]==0]\n",
    "x1c0 = [X_tr[n][1] for n in range(n_tr) if Y_tr[n]==0]\n",
    "x0c1 = [X_tr[n][0] for n in range(n_tr) if Y_tr[n]==1]\n",
    "x1c1 = [X_tr[n][1] for n in range(n_tr) if Y_tr[n]==1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#<SOL>\n",

    "#</SOL>"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
