{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "# <font color='teal'> Introduction to Neural Networks and Pytorch </font>\n",
    "\n",
    "    Notebook version: 0.1 (Nov 14, 2020)\n",
    "\n",
    "    Authors: Jerónimo Arenas García (jarenas@ing.uc3m.es)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    },
    "tags": []
   },
   "source": [
    "    Changes: v.0.1. (Nov 14, 2020) - First version\n",
    "    \n",
    "    Pending changes:\n",
    "        Use epochs instead of iters in first part of notebook\n",
    "        Add an example with dropout\n",
    "        Add theory about CNNs\n",
    "        Define functions for the training of NNs and display of the results\n",
    "        in order to simplify code cells"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "size = 18\n",
    "params = {'legend.fontsize': 'Large',\n",
    "          'axes.labelsize': size,\n",
    "          'axes.titlesize': size,\n",
    "          'xtick.labelsize': size*0.75,\n",
    "          'ytick.labelsize': size*0.75}\n",
    "plt.rcParams.update(params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## <font color='teal'> 1. Introduction and purpose of this Notebook </font>\n",
    "\n",
    "### <font color='teal'> 1.1. About Neural Networks </font>\n",
    "\n",
    "* Neural Networks (NN) have become the state of the art for many machine learning problems\n",
    "    * Natural Language Processing\n",
    "    * Computer Vision\n",
    "    * Image Recognition\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* They are in widespread use for many applications, e.g.,\n",
    "    * Language translation (<a href=\"https://arxiv.org/pdf/1609.08144.pdf\">Google Neural Machine Translation System</a>) \n",
    "    * Automatic speech recognition (<a href=\"https://machinelearning.apple.com/research/hey-siri\">Hey Siri!</a> DNN overview)\n",
    "    * Autonomous navigation (<a href=\"https://venturebeat.com/2020/04/13/facebooks-ai-teaches-robots-to-navigate-environments-using-less-data/\">Facebook Robot Autonomous 3D Navigation</a>)\n",
    "    * Automatic plate recognition\n",
    "    \n",
    "<center><img src=\"figures/ComputerVision.png\" /></center>\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "Feed Forward Neural Networks [have been around since 1960](https://www.skynettoday.com/overviews/neural-net-history) but only recently (last 10-12 years) have they met their expectations, and improve other machine learning algorithms\n",
    "\n",
    "* Computation resources are now available at large scale\n",
    "* Cloud Computing (AWS, Azure)\n",
    "* From MultiLayer Perceptrons to Deep Learning\n",
    "* Big Data sets\n",
    "* This has also made possible an intense research effort resulting in\n",
    "    * Topologies better suited to particular problems (CNNs, RNNs)\n",
    "    * New training strategies providing better generalization\n",
    "\n",
    "In parallel, Deep Learning Platforms have emerged that make design, implementation, training, and production of DNNs feasible for everyone"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### <font color='teal'> 1.2. Scope</font>\n",
    "\n",
    "* To provide just an overview of most important NNs and DNNs concepts\n",
    "* Connecting with already studied methods as starting point\n",
    "* Introduction to PyTorch\n",
    "* Providing links to external sources for further study"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "### <font color='teal'> 1.3. Outline</font>\n",
    "\n",
    "1. Introduction and purpose of this Notebook\n",
    "2. Introduction to Neural Networks\n",
    "3. Implementing Deep Networks with PyTorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### <font color='teal'> 1.4. Other resources </font>\n",
    "\n",
    "* We point here to external resources and tutorials that are excellent material for further study of the topic\n",
    "* Most of them include examples and exercises using numpy and PyTorch\n",
    "* This notebook uses examples and other material from some of these sources"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "|Tutorial|Description|\n",
    "|-----|---------------------|\n",
    "|<a href=\"https://www.simplilearn.com/tutorials/deep-learning-tutorial\"> <img src=\"figures/simplilearn.png\" width=\"100\"/> </a>|Very general tutorial including videos and an overview of top deep learning platforms|\n",
    "|<a href=\"http://d2l.ai/\"> <img src=\"figures/dl2ai.png\" width=\"100\"/> </a>|Very complete book with a lot of theory and examples for MxNET, PyTorch, and TensorFlow|\n",
    "|<a href=\"https://pytorch.org/tutorials/\"> <img src=\"figures/PyTorch.png\" width=\"100\"/> </a>|Official tutorials from the PyTorch project. Contains a 60 min overview, and a very practical *learning PyTorch with examples* tutorial|\n",
    "|<a href=\"https://www.kaggle.com/kanncaa1/deep-learning-tutorial-for-beginners\"> <img src=\"figures/kaggle.png\" width=\"100\"/> </a>|Kaggle tutorials covering an introduction to Neural Networks using Numpy, and a second one offering a PyTorch tutorial|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "In addition to this, PyTorch MOOCs can be followed for free in main sites: edX, Coursera, Udacity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## <font color='teal'> 2. Introduction to Neural Networks </font>\n",
    "\n",
    "In this section, we will implement neural networks from scratch using Numpy arrays\n",
    "\n",
    "* No need to learn any new Python libraries\n",
    "* But we need to deal with complexity of multilayer networks\n",
    "* Low-level implementation will be useful to grasp the most important concepts concerning DNNs\n",
    "    * Back-propagation\n",
    "    * Activation functions\n",
    "    * Loss functions\n",
    "    * Optimization methods\n",
    "    * Generalization\n",
    "    * Special layers and configurations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### <font color='teal'> 2.1. Data preparation </font>\n",
    "\n",
    "We start by loading some data sets that will be used to carry out the exercises\n",
    "\n",
    "### <font color='olive'>Sign language digits data set</font>\n",
    "\n",
    "* Dataset is taken from <a href=\"https://www.kaggle.com/ardamavi/sign-language-digits-dataset\"> Kaggle</a> and used in the above referred tutorial\n",
    "* 2062 digits in sign language. $64 \\times 64$ images\n",
    "* Problem with 10 classes. One hot encoding for the label matrix\n",
    "* Input data are images, we create also a flattened version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "digitsX = np.load('./data/Sign-language-digits-dataset/X.npy')\n",
    "digitsY = np.load('./data/Sign-language-digits-dataset/Y.npy')\n",
    "K = digitsX.shape[0]\n",
    "img_size = digitsX.shape[1]\n",
    "digitsX_flatten = digitsX.reshape(K,img_size*img_size)\n",
    "\n",
    "print('Size of Input Data Matrix:', digitsX.shape)\n",
    "print('Size of Flattned Input Data Matrix:', digitsX_flatten.shape)\n",
    "print('Size of label Data Matrix:', digitsY.shape)\n",
    "selected = [260, 1400]\n",
    "plt.subplot(1, 2, 1), plt.imshow(digitsX[selected[0]].reshape(img_size, img_size)), plt.axis('off')\n",
    "plt.subplot(1, 2, 2), plt.imshow(digitsX[selected[1]].reshape(img_size, img_size)), plt.axis('off')\n",
    "plt.show()\n",
    "print('Labels corresponding to figures:', digitsY[selected,])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "### <font color='olive'>Dogs vs Cats data set</font>\n",
    "\n",
    "* Dataset is taken from <a href=\"https://www.kaggle.com/c/dogs-vs-cats\"> Kaggle</a>\n",
    "* 25000 pictures of dogs and cats\n",
    "* Binary problem\n",
    "* Input data are images, we create also a flattened version\n",
    "* Original images are RGB, and arbitrary size\n",
    "* Preprocessed images are $64 \\times 64$ and gray scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "# Preprocessing of original Dogs and Cats Pictures\n",
    "# Adapted from https://medium.com/@mrgarg.rajat/kaggle-dogs-vs-cats-challenge-complete-step-by-step-guide-part-1-a347194e55b1\n",
    "# RGB channels are collapsed in GRAYSCALE\n",
    "# Images are resampled to 64x64\n",
    "\n",
    "\"\"\"\n",
    "import os, cv2  # cv2 -- OpenCV\n",
    "\n",
    "train_dir = './data/DogsCats/train/'\n",
    "rows = 64\n",
    "cols = 64\n",
    "train_images = sorted([train_dir+i for i in os.listdir(train_dir)])\n",
    "\n",
    "def read_image(file_path):\n",
    "    image = cv2.imread(file_path, cv2.IMREAD_GRAYSCALE)\n",
    "    return cv2.resize(image, (rows, cols),interpolation=cv2.INTER_CUBIC)\n",
    "\n",
    "def prep_data(images):\n",
    "    m = len(images)\n",
    "    X = np.ndarray((m, rows, cols), dtype=np.uint8)\n",
    "    y = np.zeros((m,))\n",
    "    print(\"X.shape is {}\".format(X.shape))\n",
    "  \n",
    "    for i,image_file in enumerate(images) :\n",
    "        image = read_image(image_file)\n",
    "        X[i,] = np.squeeze(image.reshape((rows, cols)))\n",
    "        if 'dog' in image_file.split('/')[-1].lower():\n",
    "            y[i] = 1\n",
    "        elif 'cat' in image_file.split('/')[-1].lower():\n",
    "            y[i] = 0\n",
    "      \n",
    "        if i%5000 == 0 :\n",
    "            print(\"Proceed {} of {}\".format(i, m))\n",
    "    \n",
    "    return X,y\n",
    "\n",
    "X_train, y_train = prep_data(train_images)\n",
    "np.save('./data/DogsCats/X.npy', X_train)\n",
    "np.save('./data/DogsCats/Y.npy', y_train)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "DogsCatsX = np.load('./data/DogsCats/X.npy')\n",
    "DogsCatsY = np.load('./data/DogsCats/Y.npy')\n",
    "K = DogsCatsX.shape[0]\n",
    "img_size = DogsCatsX.shape[1]\n",
    "DogsCatsX_flatten = DogsCatsX.reshape(K,img_size*img_size)\n",
    "\n",
    "print('Size of Input Data Matrix:', DogsCatsX.shape)\n",
    "print('Size of Flattned Input Data Matrix:', DogsCatsX_flatten.shape)\n",
    "print('Size of label Data Matrix:', DogsCatsY.shape)\n",
    "selected = [260, 16000]\n",
    "plt.subplot(1, 2, 1), plt.imshow(DogsCatsX[selected[0]].reshape(img_size, img_size)), plt.axis('off')\n",
    "plt.subplot(1, 2, 2), plt.imshow(DogsCatsX[selected[1]].reshape(img_size, img_size)), plt.axis('off')\n",
    "plt.show()\n",
    "print('Labels corresponding to figures:', DogsCatsY[selected,])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "### <font color='teal'> 2.2. Logistic Regression as a Simple Neural Network </font>\n",
    "\n",
    "* We can consider logistic regression as an extremely simple (1 layer) neural network\n",
    "\n",
    "<center><img src=\"figures/LR_network.png\" width=\"600\"/></center>\n",
    "\n",
    "* In this context, $\\text{NLL}({\\bf w})$ is normally referred to as cross-entropy loss\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "* We need to find parameters $\\bf w$ and $b$ to minimize the loss $\\rightarrow$ GD / SGD\n",
    "* Gradient computation can be simplified using the **<font color='navy'>chain rule</font>**\n",
    "\n",
    "<br>\n",
    "\\begin{align}\n",
    "\\frac{\\partial \\text{NLL}}{\\partial {\\bf w}} & = \\frac{\\partial \\text{NLL}}{\\partial {\\hat y}} \\cdot \\frac{\\partial \\hat y}{\\partial o} \\cdot \\frac{\\partial o}{\\partial {\\bf w}} \\\\\n",
    "& = \\sum_{k=0}^{K-1} \\left[\\frac{1 - y_k}{1 - \\hat y_k} - \\frac{y_k}{\\hat y_k}\\right]\\hat y_k (1-\\hat y_k) {\\bf x}_k \\\\\n",
    "& = \\sum_{k=0}^{K-1} \\left[\\hat y_k - y_k\\right] {\\bf x}_k \\\\\n",
    "\\frac{\\partial \\text{NLL}}{\\partial b} & = \\sum_{k=0}^{K-1} \\left[\\hat y_k - y_k  \\right]\n",
    "\\end{align}\n",
    "\n",
    "* Gradient Descent Optimization\n",
    "\n",
    "<br>\n",
    "$${\\bf w}_{n+1} = {\\bf w}_n + \\rho_n \\sum_{k=0}^{K-1} (y_k - \\hat y_k){\\bf x}_k$$\n",
    "$$b_{n+1} = b_n + \\rho_n \\sum_{k=0}^{K-1} (y_k - \\hat y_k)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def get_dataset(dataset_name, forze_binary=False):\n",
    "\n",
    "    if dataset_name == 'DogsCats':\n",
    "        X = DogsCatsX_flatten\n",
    "        y = DogsCatsY\n",
    "    elif dataset_name == 'digits':\n",
    "        if forze_binary:\n",
    "            #Zero and Ones are one hot encoded in columns 1 and 4\n",
    "            X0 = digitsX_flatten[np.argmax(digitsY, axis=1)==1,]\n",
    "            X1 = digitsX_flatten[np.argmax(digitsY, axis=1)==4,]\n",
    "            X = np.vstack((X0, X1))\n",
    "            y = np.zeros(X.shape[0])\n",
    "            y[X0.shape[0]:] = 1\n",
    "        else:\n",
    "            X = digitsX_flatten\n",
    "            y = digitsY\n",
    "    else:\n",
    "        print(\"-- ERROR: Unknown dataset\")\n",
    "        return\n",
    "    \n",
    "    # Joint normalization of all data. For images [-.5, .5] scaling is frequent\n",
    "    min_max_scaler = MinMaxScaler(feature_range=(-.5, .5))\n",
    "    X = min_max_scaler.fit_transform(X)\n",
    "\n",
    "    # Generate train and validation data, shuffle\n",
    "    X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42, shuffle=True)\n",
    "    \n",
    "    return X_train, X_val, y_train, y_val\n",
    "\n",
    "X_train, X_val, y_train, y_val = get_dataset('digits', forze_binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# Define some useful functions\n",
    "def logistic(t):\n",
    "    return 1.0 / (1 + np.exp(-t))\n",
    "\n",
    "def forward(w,b,x):\n",
    "    #Calcula la salida de la red\n",
    "    return logistic(x.dot(w)+b)\n",
    "\n",
    "def backward(y,y_hat,x):\n",
    "    #Calcula los gradientes\n",
    "    #w_grad = x.T.dot((1-y)*y_hat - y*(1-y_hat))/len(y)\n",
    "    #b_grad = np.sum((1-y)*y_hat - y*(1-y_hat))/len(y)\n",
    "    w_grad = x.T.dot(y_hat-y)/len(y)\n",
    "    b_grad = np.sum(y_hat-y)/len(y)\n",
    "    return w_grad, b_grad\n",
    "    \n",
    "def accuracy(y, y_hat):\n",
    "    return np.mean(y == (y_hat>=0.5))\n",
    "\n",
    "def loss(y, y_hat):\n",
    "    return -np.sum(y*np.log(y_hat)+(1-y)*np.log(1-y_hat))/len(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "#Neural Network Training\n",
    "\n",
    "epochs = 50\n",
    "rho = .05 #Use this setting for Sign Digits Dataset\n",
    "\n",
    "#Parameter initialization\n",
    "w = .1 * np.random.randn(X_train.shape[1])\n",
    "b = .1 * np.random.randn(1)\n",
    "\n",
    "loss_train = np.zeros(epochs)\n",
    "loss_val = np.zeros(epochs)\n",
    "acc_train = np.zeros(epochs)\n",
    "acc_val = np.zeros(epochs)\n",
    "\n",
    "for epoch in np.arange(epochs):\n",
    "    y_hat_train = forward(w, b, X_train)\n",
    "    y_hat_val = forward(w, b, X_val)\n",
    "    w_grad, b_grad = backward(y_train, y_hat_train, X_train)\n",
    "    w = w - rho * w_grad\n",
    "    b = b - rho * b_grad\n",
    "    \n",
    "    loss_train[epoch] = loss(y_train, y_hat_train)\n",
    "    loss_val[epoch] = loss(y_val, y_hat_val)\n",
    "    acc_train[epoch] = accuracy(y_train, y_hat_train)\n",
    "    acc_val[epoch] = accuracy(y_val, y_hat_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(14,5))\n",
    "plt.subplot(1, 2, 1), plt.plot(loss_train, 'b'), plt.plot(loss_val, 'r'), plt.legend(['train', 'val']), plt.title('Cross-entropy loss')\n",
    "plt.subplot(1, 2, 2), plt.plot(acc_train, 'b'), plt.plot(acc_val, 'r'), plt.legend(['train', 'val']), plt.title('Accuracy')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### <font color='olive'>Exercise</font>\n",
    "\n",
    "* Study the behavior of the algorithm changing the number of epochs and the learning rate\n",
    "\n",
    "* Repeat the analysis for the other dataset, trying to obtain as large an accuracy value as possible\n",
    "\n",
    "* What do you believe are the reasons for the very different performance for both datasets?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Linear logistic regression allowed us to review a few concepts that are key for Neural Networks:\n",
    "\n",
    "* Network topology (In this case, a linear network with one layer)\n",
    "* Activation functions\n",
    "* Parametric approach ($\\bf w$/$b$)\n",
    "* Parameter initialization\n",
    "* Obtaining the network prediction using *forward* computation\n",
    "* Loss function\n",
    "* Parameter gradient calculus using *backward* computation\n",
    "* Optimization method for parameters update (here, GD)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### <font color='teal'> 2.3. (Multiclass) SoftMax Regression </font>\n",
    "\n",
    "* One hot encoding output, e.g., $[0, 1, 0, 0]$, $[0, 0, 0, 1]$\n",
    "* Used to encode categorial variables without predefined order\n",
    "* Similar to logistic regression, network tries to predict class probability\n",
    "$$\\hat y_{k,j} = \\hat P(y_k=j|{\\bf x}_k)$$\n",
    "* Network output should satisfy \"probability constraints\"\n",
    "$$\\hat y_{k,j} \\in [0,1]\\qquad \\text{and} \\qquad \\sum_j \\hat y_{k,j} = 1$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* Softmax regression network topology:\n",
    "<img src=\"figures/SR_network.png\" width=\"600\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "### <font color='olive'>Notation</font>\n",
    "\n",
    "In this section, it is important to pay attention to subindexes:\n",
    "\n",
    "|Notation/ Variable Name|Definition|\n",
    "|-----------------------|---------------------------------|\n",
    "|$y_k \\in [0,\\dots,M-1]$|The label of pattern $k$|\n",
    "|${\\bf y}_k$|One hot encoding of the label of pattern $k$|\n",
    "|$y_{k,m}$|$m$-th component of vector ${\\bf y}_k$|\n",
    "|$y_{m}$|$m$-th component of generic vector ${\\bf y}$ (i.e., for an undefined pattern)|\n",
    "|$\\hat {\\bf y}_k$|Network output for pattern $k$|\n",
    "|$\\hat y_{k,m}$|$m$-th network output for pattern $k$|\n",
    "|$\\hat y_{m}$|$m$-th network output for an undefined pattern)|\n",
    "|$k$|Index used for pattern enumeration|\n",
    "|$m$|Index used for network output enumeration|\n",
    "|$j$|Secondary index for selected network output|\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### <font color='olive'>The softmax function</font>\n",
    "\n",
    "* It is to multiclass problems as the logistic function for binary classification\n",
    "* Invented in 1959 by the social scientist R. Duncan Luce\n",
    "* Transforms a set of $M$ real numbers to satisfy \"probability\" constraints\n",
    "\n",
    "<br>\n",
    "$${\\bf \\hat y} = \\text{softmax}({\\bf o}) \\qquad \\text{where} \\qquad \\hat y_j = \\frac{\\exp(o_j)}{\\sum_m \\exp(o_m)} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* Continuous and **<font color=\"navy\">differentiable</font>** function\n",
    "\n",
    "<br>\n",
    "$$\\frac{\\partial \\hat y_j}{\\partial o_j} = \\hat y_j (1 - \\hat y_j) \\qquad \\text{and} \\qquad \\frac{\\partial \\hat y_j}{\\partial o_m} = - \\hat y_j \\hat y_m$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* The classifier is still linear, since\n",
    "\n",
    "<br>\n",
    "$$\\arg\\max \\hat {\\bf y} = \\arg\\max \\hat {\\bf o} = \\arg\\max \\{{\\bf W} {\\bf x} + {\\bf b}\\}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### <font color='olive'>Cross-entropy loss for multiclass problems</font>\n",
    "\n",
    "* Similarly to logistic regression, minimization of the log-likelihood can be stated to obtain ${\\bf W}$ and ${\\bf b}$\n",
    "\n",
    "<br>\n",
    "$$\\text{Binary}: \\text{NLL}({\\bf w}, b) = - \\sum_{k=0}^{K-1} \\log \\hat P(y_k|{\\bf x}_k)$$\n",
    "$$\\text{Multiclass}: \\text{NLL}({\\bf W}, {\\bf b}) = - \\sum_{k=0}^{K-1} \\log \\hat P(y_k|{\\bf x}_k)$$\n",
    "\n",
    "* Using one hot encoding for the label vector of each sample, e.g., $y_k = 2 \\rightarrow {\\bf y}_k = [0, 0, 1, 0]$\n",
    "\n",
    "$$\\text{NLL}({\\bf W}, {\\bf b}) = - \\sum_{k=0}^{K-1} \\sum_{m=0}^{M-1} y_{k,m} \\log \\hat P(m|{\\bf x}_k)= - \\sum_{k=0}^{K-1} \\sum_{m=0}^{M-1} y_{k,m} \\log \\hat y_{k,m} = \\sum_{k=0}^{K-1} l({\\bf y}_k, \\hat {\\bf y}_k)$$\n",
    "\n",
    "* Note that for each pattern, only one element in the inner sum (the one indexed with $m$) is non-zero\n",
    "\n",
    "* In the context of Neural Networks, this cost is referred to as the cross-entropy loss\n",
    "\n",
    "<br>\n",
    "$$l({\\bf y}, \\hat {\\bf y}) = - \\sum_{m=0}^{M-1} y_{m} \\log \\hat y_{m}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### <font color='olive'>Network optimization</font>\n",
    "\n",
    "* Gradient Descent Optimization\n",
    "\n",
    "<br>\n",
    "$${\\bf W}_{n+1} = {\\bf W}_n - \\rho_n \\sum_{k=0}^{K-1} \\frac{\\partial l({\\bf y}_k,{\\hat {\\bf y}_k})}{\\partial {\\bf W}}$$\n",
    "$${\\bf b}_{n+1} = {\\bf b}_n - \\rho_n \\sum_{k=0}^{K-1} \\frac{\\partial l({\\bf y}_k,{\\hat {\\bf y}_k})}{\\partial {\\bf b}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "* We compute derivatives using the chain rule (we ignore dimension mismatchs, and rearrange at the end)\n",
    "\n",
    "<br>\n",
    "\\begin{align}\n",
    "\\frac{\\partial l({\\bf y},{\\hat {\\bf y}})}{\\partial {\\bf W}} &= \\frac{\\partial l({\\bf y},{\\hat {\\bf y}})}{\\partial \\hat {\\bf y}} \\cdot \\frac{\\partial \\hat {\\bf y}}{\\partial {\\bf o}} \\cdot \\frac{\\partial {\\bf o}}{\\partial {\\bf W}} \\\\ & = \\left[\\begin{array}{c} 0 \\\\ 0 \\\\ \\vdots \\\\ - 1/\\hat y_j \\\\ \\vdots \\end{array}\\right] \\left[ \\begin{array}{ccccc} \\hat y_1 (1 - \\hat y_1) & -\\hat y_1 \\hat y_2 & \\dots & -\\hat y_1 \\hat y_j & \\dots \\\\ -\\hat y_2 \\hat y_1 & \\hat y_2 (1 - \\hat y_2) & \\dots & -\\hat y_2 \\hat y_j & \\dots \\\\ \\vdots & \\vdots & \\ddots & \\vdots & \\vdots \\\\ - \\hat y_j \\hat y_1 & -\\hat y_j \\hat y_2 & \\dots & \\hat y_j (1-\\hat y_j) & \\dots \\\\ \\vdots & \\vdots & \\ddots & \\vdots & \\vdots \\end{array}\\right] {\\bf x}^\\top \\\\\n",
    "& = \\left[\\begin{array}{c}\\hat y_1 \\\\ \\hat y_2 \\\\ \\vdots \\\\ \\hat y_j - 1 \\\\ \\vdots \\end{array} \\right] {\\bf x}^\\top \\\\\n",
    "& = (\\hat {\\bf y} - {\\bf y}){\\bf x}^\\top \\\\\n",
    "\\\\\n",
    "\\frac{\\partial l({\\bf y},{\\hat {\\bf y}})}{\\partial {\\bf b}} & = (\\hat {\\bf y} - {\\bf y}){\\bf 1}^\\top\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "dataset = 'digits'\n",
    "X_train, X_val, y_train, y_val = get_dataset('digits')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# Define some useful functions\n",
    "def softmax(t):\n",
    "    \"\"\"Compute softmax values for each sets of scores in t.\"\"\"\n",
    "    e_t = np.exp(t)\n",
    "    return e_t / e_t.sum(axis=1)[:,np.newaxis]\n",
    "\n",
    "def forward(w, b, x):\n",
    "    #Calcula la salida de la red\n",
    "    return softmax(x.dot(w.T) + b.T)\n",
    "\n",
    "def backward(y, y_hat, x):\n",
    "    #Calcula los gradientes\n",
    "    W_grad = (y_hat - y).T.dot(x) / len(y)\n",
    "    b_grad = ((y_hat - y).sum(axis=0)[:,np.newaxis]) / len(y)\n",
    "    return W_grad, b_grad\n",
    "    \n",
    "def accuracy(y, y_hat):\n",
    "    return np.mean(np.argmax(y, axis=1) == np.argmax(y_hat, axis=1))\n",
    "\n",
    "def loss(y, y_hat):\n",
    "    return -np.sum(y * np.log(y_hat))/len(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "#Neural Network Training\n",
    "\n",
    "epochs = 300\n",
    "rho = .1\n",
    "\n",
    "#Parameter initialization\n",
    "W = .1 * np.random.randn(y_train.shape[1], X_train.shape[1])\n",
    "b = .1 * np.random.randn(y_train.shape[1], 1)\n",
    "\n",
    "loss_train = np.zeros(epochs)\n",
    "loss_val = np.zeros(epochs)\n",
    "acc_train = np.zeros(epochs)\n",
    "acc_val = np.zeros(epochs)\n",
    "\n",
    "for epoch in np.arange(epochs):\n",
    "    y_hat_train = forward(W, b, X_train)\n",
    "    y_hat_val = forward(W, b, X_val)\n",
    "    W_grad, b_grad = backward(y_train, y_hat_train, X_train)\n",
    "    W = W - rho * W_grad\n",
    "    b = b - rho * b_grad\n",
    "    \n",
    "    loss_train[epoch] = loss(y_train, y_hat_train)\n",
    "    loss_val[epoch] = loss(y_val, y_hat_val)\n",
    "    acc_train[epoch] = accuracy(y_train, y_hat_train)\n",
    "    acc_val[epoch] = accuracy(y_val, y_hat_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(14,5))\n",
    "plt.subplot(1, 2, 1), plt.plot(loss_train, 'b'), plt.plot(loss_val, 'r'), plt.legend(['train', 'val']), plt.title('Cross-entropy loss')\n",
    "plt.subplot(1, 2, 2), plt.plot(acc_train, 'b'), plt.plot(acc_val, 'r'), plt.legend(['train', 'val']), plt.title('Accuracy')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### <font color='olive'>Exercise</font>\n",
    "\n",
    "* Study the behavior of the algorithm changing the number of iterations and the learning rate\n",
    "\n",
    "* Obtain the confusion matrix, and study which classes are more difficult to classify\n",
    "\n",
    "* Think about the differences between using this 10-class network, vs training 10 binary classifiers, one for each class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "As in linear logistic regression note that we covered the following aspects of neural network design, implementation, and training:\n",
    "\n",
    "* Network topology (In this case, a linear network with one layer and $M$ ouptuts)\n",
    "* Activation functions (softmax activation)\n",
    "* Parameter initialization ($\\bf W$/$b$)\n",
    "* Obtaining the network prediction using *forward* computation\n",
    "* Loss function\n",
    "* Parameter gradient calculus using *backward* computation\n",
    "* Optimization method for parameters update (here, GD)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### <font color='teal'> 2.3. Multi Layer Networks (Deep Networks) </font>\n",
    "\n",
    "Previous networks are constrained in the sense that they can only implement linear classifiers. In this section we analyze how we can extend them to implement non-linear classification:\n",
    "* Fixed non-linear transformations of inputs: ${\\bf z} = {\\bf{f}}({\\bf x})$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* Parametrize the transformation using additional non-linear layers\n",
    "<center><img src=\"figures/LR_MLPnetwork.png\" width=\"600\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "* When counting layers, we normally ignore the input layer, since there is no computation involved\n",
    "* Intermediate layers are normally referred to as \"hidden\" layers\n",
    "* Non-linear activations result in an overall non-linear classifier\n",
    "* We can still use Gradient Descent Optimization as long as the network loss derivatives with respect to all parameters exist and are continuous\n",
    "* This is already deep learning. We can have two layers or more, each with different numbers of neurons. But as long as derivatives with respect to parameters can be calculated, the network can be optimized\n",
    "* Finding an appropriate number of layers for a particular problem, as well as the number of neurons per layer, requires exploration\n",
    "* The more data we have for training the network, the more parameters we can afford, making feasible the use of more complex topologies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### <font color='olive'>Example: 2-layer network for binary classification</font>\n",
    "\n",
    "* Network topology\n",
    "    * Hidden layer with $n_h$ neurons\n",
    "    * Hyperbolic tangent activation function for the hidden layer\n",
    "    $${\\bf h} = \\text{tanh}({\\bf o}^{(1)})= \\text{tanh}\\left({\\bf W}^{(1)} {\\bf x} + {\\bf b}^{(1)}\\right)$$\n",
    "    * Output layer is linear with logistic activation (as in logistic regression)\n",
    "    $$\\hat y = \\text{logistic}(o) = \\text{logistic}\\left({{\\bf w}^{(2)}}^\\top {\\bf h} + b^{(2)}\\right)$$\n",
    "    \n",
    "* Cross-entropy loss\n",
    "\n",
    "$$l(y,\\hat y) = -\\left[ y \\log(\\hat y) + (1 - y ) \\log(1 - \\hat y) \\right], \\qquad \\text{with } y\\in [0,1]$$\n",
    "\n",
    "* Update of output layer weights as in logistic regression (use ${\\bf h}$ instead of ${\\bf x}$)\n",
    "\n",
    "$${\\bf w}_{n+1}^{(2)} = {\\bf w}_n^{(2)} + \\rho_n \\sum_{k=0}^{K-1} (y_k - \\hat y_k){\\bf h}_k$$\n",
    "$$b_{n+1}^{(2)} = b_n^{(2)} + \\rho_n \\sum_{k=0}^{K-1} (y_k - \\hat y_k)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<center><img src=\"figures/forward_graph.png\" width=\"500\"/></center>\n",
    "\n",
    "* For updating the input layer parameters we need to use the chain rule (we ignore dimensions and rearrange at the end)\n",
    "\n",
    "\\begin{align}\n",
    "\\frac{\\partial l(y, \\hat y)}{\\partial {\\bf W}^{(1)}} \n",
    "    & = \\frac{\\partial l(y, \\hat y)}{\\partial o} \n",
    "            \\cdot \\frac{\\partial o}{\\partial {\\bf h}} \n",
    "            \\cdot \\frac{\\partial {\\bf h}}{\\partial {\\bf o}^{(1)}} \n",
    "            \\cdot \\frac{\\partial {\\bf o}^{(1)}}{\\partial {\\bf W}^{(1)}} \\\\\n",
    "    & = (\\hat y - y) [{\\bf w}^{(2)} \\odot ({\\bf 1}-{\\bf h})^2] {\\bf x}^{\\top}\n",
    "\\end{align}\n",
    "\n",
    "\\begin{align}\n",
    "\\frac{\\partial l(y, \\hat y)}{\\partial {\\bf b}^{(1)}} \n",
    "    & = \\frac{\\partial l(y, \\hat y)}{\\partial o} \n",
    "            \\cdot \\frac{\\partial o}{\\partial {\\bf h}} \n",
    "            \\cdot \\frac{\\partial {\\bf h}}{\\partial {\\bf o}^{(1)}} \n",
    "            \\cdot \\frac{\\partial {\\bf o}^{(1)}}{\\partial {\\bf b}^{(1)}} \\\\\n",
    "& = (\\hat y - y) [{\\bf w}^{(2)} \\odot ({\\bf 1}-{\\bf h})^2]\n",
    "\\end{align}\n",
    "\n",
    "where $\\odot$ denotes component-wise multiplication and the square after $({\\bf 1}-{\\bf h})$ should be computed component-wise\n",
    "\n",
    "* GD update rules become\n",
    "$${\\bf W}_{n+1}^{(1)} = {\\bf W}_n^{(1)} + \\rho_n \\sum_{k=0}^{K-1} (y_k - \\hat y_k)[{\\bf w}^{(2)} \\odot ({\\bf 1}-{\\bf h}_k)^2] {\\bf x}_k^{\\top}$$\n",
    "$${\\bf b}_{n+1}^{(1)} = {\\bf b}_n^{(1)} + \\rho_n \\sum_{k=0}^{K-1} (y_k - \\hat y_k)[{\\bf w}^{(2)} \\odot ({\\bf 1}-{\\bf h}_k)^2]$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "\n",
    "<center><img src=\"figures/forward_graph.png\" width=\"500\"/></center>\n",
    "\n",
    "* The process can be implemented as long as the derivatives of the network overall loss with respect to parameters can be computed\n",
    "\n",
    "* Forward computation graphs represent how the network output can be computed\n",
    "\n",
    "* We can then reverse the graph to compute derivatives with respect to parameters\n",
    "\n",
    "* Deep Learning libraries implement automatic gradient camputation\n",
    "   * We just define network topology\n",
    "   * Computation of gradients is carried out automatically"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dataset = 'DogsCats'\n",
    "dataset = 'digits'\n",
    "\n",
    "X_train, X_val, y_train, y_val = get_dataset(dataset, forze_binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Define some useful functions\n",
    "def logistic(t):\n",
    "    return 1.0 / (1 + np.exp(-t))\n",
    "\n",
    "def forward(W1, b1, w2, b2, x):\n",
    "    #Calcula la salida de la red\n",
    "    h = x.dot(W1.T) + b1\n",
    "    y_hat = logistic(h.dot(w2) + b2)\n",
    "    #Provide also hidden units value for backward gradient step\n",
    "    return h, y_hat\n",
    "\n",
    "def backward(y, y_hat, h, x, w2):\n",
    "    #Calcula los gradientes\n",
    "    w2_grad = h.T.dot(y_hat - y) / len(y)\n",
    "    b2_grad = np.sum(y_hat - y) / len(y)\n",
    "    W1_grad = ((w2[np.newaxis,] * ((1 - h)**2) * (y_hat - y)[:,np.newaxis]).T.dot(x)) / len(y)\n",
    "    b1_grad = ((w2[np.newaxis,] * ((1 - h)**2) * (y_hat - y)[:,np.newaxis]).sum(axis=0)) / len(y)\n",
    "    return w2_grad, b2_grad, W1_grad, b1_grad\n",
    "    \n",
    "def accuracy(y, y_hat):\n",
    "    return np.mean(y == (y_hat >= 0.5))\n",
    "\n",
    "def loss(y, y_hat):\n",
    "    return - np.sum(y * np.log(y_hat) + (1 - y) * np.log(1 - y_hat)) / len(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "#Neural Network Training\n",
    "epochs = 1000\n",
    "rho = .05\n",
    "\n",
    "#Parameter initialization\n",
    "n_h = 5\n",
    "W1 = .01 * np.random.randn(n_h, X_train.shape[1])\n",
    "b1 = .01 * np.random.randn(n_h)\n",
    "w2 = .01 * np.random.randn(n_h)\n",
    "b2 = .01 * np.random.randn(1)\n",
    "\n",
    "loss_train = np.zeros(epochs)\n",
    "loss_val = np.zeros(epochs)\n",
    "acc_train = np.zeros(epochs)\n",
    "acc_val = np.zeros(epochs)\n",
    "\n",
    "for epoch in np.arange(epochs):\n",
    "    h, y_hat_train = forward(W1, b1, w2, b2, X_train)\n",
    "    dum, y_hat_val = forward(W1, b1, w2, b2, X_val)\n",
    "    w2_grad, b2_grad, W1_grad, b1_grad = backward(y_train, y_hat_train, h, X_train, w2)\n",
    "    W1 = W1 - rho/10 * W1_grad\n",
    "    b1 = b1 - rho/10 * b1_grad\n",
    "    w2 = w2 - rho * w2_grad\n",
    "    b2 = b2 - rho * b2_grad\n",
    "    \n",
    "    loss_train[epoch] = loss(y_train, y_hat_train)\n",
    "    loss_val[epoch] = loss(y_val, y_hat_val)\n",
    "    acc_train[epoch] = accuracy(y_train, y_hat_train)\n",
    "    acc_val[epoch] = accuracy(y_val, y_hat_val)\n",
    "    \n",
    "    print(f'Current epoch: {epoch + 1}  \\r', end=\"\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### <font color='olive'>Results in Dogs vs Cats dataset ($epochs = 1000$ and $\\rho = 0.05$)</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(14,5))\n",
    "plt.subplot(1, 2, 1), plt.plot(loss_train, 'b'), plt.plot(loss_val, 'r'), plt.legend(['train', 'val']), plt.title('Cross-entropy loss')\n",
    "plt.subplot(1, 2, 2), plt.plot(acc_train, 'b'), plt.plot(acc_val, 'r'), plt.legend(['train', 'val']), plt.title('Accuracy')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "### <font color='olive'>Results in Binary Sign Digits Dataset ($epochs = 10000$ and $\\rho = 0.001$)</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(14,5))\n",
    "plt.subplot(1, 2, 1), plt.plot(loss_train, 'b'), plt.plot(loss_val, 'r'), plt.legend(['train', 'val']), plt.title('Cross-entropy loss')\n",
    "plt.subplot(1, 2, 2), plt.plot(acc_train, 'b'), plt.plot(acc_val, 'r'), plt.legend(['train', 'val']), plt.title('Accuracy')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "### <font color='olive'>Exercises</font>\n",
    "\n",
    "* Train the network using other settings for:\n",
    "    * The number of iterations\n",
    "    * The learning step\n",
    "    * The number of neurons in the hidden layer\n",
    "   \n",
    "* You may find divergence issues for some settings\n",
    "    * Related to the use of the hyperbolic tangent function in the hidden layer (numerical issues)\n",
    "    * This is also why learning step was selected smaller for the hidden layer\n",
    "    * Optimized libraries rely on certain modifications to obtain more robust implementations\n",
    "    \n",
    "* Try to solve both problems using <a href=\"https://scikit-learn.org/stable/modules/generated/sklearn.neural_network.MLPClassifier.html\">scikit-learn implementation</a>\n",
    "    * You can also explore other activation functions\n",
    "    * You can also explore other solvers to speed up convergence\n",
    "    * You can also adjust the size of minibatches\n",
    "    * Take a look at the *early_stopping* parameter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### <font color='teal'> 2.4. Multi Layer Networks for Regression </font>\n",
    "\n",
    "* Deep Learning networks can be used to solve regression problems with the following common adjustments\n",
    "\n",
    "    * Linear activation for the output unit\n",
    "    \n",
    "    * Square loss: \n",
    "    $$l(y, \\hat y) = (y - \\hat y)^2, \\qquad \\text{where} \\qquad y, \\hat y \\in \\Re$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### <font color='teal'> 2.5. Activation Functions</font>\n",
    "\n",
    "You can refer to the <a href=\"http://d2l.ai/chapter_multilayer-perceptrons/mlp.html#activation-functions\">Dive into Deep Learning book</a> for a more detailed discussion on common actiation functions for the hidden units. \n",
    "\n",
    "We extract some information about the very important **ReLU** function\n",
    "\n",
    "> *The most popular choice, due to both simplicity of implementation and its good performance on a variety of predictive tasks, is the rectified linear unit (ReLU). ReLU provides a very simple nonlinear transformation. Given an element $x$, the function is defined as the maximum of that element and 0.*\n",
    "\n",
    "> *When the input is negative, the derivative of the ReLU function is 0, and when the input is positive, the derivative of the ReLU function is 1. When the input takes value precisely equal to 0, we say that the derivative is 0 when the input is 0.*\n",
    "\n",
    "> *The reason for using ReLU is that its derivatives are particularly well behaved: either they vanish or they just let the argument through. This makes optimization better behaved and it mitigated the well-documented problem of vanishing gradients that plagued previous versions of neural networks.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "x_array = np.linspace(-6,6,100)\n",
    "y_array = np.clip(x_array, 0, a_max=None)\n",
    "plt.plot(x_array, y_array)\n",
    "plt.title('ReLU activation function')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## <font color='teal'> 3. Implementing Deep Networks with PyTorch </font>\n",
    "\n",
    "* Pytorch is a Python library that provides different levels of abstraction for implementing deep neural networks\n",
    "\n",
    "* The main features of PyTorch are:\n",
    "    * Definition of numpy-like n-dimensional *tensors*. They can be stored in / moved to GPU for parallel execution of operations\n",
    "    * Automatic calculation of gradients, making *backward gradient calculation* transparent to the user\n",
    "    * Definition of common loss functions, NN layers of different types, optimization methods, data loaders, etc, simplifying NN implementation and training\n",
    "    * Provides different levels of abstraction, thus a good balance between flexibility and simplicity\n",
    "    \n",
    "* This notebook provides just a basic review of the main concepts necessary to train NNs with PyTorch taking materials from:\n",
    "    * <a href=\"https://pytorch.org/tutorials/beginner/pytorch_with_examples.html\">Learning PyTorch with Examples</a>, by Justin Johnson\n",
    "    * <a href=\"https://pytorch.org/tutorials/beginner/nn_tutorial.html\">What is *torch.nn* really?</a>, by Jeremy Howard\n",
    "    * <a href=\"https://www.kaggle.com/kanncaa1/pytorch-tutorial-for-deep-learning-lovers\">Pytorch Tutorial for Deep Learning Lovers</a>, by Kaggle user kanncaa1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### <font color='teal'> 3.1. Installation and PyTorch introduction</font>\n",
    "\n",
    "* PyTorch can be installed with or without GPU support\n",
    "    * If you have an Anaconda installation, you can install from the command line, using the <a href=\"https://pytorch.org/\">instructions of the project website</a>\n",
    "    \n",
    "* PyTorch is also preinstalled in Google Collab with free GPU access\n",
    "    * Follow RunTime -> Change runtime type, and select GPU for HW acceleration\n",
    "    \n",
    "* Please, refer to Pytorch [getting started](https://pytorch.org/get-started/locally/) tutorial for a quick introduction regarding tensor definition, GPU vs CPU storage of tensors, operations, and bridge to Numpy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### <font color='teal'> 3.2. Torch tensors (very) general overview</font>\n",
    "\n",
    "* We can create tensors with different construction methods provided by the library, either to create new tensors from scratch or from a Numpy array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "x = torch.rand((100,200))\n",
    "digitsX_flatten_tensor = torch.from_numpy(digitsX_flatten)\n",
    "\n",
    "print(x.type())\n",
    "print(digitsX_flatten_tensor.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* Tensors can be converted back to numpy arrays\n",
    "\n",
    "* Note that in this case, a tensor and its corresponding numpy array **will share memory**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* Operations and slicing use a syntax similar to numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "print('Size of tensor x:', x.size())\n",
    "print('Tranpose of vector has size', x.t().size()) #Transpose and compute size\n",
    "print('Extracting upper left matrix of size 3 x 3:', x[:3,:3])\n",
    "print(x.mm(x.t()).size())  #mm for matrix multiplications\n",
    "xpx = x.add(x)\n",
    "xpx2 = torch.add(x,x)\n",
    "print((xpx!=xpx2).sum())   #Since all are equal, count of different terms is zero"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* Adding underscore performs operations \"*in place*\", e.g., ```x.add_(y)```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* If a GPU is available, tensors can be moved to and from the GPU device\n",
    "\n",
    "* Operations on tensors stored in a GPU will be carried out using GPU resources and will typically be highly parallelized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "    x = x.to(device)\n",
    "    y = x.add(x)\n",
    "    y = y.to('cpu')\n",
    "else:\n",
    "    print('No GPU card is available')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### <font color='teal'> 3.3. Automatic gradient calculation </font>\n",
    "\n",
    "* PyTorch tensors have a property ```requires_grad```. When true, PyTorch automatic gradient calculation will be activated for that variable\n",
    "\n",
    "* In order to compute these derivatives numerically, PyTorch keeps track of all operations carried out on these variables, organizing them in a forward computation graph.\n",
    "\n",
    "* When executing the ```backward()``` method, derivatives will be calculated\n",
    "\n",
    "* However, this should only be activated when necessary, to save computation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "x.requires_grad = True\n",
    "y = (3 * torch.log(x)).sum()\n",
    "y.backward()\n",
    "print(x.grad[:2,:2])\n",
    "print(3/x[:2,:2])\n",
    "\n",
    "x.requires_grad = False\n",
    "x.grad.zero_()\n",
    "print('Automatic gradient calculation is deactivated, and gradients set to zero')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<font color='olive'>Exercise</font>\n",
    "\n",
    "* Initialize a tensor ```x``` with the upper right $5 \\times 10$ submatrix of flattened digits\n",
    "* Compute output vector ```y``` applying a function of your choice to ```x```\n",
    "* Compute scalar value ```z``` as the sum of all elements in ```y``` squared\n",
    "* Check that ```x.grad``` calculation is correct using the ```backward``` method\n",
    "* Try to run your cell multiple times to see if the calculation is still correct. If not, implement the necessary mnodifications so that you can run the cell multiple times, but the gradient does not change from run to run\n",
    "\n",
    "**Note:** The backward method can only be run on scalar variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### <font color='teal'> 3.4. Feed Forward Network using PyTorch </font>\n",
    "\n",
    "* In this section we will change our code for a neural network to use tensors instead of numpy arrays. We will work with the sign digits datasets.\n",
    "\n",
    "* We will introduce all concepts using a single layer perceptron (softmax regression), and then implement networks with additional hidden layers\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### <font color='olive'> 3.4.1. Using Automatic differentiation </font>\n",
    "\n",
    "* We start by loading the data, and converting to tensors.\n",
    "\n",
    "* As a first step, we refactor our code to use tensor operations\n",
    "\n",
    "* We do not need to pay too much attention to particular details regarding tensor operations, since these will not be necessary when moving to higher PyTorch abstraction levels\n",
    "\n",
    "* We do not need to implement gradient calculation. PyTorch will take care of that"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "dataset = 'digits'\n",
    "\n",
    "#Joint normalization of all data. For images [-.5, .5] scaling is frequent\n",
    "min_max_scaler = MinMaxScaler(feature_range=(-.5, .5))\n",
    "X = min_max_scaler.fit_transform(digitsX_flatten)\n",
    "\n",
    "#Generate train and validation data, shuffle\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, digitsY, test_size=0.2, random_state=42, shuffle=True)\n",
    "\n",
    "#Convert to Torch tensors\n",
    "X_train_torch = torch.from_numpy(X_train)\n",
    "X_val_torch = torch.from_numpy(X_val)\n",
    "y_train_torch = torch.from_numpy(y_train)\n",
    "y_val_torch = torch.from_numpy(y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# Define some useful functions\n",
    "def softmax(t):\n",
    "    \"\"\"Compute softmax values for each sets of scores in t\"\"\"\n",
    "    return t.exp() / t.exp().sum(-1).unsqueeze(-1)\n",
    "\n",
    "def model(w,b,x):\n",
    "    #Calcula la salida de la red\n",
    "    return softmax(x.mm(w) + b)\n",
    "    \n",
    "def accuracy(y, y_hat):\n",
    "    return (y.argmax(axis=-1) == y_hat.argmax(axis=-1)).float().mean()\n",
    "\n",
    "def nll(y, y_hat):\n",
    "    return -(y * y_hat.log()).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* Syntaxis is a bit different because input variables are tensors, not arrays\n",
    "\n",
    "* This time we did not need to implement the backward function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# Parameter initialization\n",
    "W = .1 * torch.randn(X_train_torch.size()[1], y_train_torch.size()[1])\n",
    "W.requires_grad_()\n",
    "b = torch.zeros(y_train_torch.size()[1], requires_grad=True)\n",
    "\n",
    "epochs = 500\n",
    "rho = .5\n",
    "\n",
    "loss_train = np.zeros(epochs)\n",
    "loss_val = np.zeros(epochs)\n",
    "acc_train = np.zeros(epochs)\n",
    "acc_val = np.zeros(epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# Network training\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    \n",
    "    print(f'Current epoch: {epoch + 1}  \\r', end=\"\")\n",
    "    \n",
    "    #Compute network output and cross-entropy loss\n",
    "    pred = model(W,b,X_train_torch)\n",
    "    loss = nll(y_train_torch, pred)\n",
    "    \n",
    "    #Compute gradients\n",
    "    loss.backward()\n",
    "    \n",
    "    #Deactivate gradient automatic updates\n",
    "    with torch.no_grad():\n",
    "        #Computing network performance after iteration\n",
    "        loss_train[epoch] = loss.item()\n",
    "        acc_train[epoch] = accuracy(y_train_torch, pred).item()\n",
    "        pred_val = model(W, b, X_val_torch)\n",
    "        loss_val[epoch] = nll(y_val_torch, pred_val).item()\n",
    "        acc_val[epoch] = accuracy(y_val_torch, pred_val).item()\n",
    "\n",
    "        #Weight update\n",
    "        W -= rho * W.grad\n",
    "        b -= rho * b.grad\n",
    "        #Reset gradients\n",
    "        W.grad.zero_()\n",
    "        b.grad.zero_()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "It is important to deactivate gradient updates after the network has been evaluated on training data, and gradients of the loss function have been computed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(14,5))\n",
    "plt.subplot(1, 2, 1), plt.plot(loss_train, 'b'), plt.plot(loss_val, 'r'), plt.legend(['train', 'val']), plt.title('Cross-entropy loss')\n",
    "plt.subplot(1, 2, 2), plt.plot(acc_train, 'b'), plt.plot(acc_val, 'r'), plt.legend(['train', 'val']), plt.title('Accuracy')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### <font color='olive'> 3.4.2. Using torch *nn* module </font>\n",
    "\n",
    "* PyTorch *nn* module provides many attributes and methods that make the implementation and training of Neural Networks simpler\n",
    "\n",
    "* ```nn.Module``` and ```nn.Parameter``` allow to implement a more concise training loop\n",
    "\n",
    "* ```nn.Module``` is a PyTorch class that will be used to encapsulate and design a specific neural network, thus, it is central to the implementation of deep neural nets using PyTorch\n",
    "\n",
    "* ```nn.Parameter``` allow the definition of trainable network parameters. In this way, we will simplify the implementation of the training loop.\n",
    "\n",
    "* All parameters defined with ```nn.Parameter``` will have ```requires_grad = True```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "\n",
    "class my_multiclass_net(nn.Module):\n",
    "    def __init__(self, nin, nout):\n",
    "        \"\"\"This method initializes the network parameters\n",
    "        Parameters nin and nout stand for the number of input parameters (features in X)\n",
    "        and output parameters (number of classes)\"\"\"\n",
    "        super().__init__()\n",
    "        self.W = nn.Parameter(.1 * torch.randn(nin, nout))\n",
    "        self.b = nn.Parameter(torch.zeros(nout))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return softmax(x.mm(self.W) + self.b)\n",
    "    \n",
    "    def softmax(t):\n",
    "        \"\"\"Compute softmax values for each sets of scores in t\"\"\"\n",
    "        return t.exp() / t.exp().sum(-1).unsqueeze(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "my_net = my_multiclass_net(X_train_torch.size()[1], y_train_torch.size()[1])\n",
    "\n",
    "epochs = 500\n",
    "rho = .5\n",
    "\n",
    "loss_train = np.zeros(epochs)\n",
    "loss_val = np.zeros(epochs)\n",
    "acc_train = np.zeros(epochs)\n",
    "acc_val = np.zeros(epochs)\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    \n",
    "    print(f'Current epoch: {epoch + 1}  \\r', end=\"\")\n",
    "    \n",
    "    #Compute network output and cross-entropy loss\n",
    "    pred = my_net(X_train_torch)\n",
    "    loss = nll(y_train_torch, pred)\n",
    "    \n",
    "    #Compute gradients\n",
    "    loss.backward()\n",
    "    \n",
    "    #Deactivate gradient automatic updates\n",
    "    with torch.no_grad():\n",
    "        #Computing network performance after iteration\n",
    "        loss_train[epoch] = loss.item()\n",
    "        acc_train[epoch] = accuracy(y_train_torch, pred).item()\n",
    "        pred_val = my_net(X_val_torch)\n",
    "        loss_val[epoch] = nll(y_val_torch, pred_val).item()\n",
    "        acc_val[epoch] = accuracy(y_val_torch, pred_val).item()\n",
    "\n",
    "        #Weight update\n",
    "        for p in my_net.parameters():\n",
    "            p -= p.grad * rho\n",
    "        #Reset gradients\n",
    "        my_net.zero_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(14,5))\n",
    "plt.subplot(1, 2, 1), plt.plot(loss_train, 'b'), plt.plot(loss_val, 'r'), plt.legend(['train', 'val']), plt.title('Cross-entropy loss')\n",
    "plt.subplot(1, 2, 2), plt.plot(acc_train, 'b'), plt.plot(acc_val, 'r'), plt.legend(['train', 'val']), plt.title('Accuracy')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* ```nn.Module``` comes with several kinds of pre-defined layers, thus making it even simpler to implement neural networks\n",
    "\n",
    "* We can also import the Cross Entropy Loss from ```nn.Module```. When doing so:\n",
    "    - We do not have to compute the softmax, since the ```nn.CrossEntropyLoss``` already does so\n",
    "    - ```nn.CrossEntropyLoss``` receives two input arguments, the first is the output of the network, and the second is the true label as a 1-D tensor (i.e., an array of integers, one-hot encoding should not be used)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "\n",
    "class my_multiclass_net(nn.Module):\n",
    "    def __init__(self, nin, nout):\n",
    "        \"\"\"Note that now, we do not even need to initialize network parameters ourselves\"\"\"\n",
    "        super().__init__()\n",
    "        self.lin = nn.Linear(nin, nout)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.lin(x)\n",
    "    \n",
    "loss_func = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "my_net = my_multiclass_net(X_train_torch.size()[1], y_train_torch.size()[1])\n",
    "\n",
    "epochs = 500\n",
    "rho = .1\n",
    "\n",
    "loss_train = np.zeros(epochs)\n",
    "loss_val = np.zeros(epochs)\n",
    "acc_train = np.zeros(epochs)\n",
    "acc_val = np.zeros(epochs)\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    \n",
    "    print(f'Current epoch: {epoch + 1}  \\r', end=\"\")\n",
    "    \n",
    "    #Compute network output and cross-entropy loss\n",
    "    pred = my_net(X_train_torch)\n",
    "    loss = loss_func(pred, y_train_torch.argmax(axis=-1))\n",
    "    \n",
    "    #Compute gradients\n",
    "    loss.backward()\n",
    "    \n",
    "    #Deactivate gradient automatic updates\n",
    "    with torch.no_grad():\n",
    "        #Computing network performance after iteration\n",
    "        loss_train[epoch] = loss.item()\n",
    "        acc_train[epoch] = accuracy(y_train_torch, pred).item()\n",
    "        pred_val = my_net(X_val_torch)\n",
    "        loss_val[epoch] = loss_func(pred_val, y_val_torch.argmax(axis=-1)).item()\n",
    "        acc_val[epoch] = accuracy(y_val_torch, pred_val).item()\n",
    "\n",
    "        #Weight update\n",
    "        for p in my_net.parameters():\n",
    "            p -= p.grad * rho\n",
    "        #Reset gradients\n",
    "        my_net.zero_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(14,5))\n",
    "plt.subplot(1, 2, 1), plt.plot(loss_train, 'b'), plt.plot(loss_val, 'r'), plt.legend(['train', 'val']), plt.title('Cross-entropy loss')\n",
    "plt.subplot(1, 2, 2), plt.plot(acc_train, 'b'), plt.plot(acc_val, 'r'), plt.legend(['train', 'val']), plt.title('Accuracy')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Note faster convergence is observed in this case. It is actually due to a more convenient initialization of the hidden layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### <font color='olive'> 3.4.3. Network Optimization </font>\n",
    "\n",
    "* We cover in this subsection two different aspects about network training using PyTorch:\n",
    "\n",
    "    + Using ```torch.optim``` allows an easier and more interpretable encoding of neural network training, and opens the door to more sophisticated training algorithms\n",
    "    \n",
    "    + Using minibatches can speed up network convergence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "    \n",
    "* ```torch.optim``` provides two convenient methods for neural network training:\n",
    "    - ```opt.step()``` updates all network parameters using current gradients\n",
    "    - ```opt.zero_grad()``` resets all network parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "from torch import optim\n",
    "\n",
    "my_net = my_multiclass_net(X_train_torch.size()[1], y_train_torch.size()[1])\n",
    "opt = optim.SGD(my_net.parameters(), lr=0.1)\n",
    "\n",
    "epochs = 500\n",
    "\n",
    "loss_train = np.zeros(epochs)\n",
    "loss_val = np.zeros(epochs)\n",
    "acc_train = np.zeros(epochs)\n",
    "acc_val = np.zeros(epochs)\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    \n",
    "    print(f'Current epoch: {epoch + 1}  \\r', end=\"\")\n",
    "    \n",
    "    #Compute network output and cross-entropy loss\n",
    "    pred = my_net(X_train_torch)\n",
    "    loss = loss_func(pred, y_train_torch.argmax(axis=-1))\n",
    "    \n",
    "    #Compute gradients\n",
    "    loss.backward()\n",
    "    \n",
    "    #Deactivate gradient automatic updates\n",
    "    with torch.no_grad():\n",
    "        #Computing network performance after iteration\n",
    "        loss_train[epoch] = loss.item()\n",
    "        acc_train[epoch] = accuracy(y_train_torch, pred).item()\n",
    "        pred_val = my_net(X_val_torch)\n",
    "        loss_val[epoch] = loss_func(pred_val, y_val_torch.argmax(axis=-1)).item()\n",
    "        acc_val[epoch] = accuracy(y_val_torch, pred_val).item()\n",
    "\n",
    "    opt.step()\n",
    "    opt.zero_grad()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Note network optimization is carried out outside ```torch.no_grad()``` but network evaluation (other than forward output calculation for the training patterns) still need to deactivate gradient updates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(14,5))\n",
    "plt.subplot(1, 2, 1), plt.plot(loss_train, 'b'), plt.plot(loss_val, 'r'), plt.legend(['train', 'val']), plt.title('Cross-entropy loss')\n",
    "plt.subplot(1, 2, 2), plt.plot(acc_train, 'b'), plt.plot(acc_val, 'r'), plt.legend(['train', 'val']), plt.title('Accuracy')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "### <font color='olive'> Exercise </font>\n",
    "\n",
    "Implement network training with other optimization methods. You can refer to the <a href=\"https://pytorch.org/docs/stable/optim.html\">official documentation</a> and select a couple of methods. You can also try to implement adaptive learning rates using ```torch.optim.lr_scheduler```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "    \n",
    "* Each epoch of the previous implementation of network training was actually implementing Gradient Descent\n",
    "\n",
    "* In SGD only a *minibatch* of training patterns are used at every iteration\n",
    "\n",
    "* In each epoch we iterate over all training patterns sequentially selecting non-overlapping *minibatches*\n",
    "\n",
    "* Overall, convergence is usually faster than when using Gradient Descent\n",
    "\n",
    "* Torch provides methods that simplify the implementation of this strategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "train_ds = TensorDataset(X_train_torch, y_train_torch)\n",
    "train_dl = DataLoader(train_ds, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "from torch import optim\n",
    "\n",
    "my_net = my_multiclass_net(X_train_torch.size()[1], y_train_torch.size()[1])\n",
    "opt = optim.SGD(my_net.parameters(), lr=0.1)\n",
    "\n",
    "epochs = 200\n",
    "\n",
    "loss_train = np.zeros(epochs)\n",
    "loss_val = np.zeros(epochs)\n",
    "acc_train = np.zeros(epochs)\n",
    "acc_val = np.zeros(epochs)\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    \n",
    "    print(f'Current epoch: {epoch + 1}  \\r', end=\"\")\n",
    "        \n",
    "    for xb, yb in train_dl:\n",
    "        \n",
    "        #Compute network output and cross-entropy loss for current minibatch\n",
    "        pred = my_net(xb)\n",
    "        loss = loss_func(pred, yb.argmax(axis=-1))\n",
    "    \n",
    "        #Compute gradients and optimize parameters\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        opt.zero_grad()\n",
    "    \n",
    "    #At the end of each epoch, evaluate overall network performance\n",
    "    with torch.no_grad():\n",
    "        #Computing network performance after iteration\n",
    "        pred = my_net(X_train_torch)\n",
    "        loss_train[epoch] = loss_func(pred, y_train_torch.argmax(axis=-1)).item()\n",
    "        acc_train[epoch] = accuracy(y_train_torch, pred).item()\n",
    "        pred_val = my_net(X_val_torch)\n",
    "        loss_val[epoch] = loss_func(pred_val, y_val_torch.argmax(axis=-1)).item()\n",
    "        acc_val[epoch] = accuracy(y_val_torch, pred_val).item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(14,5))\n",
    "plt.subplot(1, 2, 1), plt.plot(loss_train, 'b'), plt.plot(loss_val, 'r'), plt.legend(['train', 'val']), plt.title('Cross-entropy loss')\n",
    "plt.subplot(1, 2, 2), plt.plot(acc_train, 'b'), plt.plot(acc_val, 'r'), plt.legend(['train', 'val']), plt.title('Accuracy')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### <font color='olive'> 3.4.4. Multi Layer networks using ```nn.Sequential``` </font>\n",
    "\n",
    "* PyTorch simplifies considerably the implementation of neural network training, since we do not need to implement derivatives ourselves\n",
    "\n",
    "* We can also make a simpler implementation of multilayer networks using ```nn.Sequential``` function\n",
    "\n",
    "* It returns directly a network with the requested topology, including parameters **and forward evaluation method**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "my_net = nn.Sequential(\n",
    "    nn.Linear(X_train_torch.size()[1], 200),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(200,50),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(50,20),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(20,y_train_torch.size()[1])\n",
    ")\n",
    "\n",
    "opt = optim.SGD(my_net.parameters(), lr=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "epochs = 200\n",
    "\n",
    "loss_train = np.zeros(epochs)\n",
    "loss_val = np.zeros(epochs)\n",
    "acc_train = np.zeros(epochs)\n",
    "acc_val = np.zeros(epochs)\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    \n",
    "    print(f'Current epoch: {epoch + 1}  \\r', end=\"\")\n",
    "        \n",
    "    for xb, yb in train_dl:\n",
    "        \n",
    "        #Compute network output and cross-entropy loss for current minibatch\n",
    "        pred = my_net(xb)\n",
    "        loss = loss_func(pred, yb.argmax(axis=-1))\n",
    "    \n",
    "        #Compute gradients and optimize parameters\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        opt.zero_grad()\n",
    "    \n",
    "    #At the end of each epoch, evaluate overall network performance\n",
    "    with torch.no_grad():\n",
    "        #Computing network performance after iteration\n",
    "        pred = my_net(X_train_torch)\n",
    "        loss_train[epoch] = loss_func(pred, y_train_torch.argmax(axis=-1)).item()\n",
    "        acc_train[epoch] = accuracy(y_train_torch, pred).item()\n",
    "        pred_val = my_net(X_val_torch)\n",
    "        loss_val[epoch] = loss_func(pred_val, y_val_torch.argmax(axis=-1)).item()\n",
    "        acc_val[epoch] = accuracy(y_val_torch, pred_val).item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(14,5))\n",
    "plt.subplot(1, 2, 1), plt.plot(loss_train, 'b'), plt.plot(loss_val, 'r'), plt.legend(['train', 'val']), plt.title('Cross-entropy loss')\n",
    "plt.subplot(1, 2, 2), plt.plot(acc_train, 'b'), plt.plot(acc_val, 'r'), plt.legend(['train', 'val']), plt.title('Accuracy')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "print('Validation accuracy with this net:', acc_val[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### <font color='teal'> 3.5. Generalization</font>\n",
    "\n",
    "* For complex network topologies (i.e., many parameters), network training can incur in over-fitting issues\n",
    "\n",
    "* Some common strategies to avoid this are:\n",
    "\n",
    "    - Early stopping\n",
    "    - Dropout regularization\n",
    "    \n",
    "<center><a href=\"https://medium.com/analytics-vidhya/a-simple-introduction-to-dropout-regularization-with-code-5279489dda1e\"><img src=\"figures/dropout.png\" width=\"450\"/>Image Source</a></center>\n",
    "\n",
    "* Data augmentation can also be used to avoid overfitting, as well as to achieve improved accuracy by providing the network some a priori expert knowledge\n",
    "    - E.g., if image rotations and scalings do not affect the correct class, we could enlarge the dataset by creating artificial images with these transformations\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### <font color='teal'> 3.6. Convolutional Networks for Image Processing </font>\n",
    "\n",
    "* PyTorch implements other layers that are better suited for different applications\n",
    "\n",
    "* In image processing, we normally recur to Convolutional Neural Networks, since they are able to capture the true spatial information of the image\n",
    "\n",
    "<center><a href=\"https://pytorch.org/tutorials/beginner/blitz/neural_networks_tutorial.html\"><img src=\"figures/CNN.png\" width=\"800\"/>Image Source</a></center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "dataset = 'digits'\n",
    "\n",
    "#Generate train and validation data, shuffle\n",
    "X_train, X_val, y_train, y_val = train_test_split(digitsX[:,np.newaxis,:,:], digitsY, test_size=0.2, random_state=42, shuffle=True)\n",
    "\n",
    "#Convert to Torch tensors\n",
    "X_train_torch = torch.from_numpy(X_train)\n",
    "X_val_torch = torch.from_numpy(X_val)\n",
    "y_train_torch = torch.from_numpy(y_train)\n",
    "y_val_torch = torch.from_numpy(y_val)\n",
    "\n",
    "train_ds = TensorDataset(X_train_torch, y_train_torch)\n",
    "train_dl = DataLoader(train_ds, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "class Lambda(nn.Module):\n",
    "    def __init__(self, func):\n",
    "        super().__init__()\n",
    "        self.func = func\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.func(x)\n",
    "\n",
    "my_net = nn.Sequential(\n",
    "    nn.Conv2d(1, 16, kernel_size=3, stride=2, padding=1),\n",
    "    nn.ReLU(),\n",
    "    nn.Conv2d(16, 16, kernel_size=3, stride=2, padding=1),\n",
    "    nn.ReLU(),\n",
    "    nn.Conv2d(16, 10, kernel_size=3, stride=2, padding=1),\n",
    "    nn.ReLU(),\n",
    "    nn.AvgPool2d(4),\n",
    "    Lambda(lambda x: x.view(x.size(0), -1)),\n",
    ")\n",
    "\n",
    "opt = optim.SGD(my_net.parameters(), lr=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "epochs = 2500\n",
    "\n",
    "loss_train = np.zeros(epochs)\n",
    "loss_val = np.zeros(epochs)\n",
    "acc_train = np.zeros(epochs)\n",
    "acc_val = np.zeros(epochs)\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    \n",
    "    print(f'Número de épocas: {epoch + 1}\\r', end=\"\")\n",
    "        \n",
    "    for xb, yb in train_dl:\n",
    "        \n",
    "        #Compute network output and cross-entropy loss for current minibatch\n",
    "        pred = my_net(xb)\n",
    "        loss = loss_func(pred, yb.argmax(axis=-1))\n",
    "    \n",
    "        #Compute gradients and optimize parameters\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        opt.zero_grad()\n",
    "    \n",
    "    #At the end of each epoch, evaluate overall network performance\n",
    "    with torch.no_grad():\n",
    "        # Computing network performance after iteration\n",
    "        pred = my_net(X_train_torch)\n",
    "        loss_train[epoch] = loss_func(pred, y_train_torch.argmax(axis=-1)).item()\n",
    "        acc_train[epoch] = accuracy(y_train_torch, pred).item()\n",
    "        pred_val = my_net(X_val_torch)\n",
    "        loss_val[epoch] = loss_func(pred_val, y_val_torch.argmax(axis=-1)).item()\n",
    "        acc_val[epoch] = accuracy(y_val_torch, pred_val).item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(14,5))\n",
    "plt.subplot(1, 2, 1), plt.plot(loss_train, 'b'), plt.plot(loss_val, 'r'), plt.legend(['train', 'val']), plt.title('Cross-entropy loss')\n",
    "plt.subplot(1, 2, 2), plt.plot(acc_train, 'b'), plt.plot(acc_val, 'r'), plt.legend(['train', 'val']), plt.title('Accuracy')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
