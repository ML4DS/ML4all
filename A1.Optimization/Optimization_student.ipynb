{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimización  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    Author: Jesús Cid-Sueiro\n",
    "            Jerónimo Arenas-García\n",
    "    \n",
    "    Versión: 0.1 (2019/09/13)\n",
    "             0.2 (2019/10/02): Solutions added"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise: compute the minimum of a real-valued function\n",
    "\n",
    "The goal of this exercise is to implement and test optimization algorithms for the minimization of a given function. Gradient descent and Newton's method will be explored.\n",
    "\n",
    "Our goal it so find the minimizer of the real-valued function\n",
    "$$\n",
    "f(w) = - w exp(-w)\n",
    "$$\n",
    "but the whole code will be easily modified to try with other alternative functions.\n",
    "\n",
    "You will need to import some libraries (at least, `numpy` and `matplotlib`). Insert below all the imports needed along the whole notebook. Remind that numpy is usually abbreviated as np and `matplotlib.pyplot` is usually abbreviated as `plt`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# <SOL>\n",
    "# </SOL>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 1: The function and its derivatives.\n",
    "\n",
    "**Question 1.1**: Implement the following three methods:\n",
    "\n",
    "* Method **`f`**: given $w$, it returns the value of function $f(w)$.\n",
    "* Method **`df`**: given $w$, it returns the derivative of $f$ at $w$\n",
    "* Medhod **`d2f`**: given $w$, it returns the second derivative of $f$ at $w$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Funcion f\n",
    "# <SOL>\n",
    "# </SOL>\n",
    "\n",
    "# First derivative\n",
    "# <SOL>\n",
    "# </SOL>\n",
    "\n",
    "# Second derivative\n",
    "# <SOL>\n",
    "# </SOL>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 2: Gradient descent.\n",
    "\n",
    "**Question 2.1**: Implement a method **`gd`** that, given `w` and a learning rate parameter `rho` applies a single iteration of the gradien descent algorithm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# <SOL>\n",
    "# </SOL>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 2.2**: Apply the gradient descent to optimize the given function. To do so, start with an initial value $w=0$ and iterate $20$ times. Save two lists:\n",
    "\n",
    "* A list of succesive values of $w_n$\n",
    "* A list of succesive values of the function $f(w_n)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# <SOL>\n",
    "# </SOL>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 2.3**: Plot, in a single figure:\n",
    "\n",
    "* The given function, for values ranging from 0 to 20.\n",
    "* The sequence of points $(w_n, f(w_n))$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# <SOL>\n",
    "# </SOL>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can check the effect of modifying the value of the learning rate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 2: Newton's method.\n",
    "\n",
    "**Question 3.1**: Implement a method **`newton`** that, given `w` and a learning rate parameter `rho` applies a single iteration of the Newton's method\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# <SOL>\n",
    "# </SOL>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 3**: Apply the Newton's method to optimize the given function. To do so, start with an initial value $w=0$ and iterate $20$ times. Save two lists:\n",
    "\n",
    "* A list of succesive values of $w_n$\n",
    "* A list of succesive values of the function $f(w_n)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# <SOL>\n",
    "# </SOL>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 4**: Plot, in a single figure:\n",
    "\n",
    "* The given function, for values ranging from 0 to 20.\n",
    "* The sequence of points $(w_n, f(w_n))$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# <SOL>\n",
    "# </SOL>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can check the effect of modifying the value of the learning rate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 3: Optimize other cost functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you are ready to explore these optimization algorithms with other more sophisticated functions. Try with them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
