{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Gaussian Process regression**\n",
    "\n",
    "    Authors: Miguel Lázaro Gredilla\n",
    "             Jerónimo Arenas García (jarenas@tsc.uc3m.es)\n",
    "             Jesús Cid Sueiro"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    Notebook version: 1.0 (Nov, 07, 2017)\n",
    "\n",
    "    Changes: v.1.0 - First version. Python version\n",
    "             v.1.1 - Extraction from a longer version ingluding Bayesian regresssion.\n",
    "                     Python 3 compatibility\n",
    "    \n",
    "    Pending changes: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import some libraries that will be necessary for working with data and displaying plots\n",
    "\n",
    "# To visualize plots in the notebook\n",
    "%matplotlib inline \n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import scipy.io       # To read matlab files\n",
    "from scipy import spatial\n",
    "import pylab\n",
    "pylab.rcParams['figure.figsize'] = 8, 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Introduction\n",
    "\n",
    "In this exercise the student will review several key concepts of Bayesian regression and Gaussian processes.\n",
    "\n",
    "For the purpose of this exercise, the regression model is\n",
    "\n",
    "$${s}({\\bf x}) = f({\\bf x}) + \\varepsilon$$\n",
    "\n",
    "where ${s}({\\bf x})$ is the output corresponding to input ${\\bf x}$, $f({\\bf x})$ is the unobservable latent function, and $\\varepsilon$ is white zero-mean Gaussian noise, i.e., $\\varepsilon \\sim {\\cal N}(0,\\sigma_\\varepsilon^2)$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Practical considerations\n",
    "\n",
    "   - Though sometimes unavoidable, it is recommended not to use explicit matrix inversion whenever possible. For instance, if an operation like ${\\mathbf A}^{-1} {\\mathbf b}$ must be performed, it is preferable to code it using python $\\mbox{numpy.linalg.lstsq}$ function (see http://docs.scipy.org/doc/numpy/reference/generated/numpy.linalg.lstsq.html), which provides the LS solution to the overdetermined system ${\\mathbf A} {\\mathbf w} = {\\mathbf b}$.\n",
    "   \n",
    "   - Sometimes, the computation of $\\log|{\\mathbf A}|$ (where ${\\mathbf A}$ is a positive definite matrix) can overflow available precision, producing incorrect results. A numerically more stable alternative, providing the same result is $2\\sum_i \\log([{\\mathbf L}]_{ii})$, where $\\mathbf L$ is the Cholesky decomposition of $\\mathbf A$ (i.e., ${\\mathbf A} = {\\mathbf L}^\\top {\\mathbf L}$), and $[{\\mathbf L}]_{ii}$ is the $i$th element of the diagonal of ${\\mathbf L}$.\n",
    "   \n",
    "   - Non-degenerate covariance matrices, such as the ones in this exercise, are always positive definite. It may happen, as a consequence of chained rounding errors, that a matrix which was mathematically expected to be positive definite, turns out not to be so. This implies its Cholesky decomposition will not be available. A quick way to palliate this problem is by adding a small number (such as $10^{-6}$) to the diagonal of such matrix."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reproducibility of computations\n",
    "\n",
    "To guarantee the exact reproducibility of the experiments, it may be useful to start your code initializing the seed of the random numbers generator, so that you can compare your results with the ones given in this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.random.seed(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. The stocks dataset.\n",
    "\n",
    "\n",
    "Load and properly normalize data corresponding to the evolution of the stocks of 10 airline companies. This data set is an adaptation of the Stock dataset from http://www.dcc.fc.up.pt/~ltorgo/Regression/DataSets.html, which in turn was taken from the StatLib Repository, http://lib.stat.cmu.edu/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load data from matlab file DatosLabReg.mat\n",
    "# matvar = <FILL IN>\n",
    "matvar = scipy.io.loadmat('DatosLabReg.mat')\n",
    "\n",
    "# Take main variables, Xtrain, Xtest, Ytrain, Ytest from the corresponding dictionary entries in matvar:\n",
    "# <SOL>\n",
    "Xtrain = matvar['Xtrain']\n",
    "Xtest = matvar['Xtest']\n",
    "Ytrain = matvar['Ytrain']\n",
    "Ytest = matvar['Ytest']\n",
    "# </SOL>\n",
    "\n",
    "# Data normalization\n",
    "# <SOL>\n",
    "mean_x = np.mean(Xtrain,axis=0)\n",
    "std_x = np.std(Xtrain,axis=0)\n",
    "Xtrain = (Xtrain - mean_x) / std_x\n",
    "Xtest = (Xtest - mean_x) / std_x\n",
    "# </SOL>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After running this code, you will have inside matrix `Xtrain` the evolution of (normalized) price for 9 airlines, whereas vector `Ytrain` will contain a single column with the price evolution of the tenth airline. The objective of the regression task is to estimate the price of the tenth airline from the prices of the other nine."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# 3. Non-linear regression with Gaussian Processes\n",
    "\n",
    "## 3.1. Multidimensional regression\n",
    "\n",
    "Rather than using a parametric form for $f({\\mathbf x})$, in this section we will use directly the values of the latent function that we will model with a Gaussian process\n",
    "\n",
    "$$f({\\mathbf x}) \\sim {\\cal GP}\\left(0,k_f({\\mathbf x}_i,{\\mathbf x}_j)\\right),$$\n",
    "\n",
    "where we are assuming a zero mean, and where we will use the Ornstein-Uhlenbeck covariance function, which is defined as:\n",
    "\n",
    "$$k_f({\\mathbf x}_i,{\\mathbf x}_j) = \\sigma_0^2 \\exp \\left( -\\frac{1}{l}\\|{\\mathbf x}_i-{\\mathbf x}_j\\|\\right)$$\n",
    "\n",
    "First, we will use the following gross estimation for the hyperparameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sigma_0 = np.std(Ytrain)\n",
    "sigma_eps = sigma_0 / np.sqrt(10)\n",
    "l = 8\n",
    "\n",
    "print('sigma_0 = {0}'.format(sigma_0))\n",
    "print('sigma_eps = {0}'.format(sigma_eps))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we studied in a previous session, the joint distribution of the target values in the training set, ${\\mathbf s}$, and the latent values corresponding to the test points, ${\\mathbf f}^\\ast$, is given by\n",
    "\n",
    "$$\\left[\\begin{array}{c}{\\bf s}\\\\{\\bf f}^\\ast\\end{array}\\right]~\\sim~{\\cal N}\\left({\\bf 0},\\left[\\begin{array}{cc}{\\bf K} + \\sigma_\\varepsilon^2 {\\bf I}& {\\bf K}_\\ast^\\top \\\\ {\\bf K}_\\ast & {\\bf K}_{\\ast\\ast} \\end{array}\\right]\\right)$$\n",
    "\n",
    "Using this model, obtain the posterior of ${\\mathbf s}^\\ast$ given ${\\mathbf s}$. In particular, calculate the <i>a posteriori</i> predictive mean and standard deviations, ${\\mathbb E}\\left\\{s({\\bf x}^\\ast)\\mid{\\bf s}\\right\\}$ and $\\sqrt{{\\mathbb V}\\left\\{s({\\bf x}^\\ast)\\mid{\\bf s}\\right\\}}$ for each test sample ${\\bf x}^\\ast$.\n",
    "\n",
    "Obtain the MSE and NLPD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Compute Kernel matrices.\n",
    "# You may find spatial.distance.cdist() usefull to compute the euclidean distances required by Gaussian kernels.\n",
    "# <SOL>\n",
    "# Compute appropriate distances\n",
    "dist = spatial.distance.cdist(Xtrain, Xtrain, 'euclidean')\n",
    "dist_ss = spatial.distance.cdist(Xtest, Xtest, 'euclidean')\n",
    "dist_s = spatial.distance.cdist(Xtest, Xtrain, 'euclidean')\n",
    "\n",
    "# Compute Kernel matrices\n",
    "K = (sigma_0**2)*np.exp(-dist/l)\n",
    "K_ss = (sigma_0**2)*np.exp(-dist_ss/l)\n",
    "K_s = (sigma_0**2)*np.exp(-dist_s/l)\n",
    "# </SOL>\n",
    "\n",
    "# Compute predictive mean\n",
    "# m_y = <FILL IN>\n",
    "m_y = K_s.dot(np.linalg.inv(K + sigma_eps**2 * np.eye(K.shape[0]))).dot((Ytrain))\n",
    "\n",
    "# Compute predictive variance\n",
    "# v_y = <FILL IN>\n",
    "v_y = np.diagonal(K_ss - K_s.dot(np.linalg.inv(K + sigma_eps**2 * np.eye(K.shape[0]))).dot(K_s.T)) + sigma_eps**2\n",
    "\n",
    "# Compute MSE\n",
    "# MSE = <FILL IN>\n",
    "MSE = np.mean((m_y - Ytest)**2)\n",
    "\n",
    "# Compute NLPD\n",
    "# NLPD = <FILL IN>\n",
    "NLPD = 0.5 * np.mean(((Ytest - m_y)**2)/(np.matrix(v_y).T) + 0.5*np.log(2*np.pi*np.matrix(v_y).T))\n",
    "\n",
    "print(m_y.T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should obtain the following results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print('MSE = {0}'.format(MSE))\n",
    "print('NLPD = {0}'.format(NLPD))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2. Unidimensional regression\n",
    "\n",
    "Use now only the first company to compute the non-linear regression. Obtain the posterior\n",
    "distribution of $f({\\mathbf x}^\\ast)$ evaluated at the test values ${\\mathbf x}^\\ast$, i.e, $p(f({\\mathbf x}^\\ast)\\mid {\\mathbf s})$.\n",
    "\n",
    "This distribution is Gaussian, with mean ${\\mathbb E}\\left\\{f({\\bf x}^\\ast)\\mid{\\bf s}\\right\\}$ and a covariance matrix $\\text{Cov}\\left[f({\\bf x}^\\ast)\\mid{\\bf s}\\right]$. Sample 50 random vectors from the distribution and plot them vs. the values $x^\\ast$, together with the test samples.\n",
    "\n",
    "The Bayesian model does not provide a single function, but a pdf over functions, from which we extracted 50 possible functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# <SOL>\n",
    "X_1d = np.matrix(Xtrain[:,0]).T\n",
    "Xt_1d = np.matrix(Xtest[:,0]).T\n",
    "Xt_1d = np.sort(Xt_1d,axis=0) #We sort the vector for representational purposes\n",
    "\n",
    "dist = spatial.distance.cdist(X_1d,X_1d,'euclidean')\n",
    "dist_ss = spatial.distance.cdist(Xt_1d,Xt_1d,'euclidean')\n",
    "dist_s = spatial.distance.cdist(Xt_1d,X_1d,'euclidean')\n",
    "\n",
    "K = (sigma_0**2)*np.exp(-dist/l)\n",
    "K_ss = (sigma_0**2)*np.exp(-dist_ss/l)\n",
    "K_s = (sigma_0**2)*np.exp(-dist_s/l)\n",
    "                        \n",
    "m_y = K_s.dot(np.linalg.inv(K + sigma_eps**2 * np.eye(K.shape[0]))).dot((Ytrain))\n",
    "v_f = K_ss - K_s.dot(np.linalg.inv(K + sigma_eps**2 * np.eye(K.shape[0]))).dot(K_s.T)\n",
    "\n",
    "L = np.linalg.cholesky(v_f+1e-10*np.eye(v_f.shape[0]))\n",
    "\n",
    "for iter in range(50):\n",
    "    f_ast = L.dot(np.random.randn(len(Xt_1d),1)) + m_y\n",
    "    plt.plot(np.array(Xt_1d)[:,0],f_ast[:,0],'c:');\n",
    "\n",
    "# Plot as well the test points\n",
    "plt.plot(np.array(Xtest[:,0]),Ytest[:,0],'r.',markersize=12);\n",
    "plt.plot(np.array(Xt_1d)[:,0],m_y[:,0],'b-',linewidth=3,label='Predictive mean');\n",
    "    \n",
    "plt.legend(loc='best')\n",
    "plt.xlabel('x',fontsize=18);\n",
    "plt.ylabel('s',fontsize=18);\n",
    "# </SOL>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot again the previous figure, this time including in your plot the confidence interval delimited by two standard deviations of the prediction. You can observe how $95.45\\%$ of observed data fall within the designated area."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# <SOL>\n",
    "X_1d = np.matrix(Xtrain[:,0]).T\n",
    "Xt_1d = np.matrix(Xtest[:,0]).T\n",
    "idx = np.argsort(Xt_1d,axis=0) # We sort the vector for representational purposes\n",
    "Xt_1d = np.sort(Xt_1d,axis=0)\n",
    "idx = np.array(idx).flatten().T\n",
    "Ytest = Ytest[idx]\n",
    "\n",
    "dist = spatial.distance.cdist(X_1d,X_1d,'euclidean')\n",
    "dist_ss = spatial.distance.cdist(Xt_1d,Xt_1d,'euclidean')\n",
    "dist_s = spatial.distance.cdist(Xt_1d,X_1d,'euclidean')\n",
    "\n",
    "K = (sigma_0**2)*np.exp(-dist/l)\n",
    "K_ss = (sigma_0**2)*np.exp(-dist_ss/l)\n",
    "K_s = (sigma_0**2)*np.exp(-dist_s/l)\n",
    "                        \n",
    "m_y = K_s.dot(np.linalg.inv(K + sigma_eps**2 * np.eye(K.shape[0]))).dot((Ytrain))\n",
    "v_f = K_ss - K_s.dot(np.linalg.inv(K + sigma_eps**2 * np.eye(K.shape[0]))).dot(K_s.T)\n",
    "v_f_diag = np.diagonal(v_f)\n",
    "\n",
    "L = np.linalg.cholesky(v_f+1e-10*np.eye(v_f.shape[0]))\n",
    "\n",
    "for iter in range(50):\n",
    "    f_ast = L.dot(np.random.randn(len(Xt_1d),1)) + m_y\n",
    "    plt.plot(np.array(Xt_1d)[:,0],f_ast[:,0],'c:');\n",
    "\n",
    "# Plot as well the test points\n",
    "plt.plot(np.array(Xtest[:,0]),Ytest[:,0],'r.',markersize=12);\n",
    "plt.plot(np.array(Xt_1d)[:,0],m_y[:,0],'b-',linewidth=3,label='Predictive mean');\n",
    "plt.plot(np.array(Xt_1d)[:,0],m_y[:,0]+2*v_f_diag,'m--',label='Predictive mean of f $\\pm$ 2std',linewidth=3);\n",
    "plt.plot(np.array(Xt_1d)[:,0],m_y[:,0]-2*v_f_diag,'m--',linewidth=3);\n",
    "\n",
    "#Plot now the posterior mean and posterior mean \\pm 2 std for s (i.e., adding the noise variance)\n",
    "plt.plot(np.array(Xt_1d)[:,0],m_y[:,0]+2*v_f_diag+2*sigma_eps,'m:',label='Predictive mean of s $\\pm$ 2std',linewidth=3);\n",
    "plt.plot(np.array(Xt_1d)[:,0],m_y[:,0]-2*v_f_diag-2*sigma_eps,'m:',linewidth=3);\n",
    "\n",
    "plt.legend(loc='best')\n",
    "plt.xlabel('x',fontsize=18);\n",
    "plt.ylabel('s',fontsize=18);\n",
    "# </SOL>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute now the MSE and NLPD of the model. The correct results are given below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# <SOL>\n",
    "MSE = np.mean((m_y - Ytest)**2)\n",
    "v_y = np.diagonal(v_f) + sigma_eps**2\n",
    "NLPD = 0.5 * np.mean(((Ytest - m_y)**2)/(np.matrix(v_y).T) + 0.5*np.log(2*np.pi*np.matrix(v_y).T))\n",
    "# </SOL>\n",
    "\n",
    "print('MSE = {0}'.format(MSE))\n",
    "print('NLPD = {0}'.format(NLPD))"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:mypy36]",
   "language": "python",
   "name": "conda-env-mypy36-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
