{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ada593d9",
   "metadata": {
    "id": "ada593d9",
    "jp-MarkdownHeadingCollapsed": true,
    "tags": [],
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\">\n",
    "    <ul class=\"toc-item\">\n",
    "        <li><span><a href=\"#-MNIST-Hand-Digit-Classification-\" data-toc-modified-id=\"-MNIST-Hand-Digit-Classification--1\"><font color=\"teal\"> MNIST Hand Digit Classification </font></a></span></li>\n",
    "        <li><span><a href=\"#-Part-1.-Scikit-learn-Methods-\" data-toc-modified-id=\"-Part-1.-Scikit-learn-Methods--2\"><font color=\"teal\"> Part 1. Scikit-learn Methods </font></a></span>\n",
    "            <ul class=\"toc-item\">\n",
    "                <li><span><a href=\"#-1.-Data-Preparation-\" data-toc-modified-id=\"-1.-Data-Preparation--2.1\"><font color=\"teal\"> 1. Data Preparation </font></a></span><ul class=\"toc-item\"><li><span><a href=\"#1.1.-Data-load\" data-toc-modified-id=\"1.1.-Data-load-2.1.1\">1.1. Data load</a></span></li><li><span><a href=\"#1.2.-Data-partitioning\" data-toc-modified-id=\"1.2.-Data-partitioning-2.1.2\">1.2. Data partitioning</a></span></li><li><span><a href=\"#1.3.-Data-normalization\" data-toc-modified-id=\"1.3.-Data-normalization-2.1.3\">1.3. Data normalization</a></span></li></ul></li>\n",
    "                <li><span><a href=\"#-2.-Binary-classification-\" data-toc-modified-id=\"-2.-Binary-classification--2.2\"><font color=\"teal\"> 2. Binary classification </font></a></span><ul class=\"toc-item\"><li><span><a href=\"#2.1.-Two-dimensional-representation\" data-toc-modified-id=\"2.1.-Two-dimensional-representation-2.2.1\">2.1. Two dimensional representation</a></span><ul class=\"toc-item\"><li><span><a href=\"#2.1.1.-Principal-Component-Analysis-(PCA)\" data-toc-modified-id=\"2.1.1.-Principal-Component-Analysis-(PCA)-2.2.1.1\">2.1.1. Principal Component Analysis (PCA)</a></span></li><li><span><a href=\"#2.1.2.-Linear-Classification-with-Logistic-Regression\" data-toc-modified-id=\"2.1.2.-Linear-Classification-with-Logistic-Regression-2.2.1.2\">2.1.2. Linear Classification with Logistic Regression</a></span></li><li><span><a href=\"#2.1.3.-Polynomial-Logistic-Regression\" data-toc-modified-id=\"2.1.3.-Polynomial-Logistic-Regression-2.2.1.3\">2.1.3. Polynomial Logistic Regression</a></span></li><li><span><a href=\"#2.1.4.-Multi-Layer-Perceptron\" data-toc-modified-id=\"2.1.4.-Multi-Layer-Perceptron-2.2.1.4\">2.1.4. Multi-Layer Perceptron</a></span></li></ul></li><li><span><a href=\"#2.2.-Classification-with-all-input-features\" data-toc-modified-id=\"2.2.-Classification-with-all-input-features-2.2.2\">2.2. Classification with all input features</a></span><ul class=\"toc-item\"><li><span><a href=\"#2.2.1.-Logistic-Regression\" data-toc-modified-id=\"2.2.1.-Logistic-Regression-2.2.2.1\">2.2.1. Logistic Regression</a></span></li><li><span><a href=\"#2.2.2.-K-Nearest-Neighbors\" data-toc-modified-id=\"2.2.2.-K-Nearest-Neighbors-2.2.2.2\">2.2.2. K Nearest Neighbors</a></span></li><li><span><a href=\"#2.2.3.-Multi-Layer-Perceptron\" data-toc-modified-id=\"2.2.3.-Multi-Layer-Perceptron-2.2.2.3\">2.2.3. Multi-Layer Perceptron</a></span></li></ul></li></ul></li>\n",
    "                <li><span><a href=\"#-3.-Multi-Class-Classification-\" data-toc-modified-id=\"-3.-Multi-Class-Classification--2.3\"><font color=\"teal\"> 3. Multi Class Classification </font></a></span><ul class=\"toc-item\"><li><span><a href=\"#3.1.-Principal-Component-Analysis-(PCA)\" data-toc-modified-id=\"3.1.-Principal-Component-Analysis-(PCA)-2.3.1\">3.1. Principal Component Analysis (PCA)</a></span></li><li><span><a href=\"#3.2.-Nearest-Neighbor-Method\" data-toc-modified-id=\"3.2.-Nearest-Neighbor-Method-2.3.2\">3.2. Nearest Neighbor Method</a></span></li><li><span><a href=\"#3.3.-Multi-Layer-Perceptron\" data-toc-modified-id=\"3.3.-Multi-Layer-Perceptron-2.3.3\">3.3. Multi-Layer Perceptron</a></span></li></ul></li></ul></li>\n",
    "        <li><span><a href=\"#-Part-2.-Implementing-Deep-Networks-with-PyTorch-\" data-toc-modified-id=\"-Part-2.-Implementing-Deep-Networks-with-PyTorch--3\"><font color=\"teal\"> Part 2. Implementing Deep Networks with PyTorch </font></a></span>\n",
    "            <ul class=\"toc-item\">\n",
    "                <li><span><a href=\"#-4.-Pytorch-Tutorial-\" data-toc-modified-id=\"-4.-Pytorch-Tutorial--3.1\"><font color=\"teal\"> 4. Pytorch Tutorial </font></a></span><ul class=\"toc-item\"><li><span><a href=\"#4.1.-PyTorch-Installation\" data-toc-modified-id=\"4.1.-PyTorch-Installation-3.1.1\">4.1. PyTorch Installation</a></span></li><li><span><a href=\"#4.2.-Torch-tensors-(very)-general-overview\" data-toc-modified-id=\"4.2.-Torch-tensors-(very)-general-overview-3.1.2\">4.2. Torch tensors (very) general overview</a></span></li><li><span><a href=\"#4.3.-Automatic-Gradient-Calculation\" data-toc-modified-id=\"4.3.-Automatic-Gradient-Calculation-3.1.3\">4.3. Automatic Gradient Calculation</a></span></li></ul></li>\n",
    "                <li><span><a href=\"#-5.-Feed-Forward-Networks-using-PyTorch-\" data-toc-modified-id=\"-5.-Feed-Forward-Networks-using-PyTorch--3.2\"><font color=\"teal\"> 5. Feed Forward Networks using PyTorch </font></a></span><ul class=\"toc-item\"><li><span><a href=\"#5.1.-Using-torch-nn.Module-and-nn.Parameter\" data-toc-modified-id=\"5.1.-Using-torch-nn.Module-and-nn.Parameter-3.2.1\">5.1. Using torch <code>nn.Module</code> and <code>nn.Parameter</code></a></span></li><li><span><a href=\"#5.2.-Network-Optimization\" data-toc-modified-id=\"5.2.-Network-Optimization-3.2.2\">5.2. Network Optimization</a></span></li><li><span><a href=\"#5.3.-Multi-Layer-networks-using-nn.Sequential\" data-toc-modified-id=\"5.3.-Multi-Layer-networks-using-nn.Sequential-3.2.3\">5.3. Multi Layer networks using <code>nn.Sequential</code></a></span></li><li><span><a href=\"#5.4.-Generalization\" data-toc-modified-id=\"5.4.-Generalization-3.2.4\">5.4. Generalization</a></span></li></ul></li>\n",
    "                <li><span><a href=\"#6.-Convolutional-Neural-Networks\" data-toc-modified-id=\"6.-Convolutional-Neural-Networks-3.3\">6. Convolutional Neural Networks</a></span></li></ul></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pvacO3ZY0ruw",
   "metadata": {
    "id": "pvacO3ZY0ruw"
   },
   "source": [
    "# <font color='teal'> MNIST Hand Digit Classification </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ioSe4IOP0poW",
   "metadata": {
    "id": "ioSe4IOP0poW"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "size=18\n",
    "params = {'legend.fontsize': 'Large',\n",
    "          'axes.labelsize': size,\n",
    "          'axes.titlesize': size,\n",
    "          'xtick.labelsize': size*0.75,\n",
    "          'ytick.labelsize': size*0.75}\n",
    "plt.rcParams.update(params)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8eZX0Au20x_5",
   "metadata": {
    "id": "8eZX0Au20x_5"
   },
   "source": [
    "In this notebook we will explore different strategies for solving a  classification problem consisting of determining the handwritten digit corresponding to a $ 28 \\times 28 $ pixel image (MNIST dataset).\n",
    "\n",
    "   - We will start by tackling a binary classification problem using different classification algorithms whose implementations are available in scikit-learn, including the multilayer perceptron as an example of a multilayer type neural network (multilayer perceptron, MLP).\n",
    "\n",
    "   - Next, we will consider multiclass classification, and calculate the performance of the previous algorithms in this more challenging problem.\n",
    "\n",
    "   - The last part of the notebook contains an introduction to [PyTorch](https://pytorch.org/), one of the most widely used libraries for neural network training. We will review the basic concepts of Pytorch and the different modules that allow simplifying the implementation and optimization of neural networks, including the design of convolutional neural networks (CNNs) that represent a more powerful alternative than MLPs in image classification problems.\n",
    "\n",
    "For faster executions you must run PyTorch code on GPU. For this, it is sufficient to use Gooble Colab by properly configuring the runtime environment."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pSgy6gZAvkfE",
   "metadata": {
    "id": "pSgy6gZAvkfE"
   },
   "source": [
    "# <font color='teal'> Part 1. Scikit-learn Methods </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "YdZGSCKj3MXA",
   "metadata": {
    "id": "YdZGSCKj3MXA"
   },
   "source": [
    "## <font color='teal'> 1. Data Preparation </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9RIq0oUsnjJD",
   "metadata": {
    "id": "9RIq0oUsnjJD"
   },
   "source": [
    "### 1.1. Data load"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "q0VOVrnjiGx0",
   "metadata": {
    "id": "q0VOVrnjiGx0"
   },
   "source": [
    "MNIST is a handwritten digit classification problem that includes 60,000 training patterns and 10,000 test patterns, with representations obtained from the digitization of the corresponding grayscale images and with $28\\times 28$ pixel resolution.\n",
    "\n",
    "The database can be downloaded from the [OpenML repository](https://www.openml.org/) using tools available from Scikit-learn. It is recommended that after a first download you make a local copy of the data to speed up future executions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pJFsqg8A3aQd",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 29086,
     "status": "ok",
     "timestamp": 1636533480709,
     "user": {
      "displayName": "JERONIMO ARENAS GARCIA",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhOssjUa3TkcZfTRodRH00CbPGuyuJ_qQjeMOwM=s64",
      "userId": "13981211833123710399"
     },
     "user_tz": -60
    },
    "id": "pJFsqg8A3aQd",
    "outputId": "982009e7-2b7b-4140-b799-bc9c24254add"
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_openml\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Load data from https://www.openml.org/d/554\n",
    "X, y = fetch_openml(\"mnist_784\", version=1, return_X_y=True, as_frame=False)\n",
    "# Format data in y as integers\n",
    "y = y.astype(np.int)\n",
    "\n",
    "print('Size of Input Data Matrix:', X.shape)\n",
    "print('Size of Label Vector:', y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "Ae5_EBPpjQ8z",
   "metadata": {
    "id": "Ae5_EBPpjQ8z"
   },
   "source": [
    "\n",
    "\n",
    "**Exercise 1.1:** Save data variables `X` and `y` so that you can use them in the future without donwloading the dataset again. Reload data from file to check the correctness of your solution. You can use `numpy` methods `savez` and `load` for this purpose.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "UQmqMAGN4Tr8",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2001,
     "status": "ok",
     "timestamp": 1636533482699,
     "user": {
      "displayName": "JERONIMO ARENAS GARCIA",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhOssjUa3TkcZfTRodRH00CbPGuyuJ_qQjeMOwM=s64",
      "userId": "13981211833123710399"
     },
     "user_tz": -60
    },
    "id": "UQmqMAGN4Tr8",
    "outputId": "7a8d04bc-dcd8-4a10-fc07-952142ebd92b"
   },
   "outputs": [],
   "source": [
    "# You may use numpy.savez\n",
    "# Reload variables with names Xlocal and ylocal\n",
    "\n",
    "# <SOL>\n",
    "# </SOL>\n",
    "\n",
    "# Check that the reloaded matrices shapes are the same as before\n",
    "print('Size of Input Data Matrix:', Xlocal.shape)\n",
    "print('Size of Label Vector:', ylocal.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cZjFlwuemK55",
   "metadata": {
    "id": "cZjFlwuemK55"
   },
   "source": [
    "Each row of `X` contains a different digit. You can display the original images by realigning the dimensions of each row of `X`, as shown in the following example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "evaQ4noc40V7",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 265
    },
    "executionInfo": {
     "elapsed": 16,
     "status": "ok",
     "timestamp": 1636533482700,
     "user": {
      "displayName": "JERONIMO ARENAS GARCIA",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhOssjUa3TkcZfTRodRH00CbPGuyuJ_qQjeMOwM=s64",
      "userId": "13981211833123710399"
     },
     "user_tz": -60
    },
    "id": "evaQ4noc40V7",
    "outputId": "68bd707d-0036-4298-e4bd-836b434745e7"
   },
   "outputs": [],
   "source": [
    "pos = 1900\n",
    "\n",
    "plt.imshow(X[pos,].reshape(28, 28)), plt.axis('off'), plt.show()\n",
    "print(f'The label of element {pos} is {y[pos]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "uFWLr8QqnqcH",
   "metadata": {
    "id": "uFWLr8QqnqcH"
   },
   "source": [
    "### 1.2. Data partitioning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "apYLkip1o23R",
   "metadata": {
    "id": "apYLkip1o23R"
   },
   "source": [
    "To begin, we will perform a random partition of the available data into training and test sets of 60,000 and 10,000 digits, respectively.\n",
    "\n",
    "The notebook also considers a binary problem consisting of the recognition of digits 7 and 9. We have selected this pair of digits as it is one of the most confusing, but if you wish, you can use any other pair of digits.\n",
    "\n",
    "**Exercise 1.2:** Create the binary classification problem 7 vs 9. Save the corresponding data matrices with names `X_tr_bin`, `y_tr_bin`, `X_tst_bin`, `y_tst_bin`. Make sure that the target vectors contain just 0s (for class 7) and 1s (class 9)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "go-R80Jh5ODw",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 764,
     "status": "ok",
     "timestamp": 1636533483450,
     "user": {
      "displayName": "JERONIMO ARENAS GARCIA",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhOssjUa3TkcZfTRodRH00CbPGuyuJ_qQjeMOwM=s64",
      "userId": "13981211833123710399"
     },
     "user_tz": -60
    },
    "id": "go-R80Jh5ODw",
    "outputId": "21dc4713-d37b-4040-8f97-af8c911e6eda"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_samples = 60000\n",
    "test_samples = 10000\n",
    "\n",
    "X_tr, X_tst, y_tr, y_tst = train_test_split(\n",
    "    Xlocal, ylocal, train_size=train_samples, test_size=test_samples,\n",
    "    random_state=0)\n",
    "\n",
    "print('Shape of input training data (multiclass):', X_tr.shape)\n",
    "print('Shape of target training vector (multiclass):', y_tr.shape)\n",
    "print('Shape of input test data (multiclass):', X_tst.shape)\n",
    "print('Shape of target test vector (multiclass):', y_tst.shape)\n",
    "\n",
    "# Exercise: create the binary classification problem 7 vs 9\n",
    "# <SOL>\n",
    "# </SOL>\n",
    "\n",
    "print('\\nShape of input training data (binary):', X_tr_bin.shape)\n",
    "print('Shape of target training vector (binary):', y_tr_bin.shape)\n",
    "print('Shape of input test data (binary):', X_tst_bin.shape)\n",
    "print('Shape of target test vector (binary):', y_tst_bin.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "t6lPtOXPsXOh",
   "metadata": {
    "id": "t6lPtOXPsXOh"
   },
   "source": [
    "### 1.3. Data normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "iIy3YPNXsa1t",
   "metadata": {
    "id": "iIy3YPNXsa1t"
   },
   "source": [
    "Next we will normalize the data for the multiclass and binary problems. When working with images, it is frequent to normalize the input data, corresponding to the gray intensity values of the different pixels, so that they take values in the range $[-0.5, 0.5]$.\n",
    "\n",
    "**Exercise 1.3:** Implement the normalization of the input data using the `MinMaxScaler` from `scikit-learn`. Make sure to normalize data independently for the multiclass and binary classification cases. Make also sure not to refit the scaler object when transforming the test partitions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "tQ3p5r6riZ4g",
   "metadata": {
    "id": "tQ3p5r6riZ4g"
   },
   "outputs": [],
   "source": [
    "# <SOL>\n",
    "# </SOL>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "-Bnmq8Fct5Mp",
   "metadata": {
    "id": "-Bnmq8Fct5Mp"
   },
   "source": [
    "## <font color='teal'> 2. Binary classification </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "C_NU6E8fwXGH",
   "metadata": {
    "id": "C_NU6E8fwXGH"
   },
   "source": [
    "### 2.1. Two dimensional representation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tE62DVX4whjU",
   "metadata": {
    "id": "tE62DVX4whjU"
   },
   "source": [
    "In order to be able to represent the classification frontier, we will begin our exploration by working on the two variables that present the greatest dispersion. To do this, we will use the Principal Component Analysis (PCA) algorithm.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8IG9fDwLavil",
   "metadata": {
    "id": "8IG9fDwLavil"
   },
   "source": [
    "#### 2.1.1. Principal Component Analysis (PCA)\n",
    "\n",
    "**Exercise 2.1:** Obtain the first two PCA projections for the binary classification problem. Store your results in the variables `X_tr_2D` and` X_tst_2D`. Make a scatter plot of these dimensions distinguishing the points corresponding to both classes, and reflect on the type of classification frontier that would provide a lower error rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "QXGBXfL98b7O",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 351
    },
    "executionInfo": {
     "elapsed": 2368,
     "status": "ok",
     "timestamp": 1636309400146,
     "user": {
      "displayName": "JERONIMO ARENAS GARCIA",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhOssjUa3TkcZfTRodRH00CbPGuyuJ_qQjeMOwM=s64",
      "userId": "13981211833123710399"
     },
     "user_tz": -60
    },
    "id": "QXGBXfL98b7O",
    "outputId": "763d141a-0fcb-47a7-82b6-233406587c3a"
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# <SOL>\n",
    "# </SOL>\n",
    "\n",
    "plt.figure(figsize=(7,5))\n",
    "plt.scatter(X_tr_2D[:, 0], X_tr_2D[:, 1],\n",
    "            c=y_tr_bin, edgecolor='none', alpha=0.5,\n",
    "            cmap=plt.cm.get_cmap('rainbow', 10))\n",
    "plt.xlabel('component 1')\n",
    "plt.ylabel('component 2')\n",
    "plt.colorbar();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "TD_WjynBz50S",
   "metadata": {
    "id": "TD_WjynBz50S"
   },
   "source": [
    "#### 2.1.2. Linear Classification with Logistic Regression\n",
    "\n",
    "First we will analyze the behavior of logistic regression for this dataset **using just the two first dimensions of PCA as the input variables.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "yT5MS8Hc0jDd",
   "metadata": {
    "id": "yT5MS8Hc0jDd"
   },
   "source": [
    "**Exercise 2.2:** Use `GridSearchCV` and `LogisticRegression` methods from `sklearn` to calculate the average classification error for different values of the inverse regularization parameter $C$. Explore $C$ using a logarithmic scale, e.g.,\n",
    "\n",
    "$$C = [100, 10, 1, 0.1, 0.01, 0.001, 0.0001]$$\n",
    "\n",
    "Use a 5-fold strategy to obtain the best value of parameter $C$.\n",
    "\n",
    "Store the best classifier in object `clf` (otherwise, some of the next cells might fail)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "zr-IwXnADzsz",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 348
    },
    "executionInfo": {
     "elapsed": 1117,
     "status": "ok",
     "timestamp": 1636237745826,
     "user": {
      "displayName": "JERONIMO ARENAS GARCIA",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhOssjUa3TkcZfTRodRH00CbPGuyuJ_qQjeMOwM=s64",
      "userId": "13981211833123710399"
     },
     "user_tz": -60
    },
    "id": "zr-IwXnADzsz",
    "outputId": "d1289209-2199-4459-812f-0e5342817969"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Create an array with the explored values of $C$\n",
    "# C_param = <FILL IN>\n",
    "\n",
    "# Write the code to apply grid search here:\n",
    "# <SOL>\n",
    "# </SOL>\n",
    "\n",
    "# Create and array with the validation error rates for the corresponding values of C\n",
    "# CE_param = <FILL IN>\n",
    "\n",
    "plt.semilogx(C_param, CE_param)\n",
    "plt.xlabel('$C$'), plt.ylabel('Error rates')\n",
    "plt.title('Classification Error Rate (Linear Logistic Regression)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f1bbbad-e2d7-4c67-a359-a578c9576fc4",
   "metadata": {
    "id": "yT5MS8Hc0jDd"
   },
   "source": [
    "**Exercise 2.3:** Calculate the average classification error rate (CE) and negative log-likelihood (NLL) of the best classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "394d5af4-b116-4fd4-be37-f9d11e07fcc4",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 348
    },
    "executionInfo": {
     "elapsed": 1117,
     "status": "ok",
     "timestamp": 1636237745826,
     "user": {
      "displayName": "JERONIMO ARENAS GARCIA",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhOssjUa3TkcZfTRodRH00CbPGuyuJ_qQjeMOwM=s64",
      "userId": "13981211833123710399"
     },
     "user_tz": -60
    },
    "id": "zr-IwXnADzsz",
    "outputId": "d1289209-2199-4459-812f-0e5342817969"
   },
   "outputs": [],
   "source": [
    "# Compute the average Classification Error Rate of previous classifier calculated over the test data\n",
    "# CE_LR2D = <FILL IN>\n",
    "\n",
    "# Compute probabilistic (or log-probabilistic) predictions\n",
    "# q_pred = <FILL IN>\n",
    "\n",
    "# Compute the Negative log-lilelihood of previous classifier calculated over the test data\n",
    "# NLL_LR2D = <FILL IN>\n",
    "\n",
    "print('Error rate of Linear Logistic Regression Classifier (%):', CE_LR2D)\n",
    "print('NLL of Linear Logistic Regression Classifier:', NLL_LR2D)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "SOYth_tyF0nw",
   "metadata": {
    "id": "SOYth_tyF0nw"
   },
   "source": [
    "The next cell visualizes the classification border and the probabilistic map of the selected classifier (`clf`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "xggzYojG7Q2U",
   "metadata": {
    "id": "xggzYojG7Q2U"
   },
   "outputs": [],
   "source": [
    "def plot_proba_map(X, y, clf):\n",
    "    \"\"\"\n",
    "    Plots a probabilistic map for classifier clf, and the coloured scatter plot of the input samples\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    X:   Input Data Matrix (Size (N,2))\n",
    "    y:   Targets (Size (N,))\n",
    "    clf: A classifier object. It should have a predict_proba method\n",
    "    \"\"\"\n",
    "\n",
    "    if X.shape[1] != 2:\n",
    "        print('Can only plot 2D probability maps')\n",
    "    else:\n",
    "        # Create a regtangular grid.\n",
    "        x_min, x_max = X[:, 0].min(), X[:, 0].max() \n",
    "        y_min, y_max = X[:, 1].min(), X[:, 1].max()\n",
    "        dx = x_max - x_min\n",
    "        dy = y_max - y_min\n",
    "        h = dy /400\n",
    "        xx, yy = np.meshgrid(np.arange(x_min - 0.1 * dx, x_max + 0.1 * dx, h),\n",
    "                             np.arange(y_min - 0.1 * dx, y_max + 0.1 * dy, h))\n",
    "        X_grid = np.array([xx.ravel(), yy.ravel()]).T\n",
    "\n",
    "        # Compute the classifier output for all samples in the grid.\n",
    "        pp = clf.predict_proba(X_grid)[:,1]\n",
    "        pp = pp.reshape(xx.shape)\n",
    "\n",
    "        plt.figure(figsize=(10,5))\n",
    "        plt.subplot(1, 2, 1)\n",
    "        CS = plt.contourf(xx, yy, pp, cmap=plt.cm.copper)\n",
    "        plt.contour(CS, levels=[0.5], colors='m', linewidths=(3,))\n",
    "        plt.subplot(1, 2, 2)\n",
    "        CS = plt.contourf(xx, yy, pp, cmap=plt.cm.copper)\n",
    "        plt.scatter(X[:, 0], X[:, 1], c=y, s=4, cmap='summer')\n",
    "        plt.contour(CS, levels=[0.5], colors='m', linewidths=(3,))\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c399ccRoY0Qo",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 324
    },
    "executionInfo": {
     "elapsed": 1425,
     "status": "ok",
     "timestamp": 1636238071492,
     "user": {
      "displayName": "JERONIMO ARENAS GARCIA",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhOssjUa3TkcZfTRodRH00CbPGuyuJ_qQjeMOwM=s64",
      "userId": "13981211833123710399"
     },
     "user_tz": -60
    },
    "id": "c399ccRoY0Qo",
    "outputId": "cc81beaf-4f89-468a-ac8e-f87ab338bad2"
   },
   "outputs": [],
   "source": [
    "plot_proba_map(X_tr_2D, y_tr_bin, clf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "Sj3N0_GR4Nj0",
   "metadata": {
    "id": "Sj3N0_GR4Nj0"
   },
   "source": [
    "A relevant parameter for the training of logistic regression is the optimization method used to update the parameter vector $\\bf w$. You can try other methods different from the `lbfgs` method which is the default selection. You can find information about these methods online, for instance in this stackoverflow entry: [Logistic regression python solvers' definitions](https://stackoverflow.com/questions/38640109/logistic-regression-python-solvers-definitions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "k2GKhS-LNc_r",
   "metadata": {
    "id": "k2GKhS-LNc_r"
   },
   "source": [
    "#### 2.1.3. Polynomial Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "genyAZcIoIsX",
   "metadata": {
    "id": "genyAZcIoIsX"
   },
   "source": [
    "A strategy for the implementation of classifiers that provide non-linear boundaries consists of transforming the input variables. To this end, in this section we will use polynomial transformations, exploring different values of the degree of the built-in terms to try to optimize performance.\n",
    "\n",
    "The proposed processing scheme for this section consists of the following steps:\n",
    "\n",
    "   - Polynomial expansion of the input variables using the `PolynomialFeatures` method of scikit-learn\n",
    "   - Scaling of all variables to zero mean and unit variance (`StandardScaler` method)\n",
    "   - Logistic regression\n",
    "\n",
    "The free parameters to be adjusted are, therefore, the degree of the polynomial transformation and the parameter $ C $ of the logistic regression."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "CGGx0qKtogwa",
   "metadata": {
    "id": "CGGx0qKtogwa"
   },
   "source": [
    "**Exercise 2.4**: Validate parameters `degree` and `C` of the proposed classification scheme using a 5-fold validation. Allow for polynomial transformations of degree up to 6. A very convenient way to do this is to define a processing pipeline in scikit-learn (using the `Pipeline` class), and use it together with `GridSearchCV`.\n",
    "\n",
    "Note that both parameters should be validated together, i.e., all possible combinations need to be evaluated.\n",
    "\n",
    "For the next two cells to work, you need to use the following variable names:\n",
    "\n",
    "   - `clf`: Best classifier selected with 5 fold strategy. It needs to implement a `clf.predict_proba` method\n",
    "   - `CE_poly`: Average Classification Error Rate of previous classifier calculated over the test data\n",
    "   - `NLL_poly`: Negative log-lilelihood of previous classifier calculated over the test data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Bf9clKZMNbI1",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 35089,
     "status": "ok",
     "timestamp": 1636242703697,
     "user": {
      "displayName": "JERONIMO ARENAS GARCIA",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhOssjUa3TkcZfTRodRH00CbPGuyuJ_qQjeMOwM=s64",
      "userId": "13981211833123710399"
     },
     "user_tz": -60
    },
    "id": "Bf9clKZMNbI1",
    "outputId": "9a8aa8af-9d6a-444b-913f-ebf3b4b785aa"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Write your code here\n",
    "# <SOL>\n",
    "# </SOL>\n",
    "\n",
    "print('Error rate of Polynomial Logistic Regression Classifier (%):', CE_poly)\n",
    "print('NLL of Polynomial Logistic Regression Classifier:', NLL_poly)\n",
    "print('Selected parameters:', clf.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Y_41WubvP2mt",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 324
    },
    "executionInfo": {
     "elapsed": 1325,
     "status": "ok",
     "timestamp": 1636242711605,
     "user": {
      "displayName": "JERONIMO ARENAS GARCIA",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhOssjUa3TkcZfTRodRH00CbPGuyuJ_qQjeMOwM=s64",
      "userId": "13981211833123710399"
     },
     "user_tz": -60
    },
    "id": "Y_41WubvP2mt",
    "outputId": "73818299-aa0f-4fe8-f999-87273b8c99c3"
   },
   "outputs": [],
   "source": [
    "plot_proba_map(X_tr_2D, y_tr_bin, clf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "C6oV1jXvP5sV",
   "metadata": {
    "id": "C6oV1jXvP5sV"
   },
   "source": [
    "#### 2.1.4. Multi-Layer Perceptron\n",
    "\n",
    "Rather than using a predefined type of transformation, such as polynomial, we can recur to neural networks with non-linear activation functions to learn the most appropriate representation of the input data to solve the classification problem.\n",
    "\n",
    "**Exercise 2.5:** In this section you will use the `MLPClassifier method` provided by scikit-learn for the implementation of feed-forward networks. You will only need to adjust the following parameters:\n",
    "\n",
    "   - `activation`: The activation function for the units of the intermediate layer. You can test `relu` and `tanh`.\n",
    "   - `max_iter`: Number of iterations of the optimization method. Try different values and make sure that the algorithm has completely converged.\n",
    "   - `hidden_layer_sizes`: Use just one hidden layer with 20 units\n",
    "   - `alpha`: The L2 regularization parameters.\n",
    "\n",
    "Save your results using the following variable names:\n",
    "\n",
    "   - `clf`: Best classifier selected with 5 fold strategy. It needs to implement a `clf.predict_proba` method\n",
    "   - `CE_MLP`: Average Classification Error Rate of previous classifier calculated over the test data\n",
    "   - `NLL_MLP`: Negative log-lilelihood of previous classifier calculated over the test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ExmaXdhCTQxl",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 159145,
     "status": "ok",
     "timestamp": 1636309580008,
     "user": {
      "displayName": "JERONIMO ARENAS GARCIA",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhOssjUa3TkcZfTRodRH00CbPGuyuJ_qQjeMOwM=s64",
      "userId": "13981211833123710399"
     },
     "user_tz": -60
    },
    "id": "ExmaXdhCTQxl",
    "outputId": "b0871ee7-11f1-465f-8cd0-a05898fe6133"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "# <SOL>\n",
    "# </SOL>\n",
    "\n",
    "print('CE of MLP Classifier (%):', CE_MLP)\n",
    "print('NLL of MLP Classifier:', NLL_MLP)\n",
    "print('Selected parameters:', clf.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Su8gQGI0ZgYv",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 324
    },
    "executionInfo": {
     "elapsed": 1413,
     "status": "ok",
     "timestamp": 1636243402205,
     "user": {
      "displayName": "JERONIMO ARENAS GARCIA",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhOssjUa3TkcZfTRodRH00CbPGuyuJ_qQjeMOwM=s64",
      "userId": "13981211833123710399"
     },
     "user_tz": -60
    },
    "id": "Su8gQGI0ZgYv",
    "outputId": "78567fa7-9a8f-4399-9189-c0de92daa74a"
   },
   "outputs": [],
   "source": [
    "plot_proba_map(X_tr_2D, y_tr_bin, clf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "_sOedcIrP5ve",
   "metadata": {
    "id": "_sOedcIrP5ve"
   },
   "source": [
    "### 2.2. Classification with all input features\n",
    "\n",
    "**Exercise 2.6:** Analyze the performance in the binary classification problem of the following classifiers: \n",
    "\n",
    "   - Logistic regression (validate $C$)\n",
    "   - K-nearest neighbors (validate $K$)\n",
    "   - Multi-layer perceptron (validate size of hidden layers)\n",
    "\n",
    "For comparison purposes you can use both the average classifiation error rate and the negative log-likelihood. \n",
    "\n",
    "In this section you need to use all 784 available features."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "MVs5s-pFP55_",
   "metadata": {
    "id": "MVs5s-pFP55_"
   },
   "source": [
    "#### 2.2.1. Logistic Regression\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "WdJ5-j_zu0iy",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 484
    },
    "executionInfo": {
     "elapsed": 281892,
     "status": "ok",
     "timestamp": 1636310214828,
     "user": {
      "displayName": "JERONIMO ARENAS GARCIA",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhOssjUa3TkcZfTRodRH00CbPGuyuJ_qQjeMOwM=s64",
      "userId": "13981211833123710399"
     },
     "user_tz": -60
    },
    "id": "WdJ5-j_zu0iy",
    "outputId": "5d897f26-0806-4a01-bfda-e8653fa3de4d"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# <SOL>\n",
    "# </SOL>\n",
    "\n",
    "plt.semilogx(C_param, CE_param)\n",
    "plt.xlabel('C'), plt.ylabel('(%)'), plt.title('Classification Error Rate (Linear Logistic Regression)')\n",
    "plt.show()\n",
    "\n",
    "print('CE of Linear Logistic Regression Classifier (%):', CE_LR)\n",
    "print('NLL of Linear Logistic Regression Classifier:', NLL_LR)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "gAqQXl3_uYr0",
   "metadata": {
    "id": "gAqQXl3_uYr0"
   },
   "source": [
    "#### 2.2.2. K Nearest Neighbors\n",
    "\n",
    "Be aware that, in this case, the number of test samples together with the dimension of the input data implies long processing times for classification (close to 1 min per execution in Google Colab). You might consider to subsample the test partition for the validation process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dcgknXiu1Lk",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 1911296,
     "status": "ok",
     "timestamp": 1636279319011,
     "user": {
      "displayName": "JERONIMO ARENAS GARCIA",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhOssjUa3TkcZfTRodRH00CbPGuyuJ_qQjeMOwM=s64",
      "userId": "13981211833123710399"
     },
     "user_tz": -60
    },
    "id": "9dcgknXiu1Lk",
    "outputId": "a40b25e7-954b-4f9e-de04-cf0de88e684e"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "# <SOL>\n",
    "# </SOL>\n",
    "\n",
    "plt.plot(K_param, CE_param)\n",
    "plt.xlabel('K'), plt.ylabel('(%)'), plt.title('Classification Error Rate (KNN)')\n",
    "plt.show()\n",
    "\n",
    "print('CE of KNN (%):', CE_KNN)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "O6ErAvgkuYvL",
   "metadata": {
    "id": "O6ErAvgkuYvL"
   },
   "source": [
    "#### 2.2.3. Multi-Layer Perceptron\n",
    "\n",
    "Be aware that, in this case, the number of test samples together with the dimension of the input data implies long processing times for classification (close to 1 min per execution in Google Colab). You might consider to subsample the test partition for the validation process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "MaQenA33u15-",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3026802,
     "status": "ok",
     "timestamp": 1636313410361,
     "user": {
      "displayName": "JERONIMO ARENAS GARCIA",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhOssjUa3TkcZfTRodRH00CbPGuyuJ_qQjeMOwM=s64",
      "userId": "13981211833123710399"
     },
     "user_tz": -60
    },
    "id": "MaQenA33u15-",
    "outputId": "5218af52-e2a5-41dd-ea70-cb4cb90fb3cc"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "# <SOL>\n",
    "# </SOL>\n",
    "\n",
    "print('CE of MLP Classifier (%):', CE_MLP)\n",
    "print('NLL of MLP Classifier:', NLL_MLP)\n",
    "print('Selected parameters:', clf.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "DCS-ejARuYxp",
   "metadata": {
    "id": "DCS-ejARuYxp"
   },
   "source": [
    "## <font color='teal'> 3. Multi Class Classification </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1Z-im408BD7E",
   "metadata": {
    "id": "1Z-im408BD7E"
   },
   "source": [
    "In this section we will train classification schemes that allow us to discriminate between the 10 digits that make up the complete dataset. In this case, you must use the parameters provided, and you will be asked to study the execution times required by the different methods. To do this, you can use the `time` library as follows:\n",
    "\n",
    "```\n",
    "import time\n",
    "\n",
    "start = time.time()\n",
    "#Some code should go here\n",
    "etime = time.time() - start\n",
    "````\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b-G225JTttR7",
   "metadata": {
    "id": "b-G225JTttR7"
   },
   "source": [
    "### 3.1. Principal Component Analysis (PCA)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "WMb-VBAYEa7v",
   "metadata": {
    "id": "WMb-VBAYEa7v"
   },
   "source": [
    "In a preliminary way, and as you did in section 2.2.1, we can analyze the first two PCA components of the available data. You can see that in this case there is a very important overlap between the digits of all the classes on the first two components of PCA.\n",
    "\n",
    "In this section, we will apply the different classification strategies **using all the available features**.\n",
    "\n",
    "**Exercise 3.1:** Analyze the variance of the successive projections that the PCA method would obtain, and reflect on whether a smaller-dimensional representation of the input data could be used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4GbmPimxdYk2",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 292
    },
    "executionInfo": {
     "elapsed": 5409,
     "status": "ok",
     "timestamp": 1636316665167,
     "user": {
      "displayName": "JERONIMO ARENAS GARCIA",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhOssjUa3TkcZfTRodRH00CbPGuyuJ_qQjeMOwM=s64",
      "userId": "13981211833123710399"
     },
     "user_tz": -60
    },
    "id": "4GbmPimxdYk2",
    "outputId": "96724578-21be-48de-e5f0-004a84d1a527"
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pca = PCA(2)  # project from 784 to 2 dimensions\n",
    "projected = pca.fit_transform(X_tr)\n",
    "plt.scatter(projected[:, 0], projected[:, 1],\n",
    "            c=y_tr, edgecolor='none', alpha=0.5,\n",
    "            cmap=plt.cm.get_cmap('rainbow', 10))\n",
    "plt.xlabel('component 1')\n",
    "plt.ylabel('component 2')\n",
    "plt.colorbar();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "M6ZNXyCCCyHK",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 291
    },
    "executionInfo": {
     "elapsed": 7244,
     "status": "ok",
     "timestamp": 1636316804138,
     "user": {
      "displayName": "JERONIMO ARENAS GARCIA",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhOssjUa3TkcZfTRodRH00CbPGuyuJ_qQjeMOwM=s64",
      "userId": "13981211833123710399"
     },
     "user_tz": -60
    },
    "id": "M6ZNXyCCCyHK",
    "outputId": "5d523dd6-e703-4fe2-e750-65edf23f204b"
   },
   "outputs": [],
   "source": [
    "# Solution to Exercise 3.1\n",
    "\n",
    "# <SOL>\n",
    "# </SOL>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6eD_lCq1tzY2",
   "metadata": {
    "id": "6eD_lCq1tzY2"
   },
   "source": [
    "### 3.2. Nearest Neighbor Method "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "xd6g04YFN-dr",
   "metadata": {
    "id": "xd6g04YFN-dr"
   },
   "source": [
    "In this section, you will analyze the performance of the nearest neighbor (1-NN) algorithm. The complexity of this algorithm grows with the size of the training set, so it is proposed to analyze the behavior of the algorithm for a variable size of the training set.\n",
    "\n",
    "**Exercise 3.2:** Use the 1-NN method with a varying training set size. Obtain for each size:\n",
    "\n",
    "   - The average classifiation error rate calculated on the test set\n",
    "   - The fit time for the 1-NN method\n",
    "   - The time taken to classify the complete test partition (10000 samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12yjxKyVBJPb",
   "metadata": {
    "id": "12yjxKyVBJPb",
    "outputId": "44e5b0b5-08f9-42ae-fa55-4ae7f4176f93"
   },
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "import time\n",
    "\n",
    "train_size = [250, 500, 1000, 2000, 5000, 10000, 25000, 60000]\n",
    "fit_time = []\n",
    "test_time = []\n",
    "CE = []\n",
    "\n",
    "for ntrain in train_size:\n",
    "    print('Classifying with', ntrain, 'samples')\n",
    "    # Write your code here\n",
    "    # <SOL>\n",
    "    # </SOL>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "LC-Atn_RF7xh",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 301
    },
    "executionInfo": {
     "elapsed": 632,
     "status": "ok",
     "timestamp": 1636319935586,
     "user": {
      "displayName": "JERONIMO ARENAS GARCIA",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhOssjUa3TkcZfTRodRH00CbPGuyuJ_qQjeMOwM=s64",
      "userId": "13981211833123710399"
     },
     "user_tz": -60
    },
    "id": "LC-Atn_RF7xh",
    "outputId": "3ccd6f5b-051b-4f59-cf24-d72364fc789e"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,3.5))\n",
    "plt.subplot(1, 3, 1), plt.plot(train_size, 100*np.array(CE)), plt.xlabel('Training Set Size'), plt.ylabel('(%)'), plt.title('CE')\n",
    "plt.subplot(1, 3, 2), plt.plot(train_size, fit_time), plt.xlabel('Training Set Size'), plt.ylabel('Seconds'), plt.title('Fit Time')\n",
    "plt.subplot(1, 3, 3), plt.plot(train_size, test_time), plt.xlabel('Training Set Size'), plt.ylabel('Seconds'), plt.title('Test Time')\n",
    "plt.show()\n",
    "print('Average Classification Error for the 1-NN approach', 100*np.min(CE))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "upmPdC-sPavl",
   "metadata": {
    "id": "upmPdC-sPavl"
   },
   "source": [
    "**Exercise 3.3:** Calculate the confusion matrix of the 1-NN classifier when using 1000 samples for the training set, and answer the following questions:\n",
    "   - Which two digits are most frequently confused?\n",
    "   - Which is the digit that gets misclassified most often?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "tGEWM259P5B3",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 420
    },
    "executionInfo": {
     "elapsed": 18776,
     "status": "ok",
     "timestamp": 1636320930450,
     "user": {
      "displayName": "JERONIMO ARENAS GARCIA",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhOssjUa3TkcZfTRodRH00CbPGuyuJ_qQjeMOwM=s64",
      "userId": "13981211833123710399"
     },
     "user_tz": -60
    },
    "id": "tGEWM259P5B3",
    "outputId": "d606c5b7-f523-41c9-a64f-3a84885767b9"
   },
   "outputs": [],
   "source": [
    "# <SOL>\n",
    "# </SOL>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "OO_gKc7vt8tG",
   "metadata": {
    "id": "OO_gKc7vt8tG"
   },
   "source": [
    "### 3.3. Multi-Layer Perceptron"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "FXrJj8xukfwz",
   "metadata": {
    "id": "FXrJj8xukfwz"
   },
   "source": [
    "**Exercise 3.4:** Train an MLP network using all samples in the training set. Use the following settings: \n",
    "   - `relu` activation function for the hidden units \n",
    "   - Two hidden layers with 200 and 100 neurons, respectively\n",
    "   - Maximum number of iterations for the training: 2000 iterations\n",
    "   \n",
    "Compare the performance of the MLP network with that of the 1-NN method. Consider in your comparison both the classification error rate and the fit and operation times.\n",
    "\n",
    "Calculate also the negative log likelihood of the model implemented by the Neural Network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82iWWpbDYEMK",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 249659,
     "status": "ok",
     "timestamp": 1636325828946,
     "user": {
      "displayName": "JERONIMO ARENAS GARCIA",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhOssjUa3TkcZfTRodRH00CbPGuyuJ_qQjeMOwM=s64",
      "userId": "13981211833123710399"
     },
     "user_tz": -60
    },
    "id": "82iWWpbDYEMK",
    "outputId": "c17516dc-ee93-430f-afe7-f41db880ae81"
   },
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "MLP = MLPClassifier(activation='relu', max_iter=2000, hidden_layer_sizes=(200,100))\n",
    "\n",
    "# <SOL>\n",
    "# </SOL>\n",
    "print('Negative Log Likelihood for the MLP classifier:', NLL_MLP)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "AwWMVOn3vvsv",
   "metadata": {
    "id": "AwWMVOn3vvsv"
   },
   "source": [
    "# <font color='teal'> Part 2. Implementing Deep Networks with PyTorch </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bPydAYeOv4Co",
   "metadata": {
    "id": "bPydAYeOv4Co"
   },
   "source": [
    "## <font color='teal'> 4. Pytorch Tutorial </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "lUVYTC63yZ-v",
   "metadata": {
    "id": "lUVYTC63yZ-v"
   },
   "source": [
    "* Pytorch is a Python library that provides different levels of abstraction for implementing deep neural networks\n",
    "\n",
    "* The main features of PyTorch are:\n",
    "\n",
    "    * Definition of numpy-like n-dimensional *tensors*. They can be stored in / moved to GPU for parallel execution of operations\n",
    "    * Automatic calculation of gradients, making *backward gradient calculation* transparent to the user\n",
    "    * Definition of common loss functions, NN layers of different types, optimization methods, data loaders, etc, simplifying NN implementation and training\n",
    "    * Provides different levels of abstraction, thus a good balance between flexibility and simplicity\n",
    "    \n",
    "* This notebook provides just a basic review of the main concepts necessary to train NNs with PyTorch taking materials from:\n",
    "    * <a href=\"https://pytorch.org/tutorials/beginner/pytorch_with_examples.html\">Learning PyTorch with Examples</a>, by Justin Johnson\n",
    "    * <a href=\"https://pytorch.org/tutorials/beginner/nn_tutorial.html\">What is *torch.nn* really?</a>, by Jeremy Howard\n",
    "    * <a href=\"https://www.kaggle.com/kanncaa1/pytorch-tutorial-for-deep-learning-lovers\">Pytorch Tutorial for Deep Learning Lovers</a>, by Kaggle user kanncaa1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "crTuxoykyoAe",
   "metadata": {
    "id": "crTuxoykyoAe"
   },
   "source": [
    "### 4.1. PyTorch Installation\n",
    "\n",
    "* PyTorch can be installed with or without GPU support\n",
    "    * If you have an Anaconda installation, you can install from the command line, using the <a href=\"https://pytorch.org/\">instructions of the project website</a>\n",
    "    \n",
    "* PyTorch is also preinstalled in Google Collab with free GPU access\n",
    "    * Follow RunTime -> Change runtime type, and select GPU for HW acceleration\n",
    "    \n",
    "* Please, refer to Pytorch getting started tutorial for a quick introduction regarding tensor definition, GPU vs CPU storage of tensors, operations, and bridge to Numpy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "vTUwFpyXy-nB",
   "metadata": {
    "id": "vTUwFpyXy-nB"
   },
   "source": [
    "###  4.2. Torch tensors (very) general overview\n",
    "\n",
    "Esentially, tensors are objects provided by PyTorch for numerical representation. They are generic n-dimensional arrays such as the ones used by Numpy. Apart from the library providing them, there are two important differences between Numpy arrays and PyTorch tensors:\n",
    "\n",
    "   - Tensors can be stored in / moved to GPU. When doing so, certain operations will be parallelized resulting in faster execution.\n",
    "   - Tensors provide out-of-the-box functions for tracking the operations in which they are involved, and to systematically compute derivatives, something which is very useful for the implementation of the backpropagation method used for training deep networks.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "jNuLPBnz1npI",
   "metadata": {
    "id": "jNuLPBnz1npI"
   },
   "source": [
    "\n",
    "**Creating tensors from Numpy arrays**\n",
    "\n",
    "We can create tensors with different construction methods provided by the library, either to create new tensors from scratch or from a Numpy array\n",
    "\n",
    "Tensors can be converted back to numpy arrays\n",
    "\n",
    "Note that in this case, a tensor and its corresponding numpy array **will share memory**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afEQ263Vv1-k",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 24989,
     "status": "ok",
     "timestamp": 1636533526942,
     "user": {
      "displayName": "JERONIMO ARENAS GARCIA",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhOssjUa3TkcZfTRodRH00CbPGuyuJ_qQjeMOwM=s64",
      "userId": "13981211833123710399"
     },
     "user_tz": -60
    },
    "id": "afEQ263Vv1-k",
    "outputId": "9ff63d9b-5cef-40b1-bfe4-a98f39bca647"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "x = torch.rand((100,200))\n",
    "X_tr_tensor = torch.from_numpy(X_tr)\n",
    "\n",
    "print(x.type())\n",
    "print(X_tr_tensor.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "--X1ce7l1Mao",
   "metadata": {
    "id": "--X1ce7l1Mao"
   },
   "source": [
    "**Operations and slicing with tensors**\n",
    "\n",
    "Operations and slicing involving tensors use a syntax similar to that of numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6wPQpSsg16SA",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 319,
     "status": "ok",
     "timestamp": 1636533527909,
     "user": {
      "displayName": "JERONIMO ARENAS GARCIA",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhOssjUa3TkcZfTRodRH00CbPGuyuJ_qQjeMOwM=s64",
      "userId": "13981211833123710399"
     },
     "user_tz": -60
    },
    "id": "6wPQpSsg16SA",
    "outputId": "8a988e63-8d46-4ea7-c04f-9658e05e8295"
   },
   "outputs": [],
   "source": [
    "print('Size of tensor x:', x.size())\n",
    "print('Tranpose of vector has size', x.t().size()) #Transpose and compute size\n",
    "print('Extracting upper left matrix of size 3 x 3:', x[:3,:3])\n",
    "print((x @ x.t()).size()) \n",
    "xpx = x.add(x)\n",
    "xpx2 = torch.add(x,x)\n",
    "print((xpx!=xpx2).sum())   #Since all are equal, count of different terms is zero"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9q97A7L2070w",
   "metadata": {
    "id": "9q97A7L2070w"
   },
   "source": [
    "Adding underscore performs operations \"*in place*\", e.g., `x.add_(y)`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "KYD2Sma02MPd",
   "metadata": {
    "id": "KYD2Sma02MPd"
   },
   "source": [
    "**Parallelization of Operations using GPUs**\n",
    "\n",
    "* If a GPU is available, tensors can be moved to and from the GPU device\n",
    "\n",
    "* Operations on tensors stored in a GPU will be carried out using GPU resources and will typically be highly parallelized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "I-xOBZQHvuLW",
   "metadata": {
    "executionInfo": {
     "elapsed": 14682,
     "status": "ok",
     "timestamp": 1636533554918,
     "user": {
      "displayName": "JERONIMO ARENAS GARCIA",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhOssjUa3TkcZfTRodRH00CbPGuyuJ_qQjeMOwM=s64",
      "userId": "13981211833123710399"
     },
     "user_tz": -60
    },
    "id": "I-xOBZQHvuLW",
    "outputId": "635920f1-f3d9-4018-f636-09dc0b866a28"
   },
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "    x = x.to(device)\n",
    "    y = x.add(x)\n",
    "    y = y.to('cpu')\n",
    "else:\n",
    "    print('No GPU card is available')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "LnFV0-nFpSDD",
   "metadata": {
    "id": "LnFV0-nFpSDD"
   },
   "source": [
    "**Note:** If you are using Google Colab and the previous cell indicates that you do not have access to a GPU, you may change your runtime type. However, note that doing so will restart your runtime, so that you will have to run again the initial cells of the notebook to load the data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "kPZ8Loja2hZV",
   "metadata": {
    "id": "kPZ8Loja2hZV"
   },
   "source": [
    "### 4.3. Automatic Gradient Calculation\n",
    "\n",
    "* PyTorch tensors have a property ```requires_grad```. When true, PyTorch automatic gradient calculation will be activated for that variable\n",
    "\n",
    "* In order to compute these derivatives numerically, PyTorch keeps track of all operations carried out on these variables, organizing them in a forward computation graph.\n",
    "\n",
    "* When executing the ```backward()``` method, derivatives will be calculated\n",
    "\n",
    "* However, this should only be activated when necessary, to save computation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "B-GRWH6N2mhw",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 189,
     "status": "ok",
     "timestamp": 1636533576779,
     "user": {
      "displayName": "JERONIMO ARENAS GARCIA",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhOssjUa3TkcZfTRodRH00CbPGuyuJ_qQjeMOwM=s64",
      "userId": "13981211833123710399"
     },
     "user_tz": -60
    },
    "id": "B-GRWH6N2mhw",
    "outputId": "4e8f420c-bfb3-4212-ede7-8fa86fbebe4b"
   },
   "outputs": [],
   "source": [
    "x.requires_grad = True\n",
    "y = (3 * torch.log(x)).sum()\n",
    "y.backward()\n",
    "print(x.grad[:2,:2])\n",
    "print(3/x[:2,:2])\n",
    "\n",
    "x.requires_grad = False\n",
    "x.grad.zero_()\n",
    "print('Automatic gradient calculation is deactivated, and gradients set to zero')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ckZzt_KO2xmd",
   "metadata": {
    "id": "ckZzt_KO2xmd"
   },
   "source": [
    "**Exercise 4.1:**\n",
    "\n",
    "* Initialize a tensor ```x``` as `X_tr[0,315:325]`\n",
    "* Compute output vector ```y``` applying a function of your choice to ```x```\n",
    "* Compute scalar value ```z``` as the sum of all elements in ```y``` squared\n",
    "* Check that ```x.grad``` calculation is correct using the ```backward``` method\n",
    "* Try to run your cell multiple times to see if the calculation is still correct. If not, implement the necessary modifications so that you can run the cell multiple times, but the gradient does not change from run to run\n",
    "\n",
    "**Note:** The backward method can only be run on scalar variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "UFHdNn4TsN1_",
   "metadata": {
    "id": "UFHdNn4TsN1_"
   },
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    x = torch.from_numpy(X_tr[0,315:325]).to(device)\n",
    "else:\n",
    "    x = torch.from_numpy(X_tr[0,315:325])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "GS7ZhZmbpzP6",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 187,
     "status": "ok",
     "timestamp": 1636533581689,
     "user": {
      "displayName": "JERONIMO ARENAS GARCIA",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhOssjUa3TkcZfTRodRH00CbPGuyuJ_qQjeMOwM=s64",
      "userId": "13981211833123710399"
     },
     "user_tz": -60
    },
    "id": "GS7ZhZmbpzP6",
    "outputId": "ea4e9c8f-8523-4bea-e05e-8f65f8331114"
   },
   "outputs": [],
   "source": [
    "# <SOL>\n",
    "# </SOL>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aAexovhj3VQS",
   "metadata": {
    "id": "aAexovhj3VQS"
   },
   "source": [
    "## <font color='teal'> 5. Feed Forward Networks using PyTorch </font>\n",
    "\n",
    "In this section we are going to illustrate how we can implement a multilayer perceptron-type neural network using the features of PyTorch. The network thus implemented will be equivalent to the one you would train using the scikit-learn ```MLP``` function, but thanks to the use of PyTorch it will be able to be executed parallelizing many of the calculations in GPU, which will allow a much faster training.\n",
    "\n",
    "A first possibility would be the direct implementation of the neural network, through an implementation based on PyTorch tensors of the evaluation functions of the neural network and of the derivatives of the cost function with respect to the different parameters (back-propagation). However, PyTorch module ```nn``` provides different levels of abstraction that considerably simplify the implementation of the network, in addition to making the theoretical calculation of derivatives unnecessary.\n",
    "\n",
    "Before proceding, we need to import training and test data as PyTorch tensors. The fragment below can be used for that purpose. Note that if you are using a GPU, all tensors should be moved to the GPU."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "U-cK5dD5bCFV",
   "metadata": {
    "id": "U-cK5dD5bCFV"
   },
   "source": [
    "**Exercise 5.1:** Complete the code below to create PyTorch tensors for the different MNIST variables. The tensors for the input data have already been created for you. For encoding class membership, you need to create ```y_tr``` and ```y_val``` using One-Hot-Encoding. You can easily do that using sklearn method ```LabelBinarizer```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aoAGCAD42xE4",
   "metadata": {
    "id": "aoAGCAD42xE4"
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelBinarizer\n",
    "\n",
    "#Convert to Torch tensors. Float type is required\n",
    "X_tr_torch = torch.from_numpy(X_tr).float()\n",
    "X_val_torch = torch.from_numpy(X_tst).float()\n",
    "# <SOL>\n",
    "# </SOL>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ogmdp2B8t3PO",
   "metadata": {
    "id": "ogmdp2B8t3PO"
   },
   "source": [
    "### 5.1. Using torch ```nn.Module``` and ```nn.Parameter```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5zvt1vpPWvOd",
   "metadata": {
    "id": "5zvt1vpPWvOd"
   },
   "source": [
    "PyTorch ```nn``` module provides many attributes and methods to make simple the implementation and training of Neural Networks. \n",
    "\n",
    "```nn.Module``` and ```nn.Parameter``` allow to implement a concise network configuration, and simplify the calculation of the gradients\n",
    "\n",
    "* ```nn.Module``` is a PyTorch class that will be used to encapsulate and design a specific neural network, thus, it is central to the implementation of deep neural nets using PyTorch\n",
    "\n",
    "* ```nn.Parameter``` allow the definition of trainable network parameters. In this way, we will simplify the implementation of the training loop.\n",
    "\n",
    "* All parameters defined with ```nn.Parameter``` will have ```requires_grad = True```\n",
    "\n",
    "Below you can see a PyTorch fragment for the definition of a single layer perceptron (SLP) network. You can see that at least two methods need to be defined: the initialization of the network (including parameter definition and initialization), and a ```forward``` method that implements how the network produces its output for a given input pattern. Other auxiliary functions may be defined as well.\n",
    "\n",
    "However, you can see that there is no need to implement a ```backward``` method for gradient calculation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "uR8STHXnWPuY",
   "metadata": {
    "id": "uR8STHXnWPuY"
   },
   "source": [
    "````\n",
    "from torch import nn\n",
    "\n",
    "class my_multiclass_net(nn.Module):\n",
    "    def __init__(self, nin, nout):\n",
    "        \"\"\"This method initializes the network parameters\n",
    "        Parameters nin and nout stand for the number of input parameters (features in X)\n",
    "        and output parameters (number of classes)\"\"\"\n",
    "        super().__init__()\n",
    "        self.W = nn.Parameter(.1 * torch.randn(nin, nout))\n",
    "        self.b = nn.Parameter(torch.zeros(nout))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return softmax(x @ self.W + self.b)\n",
    "    \n",
    "    def softmax(t):\n",
    "        \"\"\"Compute softmax values for each sets of scores in t\"\"\"\n",
    "        return t.exp() / t.exp().sum(-1).unsqueeze(-1)\n",
    "````\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "rj6ug3flamZd",
   "metadata": {
    "id": "rj6ug3flamZd"
   },
   "source": [
    "You can see that by using ```nn.Parameter``` and ```nn.Module``` you can easily implement any function of your choice. However, we need to be careful about matrix dimensions and some particularities which are required to correctly operate PyTorch tensors. \n",
    "\n",
    "For standard feed-forward networks such as MLPs, we can use other PyTorch abstraction levels that make these implementations even simpler.\n",
    "\n",
    "* ```nn.Module``` comes with several kinds of pre-defined layers, thus making it even simpler to implement neural networks\n",
    "\n",
    "* ```nn.CrossEntropyLoss``` implements the calculation of the negative log likelihood incorporating the softmax for the predictions (so there is no need to include it in the ```forward``` method of the network\n",
    "\n",
    "The code below shows how these predefined layers and cost functions can be used to create an SLP network in a rather straightforward manner. Note that when creating the network we just need to specify the dimensionality of the input and output data, i.e., number of input features and number of classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Jb7rEYPjxdzu",
   "metadata": {
    "id": "Jb7rEYPjxdzu"
   },
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "\n",
    "class my_multiclass_net(nn.Module):\n",
    "    def __init__(self, nin, nout):\n",
    "        \"\"\"Note that now, we do not even need to initialize network parameters ourselves\"\"\"\n",
    "        super().__init__()\n",
    "        self.lin = nn.Linear(nin, nout)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.lin(x)\n",
    "    \n",
    "loss_func = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fimNhDf2z1Bz",
   "metadata": {
    "id": "fimNhDf2z1Bz"
   },
   "source": [
    "The code below implements the training of the network using conventional gradient descent. The training takes place over a predefined number of epochs using a fixed step size.\n",
    "\n",
    "**It is important to note that:**\n",
    "\n",
    "   * Gradient updates are stopped after the evaluation of the network output for all training patterns. This is done by encapsulating any additional computations inside a block ```with torch.no_grad()```.\n",
    "   * Parameter updates are implemented by iterating over all network parameter using the method ```nn.Model.parameters()```\n",
    "   * After parameter update, gradients are set back to zero for the next epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mz-iO4Pdiaer",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 50228,
     "status": "ok",
     "timestamp": 1636496658527,
     "user": {
      "displayName": "JERONIMO ARENAS GARCIA",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhOssjUa3TkcZfTRodRH00CbPGuyuJ_qQjeMOwM=s64",
      "userId": "13981211833123710399"
     },
     "user_tz": -60
    },
    "id": "mz-iO4Pdiaer",
    "outputId": "7a448241-f154-4469-cdd5-cc9e60f25aaa"
   },
   "outputs": [],
   "source": [
    "def CE(y, y_hat):\n",
    "    return 1-(y.argmax(axis=-1) == y_hat.argmax(axis=-1)).float().mean()\n",
    "    \n",
    "my_net = my_multiclass_net(X_tr_torch.size()[1], y_tr_torch.size()[1])\n",
    "epochs = 300\n",
    "rho = .1\n",
    "\n",
    "loss_tr = np.zeros(epochs)\n",
    "loss_val = np.zeros(epochs)\n",
    "CE_tr = np.zeros(epochs)\n",
    "CE_val = np.zeros(epochs)\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    \n",
    "    print(f'Current epoch: {epoch+1}    \\r', end=\"\")\n",
    "    \n",
    "    #Compute network output and cross-entropy loss\n",
    "    pred = my_net(X_tr_torch)\n",
    "    loss = loss_func(pred, y_tr_torch.argmax(axis=-1))\n",
    "    \n",
    "    #Compute gradients\n",
    "    loss.backward()\n",
    "    \n",
    "    #Deactivate gradient automatic updates\n",
    "    with torch.no_grad():\n",
    "        #Computing network performance after iteration\n",
    "        loss_tr[epoch] = loss.item()\n",
    "        CE_tr[epoch] = CE(y_tr_torch, pred).item()\n",
    "        pred_val = my_net(X_val_torch)\n",
    "        loss_val[epoch] = loss_func(pred_val, y_val_torch.argmax(axis=-1)).item()\n",
    "        CE_val[epoch] = CE(y_val_torch, pred_val).item()\n",
    "\n",
    "        #Weight update\n",
    "        for p in my_net.parameters():\n",
    "            p -= p.grad * rho\n",
    "        #Reset gradients\n",
    "        my_net.zero_grad()\n",
    "\n",
    "print('Training of the network took', time.time()-start, 'seconds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "FA4DIFFMqAqt",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 344
    },
    "executionInfo": {
     "elapsed": 579,
     "status": "ok",
     "timestamp": 1636496659096,
     "user": {
      "displayName": "JERONIMO ARENAS GARCIA",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhOssjUa3TkcZfTRodRH00CbPGuyuJ_qQjeMOwM=s64",
      "userId": "13981211833123710399"
     },
     "user_tz": -60
    },
    "id": "FA4DIFFMqAqt",
    "outputId": "f6d1314a-f000-4f50-d5cd-5652b04f7c4e"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(14,5))\n",
    "plt.subplot(1, 2, 1), plt.plot(loss_tr, 'b'), plt.plot(loss_val, 'r'), plt.legend(['train', 'val']), plt.title('Cross-entropy loss')\n",
    "plt.subplot(1, 2, 2), plt.plot(CE_tr, 'b'), plt.plot(CE_val, 'r'), plt.legend(['train', 'val']), plt.title('Classification Error')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dszyXYzEy5jF",
   "metadata": {
    "id": "dszyXYzEy5jF"
   },
   "source": [
    "### 5.2. Network Optimization\n",
    "\n",
    "We cover in this subsection two different aspects about network training using PyTorch:\n",
    "\n",
    "   * Using ```torch.optim``` allows an easier and more interpretable encoding of neural network training, and opens the door to more sophisticated training algorithms\n",
    "    \n",
    "   * Using **minibatches** can speed up network convergence. The idea of minibatches is that, at each epoch, gradients are evaluated over just a subset of the training input data. Training of the network will normally require more epochs but, as each epoch requires the evaluation of the network output for a smaller subset of training samples, the overall training time is normally reduced significantly.\n",
    "\n",
    "    \n",
    "```torch.optim``` provides two convenient methods for neural network training:\n",
    "   * ```opt.step()``` updates all network parameters using current gradients\n",
    "   * ```opt.zero_grad()``` resets all network parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8R2j1-vF3LtD",
   "metadata": {
    "id": "8R2j1-vF3LtD"
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "train_ds = TensorDataset(X_tr_torch, y_tr_torch)\n",
    "train_dl = DataLoader(train_ds, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7RcDdV1c3StI",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 161691,
     "status": "ok",
     "timestamp": 1636535928502,
     "user": {
      "displayName": "JERONIMO ARENAS GARCIA",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhOssjUa3TkcZfTRodRH00CbPGuyuJ_qQjeMOwM=s64",
      "userId": "13981211833123710399"
     },
     "user_tz": -60
    },
    "id": "7RcDdV1c3StI",
    "outputId": "3c056c86-d364-4b39-cbbb-fe82cb496299"
   },
   "outputs": [],
   "source": [
    "from torch import optim\n",
    "\n",
    "my_net = my_multiclass_net(X_tr_torch.size()[1], y_tr_torch.size()[1])\n",
    "opt = optim.SGD(my_net.parameters(), lr=0.1)\n",
    "\n",
    "epochs = 150\n",
    "\n",
    "loss_tr = np.zeros(epochs)\n",
    "loss_val = np.zeros(epochs)\n",
    "CE_tr = np.zeros(epochs)\n",
    "CE_val = np.zeros(epochs)\n",
    "\n",
    "start = time.time()\n",
    "for epoch in range(epochs):\n",
    "    \n",
    "    print(f'Current epoch: {epoch+1}    \\r', end=\"\")\n",
    "    \n",
    "    # In each epoch we iterate over all minibatches\n",
    "    for xb, yb in train_dl:\n",
    "        \n",
    "        #Compute network output and cross-entropy loss for current minibatch\n",
    "        pred = my_net(xb)\n",
    "        loss = loss_func(pred, yb.argmax(axis=-1))\n",
    "    \n",
    "        #Compute gradients and optimize parameters\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        opt.zero_grad()\n",
    "    \n",
    "    #At the end of each epoch, evaluate overall network performance\n",
    "    with torch.no_grad():\n",
    "        #Computing network performance after iteration\n",
    "        pred = my_net(X_tr_torch)\n",
    "        loss_tr[epoch] = loss_func(pred, y_tr_torch.argmax(axis=-1)).item()\n",
    "        CE_tr[epoch] = CE(y_tr_torch, pred).item()\n",
    "        pred_val = my_net(X_val_torch)\n",
    "        loss_val[epoch] = loss_func(pred_val, y_val_torch.argmax(axis=-1)).item()\n",
    "        CE_val[epoch] = CE(y_val_torch, pred_val).item()\n",
    "\n",
    "print('Neural Network training completed in', time.time()-start, 'seconds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28-vTecv4I4U",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 344
    },
    "executionInfo": {
     "elapsed": 618,
     "status": "ok",
     "timestamp": 1636535932430,
     "user": {
      "displayName": "JERONIMO ARENAS GARCIA",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhOssjUa3TkcZfTRodRH00CbPGuyuJ_qQjeMOwM=s64",
      "userId": "13981211833123710399"
     },
     "user_tz": -60
    },
    "id": "28-vTecv4I4U",
    "outputId": "c7ca8edd-662c-46ba-d9cf-5ef3d890fb58"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(14,5))\n",
    "plt.subplot(1, 2, 1), plt.plot(loss_tr, 'b'), plt.plot(loss_val, 'r'), plt.legend(['train', 'val']), plt.title('Cross-entropy loss')\n",
    "plt.subplot(1, 2, 2), plt.plot(100*CE_tr, 'b'), plt.plot(100*CE_val, 'r'), plt.legend(['train', 'val']), plt.title('Classification Error (%)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "nW1ff_rl6xrS",
   "metadata": {
    "id": "nW1ff_rl6xrS"
   },
   "source": [
    "Comparing this figures to those for the conventional gradient descent method, we can extract a number of conclusions:\n",
    "\n",
    "   * Convergence is radically faster when using SGD with minibatches. Note that just after the first epoch the error (both in terms of loss function and CE) is already smaller than that achieved with conventional gradient descent\n",
    "\n",
    "   * In this case, the error in the validation set starts increasing slightly after a number of epochs. I.e., even for a linear classifier overfitting may occur given the high dimensionality of the input data\n",
    "\n",
    "   * Note that the final classification error is much larger than that observed in Section 3; however keep in mind that the network we have just implemented is constrained to linear classification."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "gOiouzxP4pJ8",
   "metadata": {
    "id": "gOiouzxP4pJ8"
   },
   "source": [
    "**Exercise 5.2:** Implement network training with other optimization methods. You can refer to the <a href=\"https://pytorch.org/docs/stable/optim.html\">official documentation</a> and select a couple of methods. You can also try to implement adaptive learning rates using ```torch.optim.lr_scheduler```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "gdHjKB0h41Bd",
   "metadata": {
    "id": "gdHjKB0h41Bd"
   },
   "source": [
    "### 5.3. Multi Layer networks using ```nn.Sequential```\n",
    "\n",
    "As we have seen, PyTorch simplifies considerably the implementation of neural network training, since we do not need to implement derivatives ourselves.\n",
    "\n",
    "We can also make a simpler implementation of multilayer networks using ```nn.Sequential``` function. It returns directly a network with the requested topology, including parameters **and forward evaluation method**\n",
    "\n",
    "For instance, the cell below defines a Network with two hidden layers with 200 and 100 units for the resolution of the MNIST multiclass problem. `Relu` activation is used at the output of the neurons of the hidden layers.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "GTuNBX-hDlOB",
   "metadata": {
    "id": "GTuNBX-hDlOB"
   },
   "outputs": [],
   "source": [
    "my_MLP_net = nn.Sequential(\n",
    "    nn.Linear(X_tr_torch.size()[1], 200),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(200,100),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(100,y_tr_torch.size()[1])\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94KgxxakD6ai",
   "metadata": {
    "id": "94KgxxakD6ai"
   },
   "source": [
    "**Exercise 5.3:** Train the MLP network we have just defined on the MNIST dataset using the following settings:\n",
    "   * Loss function: ```nn.CrossEntropyLoss()```\n",
    "   * Optimization algorithm: ```optim.Adam()``` with learning rate `1e-4`\n",
    "   * Minibatch size: 256\n",
    "   * Number of epochs: 100\n",
    "\n",
    "Calculate the time required to train the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "zgM92ESLD17u",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 227246,
     "status": "ok",
     "timestamp": 1636537246371,
     "user": {
      "displayName": "JERONIMO ARENAS GARCIA",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhOssjUa3TkcZfTRodRH00CbPGuyuJ_qQjeMOwM=s64",
      "userId": "13981211833123710399"
     },
     "user_tz": -60
    },
    "id": "zgM92ESLD17u",
    "outputId": "15fe6019-396c-4206-fd95-65f8d0c16347"
   },
   "outputs": [],
   "source": [
    "# <SOL>\n",
    "# </SOL>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "k5z32MAcGCdx",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 344
    },
    "executionInfo": {
     "elapsed": 863,
     "status": "ok",
     "timestamp": 1636537255563,
     "user": {
      "displayName": "JERONIMO ARENAS GARCIA",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhOssjUa3TkcZfTRodRH00CbPGuyuJ_qQjeMOwM=s64",
      "userId": "13981211833123710399"
     },
     "user_tz": -60
    },
    "id": "k5z32MAcGCdx",
    "outputId": "9e313854-0e47-4d81-cb1e-f612c0578480"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(14,5))\n",
    "plt.subplot(1, 2, 1), plt.plot(loss_tr, 'b'), plt.plot(loss_val, 'r'), plt.legend(['train', 'val']), plt.title('Cross-entropy loss')\n",
    "plt.subplot(1, 2, 2), plt.plot(100*CE_tr, 'b'), plt.plot(100*CE_val, 'r'), plt.legend(['train', 'val']), plt.title('Classification Error (%)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "CiAum3xjKUmP",
   "metadata": {
    "id": "CiAum3xjKUmP"
   },
   "source": [
    "**Exercise 5.4:** Modify your code so that network training is done using GPUs. Compare the training time when using CPU and GPU implementations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "KXsDpKmLGCg_",
   "metadata": {
    "id": "KXsDpKmLGCg_"
   },
   "outputs": [],
   "source": [
    "# Network, training and validation data should be moved to GPU.\n",
    "# You can do that with .to(device) or .cuda() functions\n",
    "\n",
    "my_MLP_cuda = nn.Sequential(\n",
    "    nn.Linear(X_tr_torch.size()[1], 200),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(200,100),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(100,y_tr_torch.size()[1])\n",
    ").cuda()\n",
    "\n",
    "X_tr_cuda = X_tr_torch.to(device)\n",
    "X_val_cuda = X_val_torch.to(device)\n",
    "y_tr_cuda = y_tr_torch.to(device)\n",
    "y_val_cuda = y_val_torch.to(device)\n",
    "\n",
    "# Adapt training code\n",
    "# <SOL>\n",
    "# </SOL>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "-6r1TEaZOvuw",
   "metadata": {
    "id": "-6r1TEaZOvuw"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(14,5))\n",
    "plt.subplot(1, 2, 1), plt.plot(loss_tr, 'b'), plt.plot(loss_val, 'r'), plt.legend(['train', 'val']), plt.title('Cross-entropy loss')\n",
    "plt.subplot(1, 2, 2), plt.plot(100*CE_tr, 'b'), plt.plot(100*CE_val, 'r'), plt.legend(['train', 'val']), plt.title('Classification Error (%)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eETYknXzwIgi",
   "metadata": {
    "id": "eETYknXzwIgi"
   },
   "source": [
    "<font color='red'>Important: This is the end of material currently available for Neural Networks. For next years, we should extend in the following directions (at least): 1/ Generalización mediante distintas estrategias: otras funciones de coste, dropout, early stopping, términos de regularización, etc; 2/ La continuación natural para este notebook sería seguir con redes convolucionales, para lo cual podríamos seguir el tutorial que listo más abajo. Los alumnos podrían revisar también qué prestaciones han alcanzado en este dataset otras estrategias... </font>\n",
    "\n",
    "### 5.4. Generalization\n",
    "\n",
    "* For complex network topologies (i.e., many parameters), network training can incur in over-fitting issues\n",
    "\n",
    "* Some common strategies to avoid this are:\n",
    "\n",
    "    - Early stopping\n",
    "    - Dropout regularization\n",
    "    \n",
    "<center><a href=\"https://medium.com/analytics-vidhya/a-simple-introduction-to-dropout-regularization-with-code-5279489dda1e\"><img src=\"./figures/dropout.png\" />Image Source</a></center>\n",
    "\n",
    "* Data augmentation can also be used to avoid overfitting, as well as to achieve improved accuracy by providing the network some a priori expert knowledge\n",
    "    - E.g., if image rotations and scalings do not affect the correct class, we could enlarge the dataset by creating artificial images with these transformations\n",
    "\n",
    "\n",
    "## 6. Convolutional Neural Networks\n",
    "\n",
    "* [Tutorial que podría sernos de utilidad](https://adventuresinmachinelearning.com/convolutional-neural-networks-tutorial-in-pytorch/)\n",
    "* [Página web de MNIST en Wikipedia](https://en.wikipedia.org/wiki/MNIST_database#Classifiers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Ui9Wr4saBJDf",
   "metadata": {
    "id": "Ui9Wr4saBJDf"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Hand_Digit_with_NN_professor.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
