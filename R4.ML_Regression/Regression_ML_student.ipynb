{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Parametric Model-Based regression\n",
    "\n",
    "    Notebook version: 1.3 (Sep 20, 2019)\n",
    "\n",
    "    Author: Jesús Cid-Sueiro (jesus.cid@uc3m.es)\n",
    "            Jerónimo Arenas García (jarenas@tsc.uc3m.es)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "    Changes: v.1.0 - First version, expanding some cells from the Bayesian Regression \n",
    "                     notebook\n",
    "             v.1.1 - Python 3 version.\n",
    "             v.1.2 - Revised presentation. \n",
    "             v.1.3 - Updated index notation\n",
    "    \n",
    "    Pending changes: * Include regression on the stock data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Import some libraries that will be necessary for working with data and displaying plots\n",
    "\n",
    "# To visualize plots in the notebook\n",
    "%matplotlib inline \n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import scipy.io       # To read matlab files\n",
    "import pylab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## A quick note on the mathematical notation\n",
    "\n",
    "In this notebook we will make extensive use of probability distributions. In general, we will use capital leters\n",
    "${\\bf X}$, $S$, $E$ ..., to denote random variables, and lower-case letters ${\\bf x}$, $s$, $\\epsilon$ ..., to denote the values they can take. \n",
    "\n",
    "In general, we will use letter $p$ for probability density functions (pdf). When necessary, we will use, capital subindices to make the random variable explicit. For instance, $p_{{\\bf X}, S}({\\bf x}, s)$ would be the joint pdf of random variables ${\\bf X}$ and $S$ at values ${\\bf x}$ and $s$, respectively. \n",
    "\n",
    "However, to avoid a notation overload, we will omit subindices when they are clear from the context. For instance, we will use $p({\\bf x}, s)$ instead of $p_{{\\bf X}, S}({\\bf x}, s)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 1. Model-based parametric regression\n",
    "\n",
    "### 1.1. The regression problem\n",
    "\n",
    "Given an observation vector ${\\bf x}$, the goal of the regression problem is to find a function $f({\\bf x})$ providing *good* predictions about some unknown variable $s$. To do so, we assume that a set of *labelled* training examples, $\\{{\\bf x}_k, s_k\\}_{k=0}^{K-1}$ is available. \n",
    "\n",
    "The predictor function should make good predictions for new observations ${\\bf x}$ not used during training. In practice, this is tested using a second set (the *test set*) of labelled samples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 1.2. The underlying model assumption\n",
    "\n",
    "Many regression algorithms are grounded on the idea that all samples from the training set have been generated \n",
    "independently by some common stochastic process.\n",
    "\n",
    "<img src=\"figs/data_model.png\" width=180>\n",
    "\n",
    "If $p({\\bf x}, s)$ were known, we could apply estimation theory to estimate $s$ for a given ${\\bf x}$ using $p$. For instance, we could apply any of the following classical estimates:\n",
    "\n",
    "   * Maximum A Posterior (MAP): $$\\hat{s}_{\\text{MAP}} = \\arg\\max_s p(s| {\\bf x})$$\n",
    "   * Minimum Mean Square Error (MSE): $$\\hat{s}_{\\text{MSE}} = \\mathbb{E}\\{S |{\\bf x}\\} = \\int s \\, p(s| {\\bf x}) \\, ds $$\n",
    "\n",
    "Note that, since these estimators depend on $p(s |{\\bf x})$, knowing the posterior distribution of the target variable is enough, and we do not need to know the joint distribution $p({\\bf x}, s)$.\n",
    "\n",
    "More importantly, note that **if we knew the underlying model, we would not need the data** in ${\\cal D}$ to make predictions on new data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Exercise 1:\n",
    "\n",
    "Assume the target variable $s$ is a scaled noisy version of the input variable $x$: \n",
    "$$\n",
    "s = 2 x + \\epsilon\n",
    "$$\n",
    "where $\\epsilon$ is Gaussian a noise variable with zero mean and unit variance, which does not depend on $x$.\n",
    "\n",
    "  1. Compute the target model $p(s| x)$\n",
    "  2. Compute prediction $\\hat{s}_\\text{MAP}$ for an arbitrary input $x$\n",
    "  3. Compute prediction $\\hat{s}_\\text{MSE}$ for an arbitrary input $x$\n",
    "  4. Compute prediction $\\hat{s}_\\text{MSE}$ for input $x=4$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Solution:\n",
    "\n",
    "[comment]: # (<SOL>)\n",
    "[comment]: # (</SOL>)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 1.3. Model-based regression\n",
    "\n",
    "In practice, the underlying model is usually unknown. \n",
    "\n",
    "Model based-regression methods exploit the idea of using the training data to estimate the posterior distribution $p(s|{\\bf x})$ and then apply estimation theory to make predictions.\n",
    "\n",
    "<img src=\"figs/ModelBasedReg.png\" width=280>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 1.4. Parametric model-based regression\n",
    "\n",
    "In some cases, we may have a partial knowledge about the underlying mode. In this notebook we will assume that $p$ belongs to a parametric family of distributions $p(s|{\\bf x},{\\bf w})$, where ${\\bf w}$ is some unknown parameter. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Exercise 2:\n",
    "\n",
    "Assume the target variable $s$ is a scaled noisy version of the input variable $x$: \n",
    "$$\n",
    "s = w x + \\epsilon\n",
    "$$\n",
    "where $\\epsilon$ is Gaussian a noise variable with zero mean and unit variance, which does not depend on $x$. Assume that $w$ is known. \n",
    "  1. Compute the target model $p(s| x, w)$\n",
    "  2. Compute prediction $\\hat{s}_\\text{MAP}$ for an arbitrary input $x$\n",
    "  3. Compute prediction $\\hat{s}_\\text{MSE}$ for an arbitrary input $x$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Solution:\n",
    "\n",
    "[comment]: # (<SOL>)\n",
    "[comment]: # (</SOL>)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "We will use the training data to estimate ${\\bf w}$\n",
    "\n",
    "<img src=\"figs/ParametricReg.png\" width=300>\n",
    "\n",
    "The estimation of ${\\bf w}$ from a given dataset $\\mathcal{D}$ is the goal of the following sections"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 2. Maximum Likelihood parameter estimation.\n",
    "\n",
    "The ML (Maximum Likelihood) principle is well-known in statistics and can be stated as follows: take the value of the parameter to be estimated (in our case, ${\\bf w}$) that best explains the given observations (in our case, the training dataset $\\mathcal{D}$). Mathematically, this can be expressed as follows:\n",
    "$$\n",
    "\\hat{\\bf w}_{\\text{ML}} = \\arg \\max_{\\bf w} p(\\mathcal{D}|{\\bf w})\n",
    "$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Exercise 3:\n",
    "\n",
    "All samples in dataset ${\\cal D} = \\{(x_k, s_k), k=0,\\ldots,K-1 \\}$ \n",
    "$$\n",
    "s_k = w \\cdot x_k + \\epsilon_k\n",
    "$$\n",
    "where $\\epsilon_k$ are i.i.d. (independent and identically distributed) Gaussian noise random variables with zero mean and unit variance, which do not depend on $x_k$. \n",
    "\n",
    "Compute the ML estimate, $\\hat{w}_{\\text{ML}}$, of $w$.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Solution:\n",
    "\n",
    "[comment]: # (<SOL>)\n",
    "# </SOL>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* **4.2.** Compute the ML estimate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# wML = <FILL IN>\n",
    "\n",
    "print(\"The ML estimate is {}\".format(wML))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* **4.3.** Plot the likelihood as a function of parameter $w$ along the interval $-0.5\\le w \\le 2$, verifying that the ML estimate takes the maximum value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "sigma_eps = 1\n",
    "K = len(s)\n",
    "wGrid = np.arange(-0.5, 2, 0.01)\n",
    "\n",
    "p = []\n",
    "for w in wGrid:\n",
    "    d = s - X*w\n",
    "    # p.append(<FILL IN>)\n",
    "\n",
    "# Compute the likelihood for the ML parameter wML\n",
    "# d = <FILL IN>\n",
    "# pML = [<FILL IN>]\n",
    "\n",
    "# Plot the likelihood function and the optimal value\n",
    "plt.figure()\n",
    "plt.plot(wGrid, p)\n",
    "plt.stem([wML], pML)\n",
    "plt.xlabel('$w$')\n",
    "plt.ylabel('Likelihood function')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* **4.4.** Plot the prediction function on top of the data scatter plot "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "xgrid = np.arange(0, 1.2, 0.01)\n",
    "# sML = <FILL IN>\n",
    "\n",
    "plt.figure()\n",
    "plt.scatter(X, s)\n",
    "# plt.plot(<FILL IN>)\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('s')\n",
    "plt.axis('tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 2.1. Model assumptions\n",
    "\n",
    "In order to solve exercise 4 we have taken advantage of the statistical independence of the noise components. Some independence assumptions are required in general to compute the ML estimate in other scenarios.\n",
    "\n",
    "In order to estimate ${\\bf w}$ from the training data in a mathematicaly rigorous and compact form let us group the target variables into a vector\n",
    "$$\n",
    "{\\bf s} = \\left(s_0, \\dots, s_{K-1}\\right)^\\top\n",
    "$$\n",
    "and the input vectors into a matrix\n",
    "$$\n",
    "{\\bf X} = \\left({\\bf x}_0, \\dots, {\\bf x}_{K-1}\\right)^\\top\n",
    "$$\n",
    "\n",
    "We will make the following assumptions:\n",
    "\n",
    "   * A1. All samples in ${\\cal D}$ have been generated by the same distribution, $p({\\bf x}, s \\mid {\\bf w})$\n",
    "   * A2. Input variables ${\\bf x}$ do not depend on ${\\bf w}$. This implies that\n",
    "$$\n",
    "p({\\bf X} \\mid {\\bf w}) = p({\\bf X})\n",
    "$$\n",
    "   * A3. Targets $s_{0},\\ldots, s_{K-1}$ are statistically independent, given ${\\bf w}$ and the inputs ${\\bf x}_0,\\ldots, {\\bf x}_{K-1}$, that is:\n",
    "$$\n",
    "p({\\bf s} \\mid {\\bf X}, {\\bf w}) = \\prod_{k=0}^{K-1} p(s_k \\mid {\\bf x}_k, {\\bf w})\n",
    "$$\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Since ${\\cal D} = ({\\bf X}, {\\bf s})$, we can write\n",
    "$$p(\\mathcal{D}|{\\bf w}) \n",
    "    = p({\\bf s}, {\\bf X}|{\\bf w}) \n",
    "    = p({\\bf s} | {\\bf X}, {\\bf w}) p({\\bf X}|{\\bf w})\n",
    "$$\n",
    "Using assumption A2,\n",
    "$$\n",
    "p(\\mathcal{D}|{\\bf w}) \n",
    "    = p({\\bf s} | {\\bf X}, {\\bf w}) p({\\bf X})\n",
    "$$\n",
    "\n",
    "and, finally, using assumption A3, we can express the estimation problem as the computation of\n",
    "\n",
    "\\begin{align}\n",
    "\\hat{\\bf w}_{\\text{ML}}\n",
    "                &= \\arg \\max_{\\bf w} p({\\bf s}|{\\bf X},{\\bf w})  \\\\\n",
    "   \\qquad \\quad &= \\arg \\max_{\\bf w} \\prod_{k=0}^{K-1} p(s_k \\mid {\\bf x}_k, {\\bf w})  \\\\\n",
    "   \\qquad \\quad &= \\arg \\max_{\\bf w} \\sum_{k=0}^{K-1}\\log  p(s_k \\mid {\\bf x}_k, {\\bf w})\n",
    "\\end{align}\n",
    "\n",
    "Any of the last three terms can be used to optimize ${\\bf w}$. The sum in the last term is usually called the **log-likelihood** function, $L({\\bf w})$, whereas the product in the previous line is simply referred as the **likelihood** function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 2.2. Summary.\n",
    "\n",
    "Let's summarize what we need to do in order to design a regression algorithm based on ML estimation:\n",
    "\n",
    "1. Assume a parametric data model $p(s| {\\bf x},{\\bf w})$\n",
    "2. Using the data model and the i.i.d. assumption, compute $p({\\bf s}| {\\bf X},{\\bf w})$.\n",
    "3. Find an expression for ${\\bf w}_{\\text{ML}}$\n",
    "4. Assuming ${\\bf w} = {\\bf w}_{\\text{ML}}$, compute the MAP or the minimum MSE estimate of $s$ given ${\\bf x}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 3. ML estimation for a Gaussian model.\n",
    "\n",
    "### 3.1. Step 1: The Gaussian generative model\n",
    "\n",
    "Let us assume that the target variables $s_k$ in dataset $\\mathcal{D}$ are given by\n",
    "$$\n",
    "s_k = {\\bf w}^\\top {\\bf z}_k + \\varepsilon_k\n",
    "$$\n",
    "\n",
    "where ${\\bf z}_k$ is the result of some transformation of the inputs, ${\\bf z}_k = T({\\bf x}_k)$, and $\\varepsilon_k$ are i.i.d. instances of a Gaussian random variable with mean zero and varianze $\\sigma_\\varepsilon^2$, i.e.,\n",
    "$$\n",
    "p_E(\\varepsilon) = \\frac{1}{\\sqrt{2\\pi}\\sigma_\\varepsilon}\n",
    "                   \\exp\\left(-\\frac{\\varepsilon^2}{2\\sigma_\\varepsilon^2}\\right)\n",
    "$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Assuming that the noise variables are independent of ${\\bf x}$ and ${\\bf w}$, then, for a given ${\\bf x}$ and ${\\bf w}$, the target variable is gaussian with mean ${\\bf w}^\\top {\\bf z}_k$ and variance $\\sigma_\\varepsilon^2$\n",
    "$$\n",
    "p(s|{\\bf x}, {\\bf w}) = p_E(s-{\\bf w}^\\top{\\bf z}) =\n",
    "    \\frac{1}{\\sqrt{2\\pi}\\sigma_\\varepsilon}\n",
    "    \\exp\\left(-\\frac{(s-{\\bf w}^\\top{\\bf z})^2}{2\\sigma_\\varepsilon^2}\\right)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### 3.2. Step 2: Likelihood function\n",
    "\n",
    "Now we need to compute the likelihood function $p({\\bf s}, {\\bf X} | {\\bf w})$. If the samples are i.i.d. we can write\n",
    "$$\n",
    "p({\\bf s}| {\\bf X}, {\\bf w})\n",
    "    = \\prod_{k=0}^{K-1} p(s_k| {\\bf x}_k, {\\bf w}) \n",
    "    = \\prod_{k=0}^{K-1} \\frac{1}{\\sqrt{2\\pi}\\sigma_\\varepsilon}\n",
    "    \\exp\\left(-\\frac{\\left(s_k-{\\bf w}^\\top{\\bf z}_k\\right)^2}{2\\sigma_\\varepsilon^2}\\right) \\\\\n",
    "    = \\left(\\frac{1}{\\sqrt{2\\pi}\\sigma_\\varepsilon}\\right)^K\n",
    "      \\exp\\left(-\\sum_{k=1}^K \\frac{\\left(s_k-{\\bf w}^\\top{\\bf z}_k\\right)^2}{2\\sigma_\\varepsilon^2}\\right) \\\\\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Finally, grouping variables ${\\bf z}_k$ in\n",
    "$${\\bf Z} = \\left({\\bf z}_0, \\dots, {\\bf z}_{K-1}\\right)^\\top$$\n",
    "we get\n",
    "$$\n",
    "p({\\bf s}| {\\bf X}, {\\bf w})\n",
    "    = \\left(\\frac{1}{\\sqrt{2\\pi}\\sigma_\\varepsilon}\\right)^K\n",
    "      \\exp\\left(-\\frac{1}{2\\sigma_\\varepsilon^2}\\|{\\bf s}-{\\bf Z}{\\bf w}\\|^2\\right)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### 3.3. Step 3: ML estimation.\n",
    "\n",
    "The <b>maximum likelihood</b> solution is then given by:\n",
    "$$\n",
    "{\\bf w}_{ML} = \\arg \\max_{\\bf w} p({\\bf s}|{\\bf w}) = \\arg \\min_{\\bf w} \\|{\\bf s} - {\\bf Z}{\\bf w}\\|^2\n",
    "$$\n",
    "Note that $\\|{\\bf s} - {\\bf Z}{\\bf w}\\|^2$ is the sum or the squared prediction errors (Sum of Squared Errors, SSE) for all samples in the dataset. This is also called the * **Least Squares** * (LS) solution.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "The LS solution can be easily computed by differentiation,\n",
    "$$\n",
    "\\nabla_{\\bf w} \\|{\\bf s} - {\\bf Z}{\\bf w}\\|^2\\Bigg|_{{\\bf w} = {\\bf w}_\\text{ML}}\n",
    "= - 2 {\\bf Z}^\\top{\\bf s} + 2 {\\bf Z}^\\top{\\bf Z} {\\bf w}_{\\text{ML}}\n",
    "= {\\bf 0}\n",
    "$$\n",
    "and it is equal to\n",
    "$$\n",
    "{\\bf w}_\\text{ML} = ({\\bf Z}^\\top{\\bf Z})^{-1}{\\bf Z}^\\top{\\bf s}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### 3.4. Step 4: Prediction function.\n",
    "\n",
    "The last step consists on computing an estimate of $s$ by assuming that the true value of the weight parameters is ${\\bf w}_\\text{ML}$. In particular, the minimum MSE estimate is\n",
    "$$\n",
    "\\hat{s}_\\text{MSE} = \\mathbb{E}\\{s|{\\bf x},{\\bf w}_\\text{ML}\\}\n",
    "$$\n",
    "\n",
    "Knowing that, given ${\\bf x}$ and ${\\bf w}$, $s$ is normally distributed with mean ${\\bf w}^\\top {\\bf z}$ we can write\n",
    "$$\n",
    "\\hat{s}_\\text{MSE} = {\\bf w}_\\text{ML}^\\top {\\bf z}\n",
    "$$\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Exercise 5:\n",
    "\n",
    "Assume that the targets in the one-dimensional dataset given by"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "X = np.array([0.15, 0.41, 0.53, 0.80, 0.89, 0.92, 0.95]) \n",
    "s = np.array([0.09, 0.16, 0.63, 0.44, 0.55, 0.82, 0.95]) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "have been generated by the polynomial Gaussian model \n",
    "$$\n",
    "s = w_0 + w_1 x + w_2 x^2 + \\epsilon\n",
    "$$\n",
    "(i.e., with ${\\bf z} = T(x) = (1, x, x^2)^\\intercal$) with noise variance \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "sigma_eps = 0.3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* **5.1.** Compute the ML estimate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Compute the extended input matrix Z\n",
    "nx = len(X)\n",
    "# Z = <FILL IN>\n",
    "\n",
    "# Compute the ML estimate using linalg.lstsq from Numpy.\n",
    "# wML = <FILL IN>\n",
    "print(wML)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* **5.2.** Compute the value of the log-likelihood function for ${\\bf w}={\\bf w}_\\text{ML}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "K = len(s)\n",
    "\n",
    "# Compute the likelihood for the ML parameter wML\n",
    "# d = <FILL IN>\n",
    "\n",
    "# LwML = [<FILL IN>]\n",
    "\n",
    "print(LwML)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* **5.3.** Plot the prediction function over the data scatter plot "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "xgrid = np.arange(0, 1.2, 0.01)\n",
    "nx = len(xgrid)\n",
    "# Compute the input matrix for the grid data in x\n",
    "# Z = <FILL IN>\n",
    "\n",
    "# sML = <FILL IN>\n",
    "\n",
    "plt.figure()\n",
    "plt.scatter(X, s)\n",
    "# plt.plot(<FILL IN>)\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('s')\n",
    "plt.axis('tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Exercise 6:\n",
    "\n",
    "Assume the dataset $\\mathcal{D} = \\{(x_k, s_k, k=0,\\ldots, K-1\\}$ contains i.i.d. samples from a distribution with posterior density given by\n",
    "$$\n",
    "p(s \\mid x, w) = w x \\exp(- w x s), \\qquad s\\ge0, \\,\\, x\\ge 0, \\,\\, w\\ge 0\n",
    "$$\n",
    "\n",
    "* **6.1.** Determine an expression for the likelihood function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "\n",
    "**Solution**:\n",
    "<SOL>\n",
    "</SOL>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* **6.2.** Draw the likelihood function for the dataset in **Exercise 4** in the range $0\\le w\\le 6$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "K = len(s)\n",
    "wGrid = np.arange(0, 6, 0.01)\n",
    "\n",
    "p = []\n",
    "Px = np.prod(X)\n",
    "xs = np.dot(X,s)\n",
    "for w in wGrid:\n",
    "    # p.append(<FILL IN>)\n",
    "\n",
    "plt.figure()\n",
    "# plt.plot(<FILL IN>)\n",
    "plt.xlabel('$w$')\n",
    "plt.ylabel('Likelihood function')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* **6.3.** Determine the maximum likelihood coefficient, $w_\\text{ML}$. \n",
    "\n",
    "(*Hint: you can maximize the log-likelihood function instead of the likelihood function in order to simplify the differentiation*)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "**Solution**:\n",
    "\n",
    "<SOL>\n",
    "</SOL>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* **6.4.** Compute $w_\\text{ML}$ for the dataset in **Exercise 4**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# wML = <FILL IN>\n",
    "print(wML)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* **6.5.** Assuming $w = w_\\text{ML}$, compute the prediction function based on the estimate $s_{MSE}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "**Solution**:\n",
    "\n",
    "<SOL>\n",
    "</SOL>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* **6.6.** Plot the prediction function obtained in ap. 6.5, and compare it with the linear predictor in exercise 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "xgrid = np.arange(0.1, 1.2, 0.01)\n",
    "# sML = <FILL IN>\n",
    "\n",
    "plt.figure()\n",
    "plt.scatter(X, s)\n",
    "# plt.plot(<FILL IN>)\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('s')\n",
    "plt.axis('tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Subjectively, we can see that the predictor computed in exercise 6 does not fit the given data very well. This could be a false perception. If the data have been truly generated by the parametric model assumed in exercise 6 (i.e. $p(s \\mid x, w) = w x \\exp(- w x s)$, the apparent missbehavior of the estimator could be caused by the natural randomness of the data, and a greater amount of data would show a better adjustement. \n",
    "\n",
    "Alternative, it may be the case the model assumed in sec. 6 is incorrect. Again, more data would be useful to asses that.\n",
    "\n",
    "This shows that the choice of the data model is important. In many applications, no parametric data model is available, and the data scientist must make a choice based on the nature of the data or any previous knowledge about the statistical behavior of the data. \n",
    "\n",
    "If no previous information is available, the data scientist can try different models, and compare using validation data and some cross validation technique.\n"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
