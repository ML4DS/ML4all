{\rtf1\ansi\ansicpg1252\cocoartf1404\cocoasubrtf130
{\fonttbl\f0\fmodern\fcharset0 Courier;\f1\fnil\fcharset0 HelveticaNeue;\f2\fmodern\fcharset0 Courier-Oblique;
\f3\fmodern\fcharset0 Courier-Bold;\f4\fswiss\fcharset0 Helvetica;}
{\colortbl;\red255\green255\blue255;\red0\green0\blue109;\red0\green0\blue0;\red255\green255\blue255;
\red41\green101\blue168;\red51\green110\blue109;\red245\green245\blue245;\red38\green38\blue38;\red83\green83\blue83;
\red15\green112\blue1;\red0\green0\blue255;\red169\green14\blue26;}
{\*\listtable{\list\listtemplateid1\listhybrid{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{disc\}}{\leveltext\leveltemplateid1\'01\uc0\u8226 ;}{\levelnumbers;}\fi-360\li720\lin720 }{\listname ;}\listid1}
{\list\listtemplateid2\listhybrid{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{disc\}}{\leveltext\leveltemplateid101\'01\uc0\u8226 ;}{\levelnumbers;}\fi-360\li720\lin720 }{\listname ;}\listid2}}
{\*\listoverridetable{\listoverride\listid1\listoverridecount0\ls1}{\listoverride\listid2\listoverridecount0\ls2}}
{\info
{\title Pract_regression}}\paperw12240\paperh15840\vieww30140\viewh20800\viewkind0
\deftab720
\pard\pardeftab720\sl340\qr\partightenfactor0

\f0\fs28 \cf2 \expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec2 \
\pard\pardeftab720\sl500\partightenfactor0

\f1\b\fs52 \cf3 \cb4 \strokec3 Bayesian and Gaussian Process regression\cf5 \strokec5 \'b6\cf3 \cb1 \strokec3 \
\pard\pardeftab720\sl400\partightenfactor0

\f0\b0\fs28 \cf3 \cb4 Notebook version: 1.0 (Oct 16, 2015)\
\
Authors: Miguel L\'e1zaro Gredilla\
         Jer\'f3nimo Arenas Garc\'eda (jarenas@tsc.uc3m.es)\
\
Changes: v.1.0 - First version. Python version\
\
Pending changes: \
\pard\pardeftab720\sl340\qr\partightenfactor0
\cf2 \strokec2 In\'a0[1]:\cb1 \
\pard\pardeftab720\sl340\partightenfactor0

\f2\i \cf6 \cb7 \strokec6 # Import some libraries that will be necessary for working with data and displaying plots
\f0\i0 \cf8 \strokec8 \
\

\f2\i \cf6 \strokec6 # To visualize plots in the notebook
\f0\i0 \cf8 \strokec8 \
\pard\pardeftab720\sl340\partightenfactor0
\cf9 \strokec9 %
\f3\b \cf10 \strokec10 matplotlib
\f0\b0 \cf8 \strokec8  inline \
\
\pard\pardeftab720\sl340\partightenfactor0

\f3\b \cf10 \strokec10 import
\f0\b0 \cf8 \strokec8  
\f3\b \cf11 \strokec11 matplotlib
\f0\b0 \cf8 \strokec8 \

\f3\b \cf10 \strokec10 import
\f0\b0 \cf8 \strokec8  
\f3\b \cf11 \strokec11 matplotlib.pyplot
\f0\b0 \cf8 \strokec8  
\f3\b \cf10 \strokec10 as
\f0\b0 \cf8 \strokec8  
\f3\b \cf11 \strokec11 plt
\f0\b0 \cf8 \strokec8 \

\f3\b \cf10 \strokec10 import
\f0\b0 \cf8 \strokec8  
\f3\b \cf11 \strokec11 numpy
\f0\b0 \cf8 \strokec8  
\f3\b \cf10 \strokec10 as
\f0\b0 \cf8 \strokec8  
\f3\b \cf11 \strokec11 np
\f0\b0 \cf8 \strokec8 \

\f3\b \cf10 \strokec10 import
\f0\b0 \cf8 \strokec8  
\f3\b \cf11 \strokec11 scipy.io
\f0\b0 \cf8 \strokec8        
\f2\i \cf6 \strokec6 # To read matlab files
\f0\i0 \cf8 \strokec8 \

\f3\b \cf10 \strokec10 from
\f0\b0 \cf8 \strokec8  
\f3\b \cf11 \strokec11 scipy
\f0\b0 \cf8 \strokec8  
\f3\b \cf10 \strokec10 import
\f0\b0 \cf8 \strokec8  spatial\

\f3\b \cf10 \strokec10 import
\f0\b0 \cf8 \strokec8  
\f3\b \cf11 \strokec11 pylab
\f0\b0 \cf8 \strokec8 \
pylab\cf9 \strokec9 .\cf8 \strokec8 rcParams[\cf12 \strokec12 'figure.figsize'\cf8 \strokec8 ] \cf9 \strokec9 =\cf8 \strokec8  \cf9 \strokec9 15\cf8 \strokec8 , \cf9 \strokec9 10\cf8 \strokec8 \
\pard\pardeftab720\sl340\qr\partightenfactor0
\cf2 \cb1 \strokec2 \
\pard\pardeftab720\sl500\partightenfactor0

\f1\b\fs52 \cf3 \cb4 \strokec3 1. Introduction\cf5 \strokec5 \'b6\cf3 \cb1 \strokec3 \
\pard\pardeftab720\sl400\partightenfactor0

\b0\fs28 \cf3 \cb4 In this exercise the student will review several key concepts of Bayesian regression and Gaussian processes.\cb1 \
\cb4 For the purpose of this exercise, the regression model is\cb1 \
\pard\pardeftab720\sl400\partightenfactor0
\cf3 \cb4 $$\{s\}(\{\\bf x\}) = f(\{\\bf x\}) + \\varepsilon$$\
\pard\pardeftab720\sl400\partightenfactor0
\cf3 where $\{s\}(\{\\bf x\})$ is the output corresponding to input $\{\\bf x\}$, $f(\{\\bf x\})$ is the unobservable latent function, and $\\varepsilon$ is white zero-mean Gaussian noise, i.e., $\\varepsilon \\sim \{\\cal N\}(0,\\sigma_\\varepsilon^2)$.\cb1 \
\pard\pardeftab720\sl340\qr\partightenfactor0

\f0 \cf2 \strokec2 \
\pard\pardeftab720\sl360\partightenfactor0

\f1\b\fs36 \cf3 \cb4 \strokec3 Practical considerations\cf5 \strokec5 \'b6\cf3 \cb1 \strokec3 \
\pard\tx220\tx720\pardeftab720\li720\fi-720\sl400\partightenfactor0
\ls1\ilvl0
\b0\fs28 \cf3 \cb4 \kerning1\expnd0\expndtw0 \outl0\strokewidth0 {\listtext	\'95	}\expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec3 Though sometimes unavoidable, it is recommended not to use explicit matrix inversion whenever possible. For instance, if an operation like $\{\\mathbf A\}^\{-1\} \{\\mathbf b\}$ must be performed, it is preferable to code it using python $\\mbox\{numpy.linalg.lstsq\}$ function (see {\field{\*\fldinst{HYPERLINK "http://docs.scipy.org/doc/numpy/reference/generated/numpy.linalg.lstsq.html"}}{\fldrslt \cf5 \ul \ulc5 \strokec5 http://docs.scipy.org/doc/numpy/reference/generated/numpy.linalg.lstsq.html}}), which provides the LS solution to the overdetermined system $\{\\mathbf A\} \{\\mathbf w\} = \{\\mathbf b\}$.\cb1 \uc0\u8232 \
\ls1\ilvl0\cb4 \kerning1\expnd0\expndtw0 \outl0\strokewidth0 {\listtext	\'95	}\expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec3 Sometimes, the computation of $\\log|\{\\mathbf A\}|$ (where $\{\\mathbf A\}$ is a positive definite matrix) can overflow available precision, producing incorrect results. A numerically more stable alternative, providing the same result is $2\\sum_i \\log([\{\\mathbf L\}]_\{ii\})$, where $\\mathbf L$ is the Cholesky decomposition of $\\mathbf A$ (i.e., $\{\\mathbf A\} = \{\\mathbf L\}^\\top \{\\mathbf L\}$), and $[\{\\mathbf L\}]_\{ii\}$ is the $i$th element of the diagonal of $\{\\mathbf L\}$.\cb1 \uc0\u8232 \
\ls1\ilvl0\cb4 \kerning1\expnd0\expndtw0 \outl0\strokewidth0 {\listtext	\'95	}\expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec3 Non-degenerate covariance matrices, such as the ones in this exercise, are always positive definite. It may happen, as a consequence of chained rounding errors, that a matrix which was mathematically expected to be positive definite, turns out not to be so. This implies its Cholesky decomposition will not be available. A quick way to palliate this problem is by adding a small number (such as $10^\{-6\}$) to the diagonal of such matrix.\cb1 \uc0\u8232 \
\pard\pardeftab720\sl340\qr\partightenfactor0

\f0 \cf2 \strokec2 \
\pard\pardeftab720\sl360\partightenfactor0

\f1\b\fs36 \cf3 \cb4 \strokec3 Reproducibility of computations\cf5 \strokec5 \'b6\cf3 \cb1 \strokec3 \
\pard\pardeftab720\sl400\partightenfactor0

\b0\fs28 \cf3 \cb4 To guarantee the exact reproducibility of the experiments, it may be useful to start your code initializing the seed of the random numbers generator, so that you can compare your results with the ones given in this notebook.\cb1 \
\pard\pardeftab720\sl340\qr\partightenfactor0

\f0 \cf2 \cb4 \strokec2 In\'a0[2]:\cb1 \
\pard\pardeftab720\sl340\partightenfactor0
\cf8 \cb7 \strokec8 np\cf9 \strokec9 .\cf8 \strokec8 random\cf9 \strokec9 .\cf8 \strokec8 seed(\cf9 \strokec9 3\cf8 \strokec8 )\
\pard\pardeftab720\sl340\qr\partightenfactor0
\cf2 \cb1 \strokec2 \
\pard\pardeftab720\sl500\partightenfactor0

\f1\b\fs52 \cf3 \cb4 \strokec3 2. Bayesian regression with a linear model\cf5 \strokec5 \'b6\cf3 \cb1 \strokec3 \
\pard\pardeftab720\sl400\partightenfactor0

\b0\fs28 \cf3 \cb4 During this section, we will assume the following parametric model for the latent function\cb1 \
\pard\pardeftab720\sl400\partightenfactor0
\cf3 \cb4 $$f(\{\\bf x\}) = \{\\bf x\}^\\top \{\\bf w\}$$\
\pard\pardeftab720\sl400\partightenfactor0
\cf3 i.e., a linear model in the observations, where $\{\\bf w\}$ contains the parameters of the model. The 
\i a priori
\i0  distribution of $\{\\bf w\}$ is assumed to be\cb1 \
\pard\pardeftab720\sl400\partightenfactor0
\cf3 \cb4 $$\{\\bf w\} \\sim \{\\cal N\}(\{\\bf 0\}, \\sigma_0^2~\{\\bf I\})$$ \cb1 \
\pard\pardeftab720\sl340\qr\partightenfactor0

\f0 \cf2 \strokec2 \
\pard\pardeftab720\sl420\partightenfactor0

\f1\b\fs44 \cf3 \cb4 \strokec3 2.1. Synthetic data generation\cf5 \strokec5 \'b6\cf3 \cb1 \strokec3 \
\pard\pardeftab720\sl400\partightenfactor0

\b0\fs28 \cf3 \cb4 First, we are going to generate synthetic data (so that we have the ground-truth model) and use them to make sure everything works correctly and our estimations are sensible.\cb1 \
\cb4 Set parameters $\\sigma_0^2 = 2$ and $\\sigma_\{\\varepsilon\}^2 = 0.2$. Generate a weight vector $\\mbox\{true_w\}$ with two elements from the 
\i a priori
\i0  distribution of the weights. This vector determines the regression line that we want to find (i.e., the optimum unknown solution).\cb1 \
\cb4 Generate an input matrix $\\mbox\{X\}$ containing the constant term 1 in all elements of the first column and values between 0 and 2 (included), with a 0.1 step, in the second column.\cb1 \
\cb4 Finally, generate the output vector $\{\\mbox s\}$ as the product $\\mbox\{X\} \\ast \\mbox\{true_w\}$ plus Gaussian noise of pdf $\{\\cal N\}(0,\\sigma_\\varepsilon^2)$ at each element.\cb1 \
\cb4 Plot the generated data. You will notice a linear behavior, but the presence of noise makes it hard to estimate precisely the original straight line that generated them (which is stored in $\\mbox\{true_w\}$).\cb1 \
\pard\pardeftab720\sl340\qr\partightenfactor0

\f0 \cf2 \cb4 \strokec2 In\'a0[3]:\cb1 \
\pard\pardeftab720\sl340\partightenfactor0

\f2\i \cf6 \cb7 \strokec6 # Parameter settings
\f0\i0 \cf8 \strokec8 \
sigma_0 \cf9 \strokec9 =\cf8 \strokec8  np\cf9 \strokec9 .\cf8 \strokec8 sqrt(\cf9 \strokec9 2\cf8 \strokec8 )\
sigma_eps \cf9 \strokec9 =\cf8 \strokec8  np\cf9 \strokec9 .\cf8 \strokec8 sqrt(\cf9 \strokec9 0.2\cf8 \strokec8 )\
\

\f2\i \cf6 \strokec6 #Optimum solution
\f0\i0 \cf8 \strokec8 \
true_w \cf9 \strokec9 =\cf8 \strokec8  \
\

\f2\i \cf6 \strokec6 #Training datapoints
\f0\i0 \cf8 \strokec8 \

\f2\i \cf6 \strokec6 ####################################
\f0\i0 \cf8 \strokec8 \

\f2\i \cf6 \strokec6 ##### Fill in your code here #######
\f0\i0 \cf8 \strokec8 \

\f2\i \cf6 \strokec6 ####################################
\f0\i0 \cf8 \strokec8 \
\

\f2\i \cf6 \strokec6 #Plot training points
\f0\i0 \cf8 \strokec8 \
plt\cf9 \strokec9 .\cf8 \strokec8 scatter(X,s);\
plt\cf9 \strokec9 .\cf8 \strokec8 xlabel(\cf12 \strokec12 'x'\cf8 \strokec8 ,fontsize\cf9 \strokec9 =18\cf8 \strokec8 );\
plt\cf9 \strokec9 .\cf8 \strokec8 ylabel(\cf12 \strokec12 's'\cf8 \strokec8 ,fontsize\cf9 \strokec9 =18\cf8 \strokec8 );\
\pard\pardeftab720\sl340\qr\partightenfactor0
\cf3 \cb1 \strokec3 \
\pard\pardeftab720\sl400\partightenfactor0

\f1 \cf3 {{\NeXTGraphic unknown.png \width18000 \height12280
}¬}\pard\pardeftab720\sl400\partightenfactor0
\cf3 \cb4  \cb1 \
\pard\pardeftab720\sl340\qr\partightenfactor0

\f0 \cf2 \strokec2 \
\pard\pardeftab720\sl420\partightenfactor0

\f1\b\fs44 \cf3 \cb4 \strokec3 2.2. Posterior pdf of the weight vector\cf5 \strokec5 \'b6\cf3 \cb1 \strokec3 \
\pard\pardeftab720\sl400\partightenfactor0

\b0\fs28 \cf3 \cb4 Let us see to which extent it is possible to determine the original straight line from observed data. Knowing that the generative model is linear (i.e., $f(\{\\bf x\}) = \{\\bf x\}^\\top\{\\bf w\}$, and knowing also the prior pdf of weights $p(\{\\bf w\}) = \{\\cal N\}(\{\\bf 0\},\\sigma_0^2~\{\\bf I\})$ and noise $p(\\varepsilon) = \{\\cal N\}(0,\\sigma_\\varepsilon^2)$, compute the posterior pdf of the weights, $p(\{\\bf w\}\\mid\{\\bf s\})$.\cb1 \
\pard\pardeftab720\sl340\qr\partightenfactor0

\f0 \cf2 \cb4 \strokec2 In\'a0[4]:\cb1 \
\pard\pardeftab720\sl340\partightenfactor0
\cf8 \cb7 \strokec8 Cov_w \cf9 \strokec9 =\cf8 \strokec8  \
mean_w \cf9 \strokec9 =\cf8 \strokec8  \
\pard\pardeftab720\sl340\qr\partightenfactor0
\cf2 \cb1 \strokec2 \
\pard\pardeftab720\sl400\partightenfactor0

\f1 \cf3 \cb4 \strokec3 The results is:\cb1 \
\pard\pardeftab720\sl340\qr\partightenfactor0

\f0 \cf2 \cb4 \strokec2 In\'a0[5]:\cb1 \
\pard\pardeftab720\sl340\partightenfactor0

\f3\b \cf10 \cb7 \strokec10 print
\f0\b0 \cf8 \strokec8  \cf12 \strokec12 'true_w = '\cf8 \strokec8  \cf9 \strokec9 +\cf8 \strokec8  \cf10 \strokec10 str\cf8 \strokec8 (true_w)\

\f3\b \cf10 \strokec10 print
\f0\b0 \cf8 \strokec8  \cf12 \strokec12 'mean_w = '\cf8 \strokec8  \cf9 \strokec9 +\cf8 \strokec8  \cf10 \strokec10 str\cf8 \strokec8 (mean_w)\

\f3\b \cf10 \strokec10 print
\f0\b0 \cf8 \strokec8  \cf12 \strokec12 'Cov_w = '\cf8 \strokec8  \cf9 \strokec9 +\cf8 \strokec8  \cf10 \strokec10 str\cf8 \strokec8 (Cov_w)\
\pard\pardeftab720\sl340\qr\partightenfactor0
\cf3 \cb1 \strokec3 \
\pard\pardeftab720\sl340\partightenfactor0
\cf3 \cb4 true_w = [ 2.52950265  0.61731815]\
mean_w = [ 2.29909556  0.75291393]\
Cov_w = [[ 0.03455724 -0.02519798]\
 [-0.02519798  0.02531797]]\
\pard\pardeftab720\sl340\qr\partightenfactor0
\cf2 \cb1 \strokec2 \
\pard\pardeftab720\sl420\partightenfactor0

\f1\b\fs44 \cf3 \cb4 \strokec3 2.3. Sampling regression curves from the posterior\cf5 \strokec5 \'b6\cf3 \cb1 \strokec3 \
\pard\pardeftab720\sl400\partightenfactor0

\b0\fs28 \cf3 \cb4 Plot now the functions corresponding to different samples drawn from the posterior distribution of the weight vector. To this end, generate random vectors $\{\\bf w\}_l$ with $l = 1,\\dots, 50$, from the posterior density of the weights, $p(\{\\bf w\}\\mid\{\\bf s\})$, and use them to generate 50 straight lines, $f(\{\\bf x\}^\\ast) = \{\{\\bf x\}^\\ast\}^\\top \{\\bf w\}_l$, with the second component of $\{\\bf x\}^\\ast$ between $-1$ and $3$, with step $0.1$.\cb1 \
\cb4 Plot the original ground-truth straight line, corresponding to $\\mbox\{true_w\}$, along with the $50$ generated straight lines and the original samples, all in the same plot. As you can check, the Bayesian model is not providing a single answer, but instead a density over them, from which we have extracted 50 options.\cb1 \
\pard\pardeftab720\sl340\qr\partightenfactor0

\f0 \cf2 \cb4 \strokec2 In\'a0[6]:\cb1 \
\pard\pardeftab720\sl340\partightenfactor0
\cf8 \cb7 \strokec8  \
\pard\pardeftab720\sl340\qr\partightenfactor0
\cf3 \cb1 \strokec3 \
\pard\pardeftab720\sl400\partightenfactor0

\f1 \cf3 {{\NeXTGraphic 1__#$!@%!#__unknown.png \width17800 \height12280
}¬}\pard\pardeftab720\sl400\partightenfactor0
\cf3 \cb4  \cb1 \
\pard\pardeftab720\sl340\qr\partightenfactor0

\f0 \cf2 \strokec2 \
\pard\pardeftab720\sl420\partightenfactor0

\f1\b\fs44 \cf3 \cb4 \strokec3 2.4. Plotting the confidence intervals\cf5 \strokec5 \'b6\cf3 \cb1 \strokec3 \
\pard\pardeftab720\sl400\partightenfactor0

\b0\fs28 \cf3 \cb4 On top of the previous figure (copy here your code from the previous section), plot functions\cb1 \
\pard\pardeftab720\sl400\partightenfactor0
\cf3 \cb4 $$\{\\mathbb E\}\\left\\\{f(\{\\bf x\}^\\ast)\\mid\{\\bf s\}\\right\\\}$$\
\pard\pardeftab720\sl400\partightenfactor0
\cf3 and\cb1 \
\pard\pardeftab720\sl400\partightenfactor0
\cf3 \cb4 $$\{\\mathbb E\}\\left\\\{f(\{\\bf x\}^\\ast)\\mid\{\\bf s\}\\right\\\} \\pm 2 \\sqrt\{\{\\mathbb V\}\\left\\\{f(\{\\bf x\}^\\ast)\\mid\{\\bf s\}\\right\\\}\}$$\
\pard\pardeftab720\sl400\partightenfactor0
\cf3 (i.e., the posterior mean of $f(\{\\bf x\}^\\ast)$, as well as two standard deviations above and below).\cb1 \
\cb4 It is possible to show analytically that this region comprises $95.45\\%$ probability of the posterior probability $p(f(\{\\bf x\}^\\ast)\\mid \{\\bf s\})$ at each $\{\\bf x\}^\\ast$.\cb1 \
\pard\pardeftab720\sl340\qr\partightenfactor0

\f0 \cf2 \cb4 \strokec2 In\'a0[7]:\cb1 \
\pard\pardeftab720\sl340\partightenfactor0
\cf8 \cb7 \strokec8  \
\pard\pardeftab720\sl340\qr\partightenfactor0
\cf3 \cb1 \strokec3 \
\pard\pardeftab720\sl400\partightenfactor0

\f1 \cf3 {{\NeXTGraphic 2__#$!@%!#__unknown.png \width17800 \height12280
}¬}\pard\pardeftab720\sl400\partightenfactor0
\cf3 \cb4  \cb1 \
\pard\pardeftab720\sl340\qr\partightenfactor0

\f0 \cf2 \strokec2 \
\pard\pardeftab720\sl400\partightenfactor0

\f1 \cf3 \cb4 \strokec3 Plot now $\{\\mathbb E\}\\left\\\{s(\{\\bf x\}^\\ast)\\mid\{\\bf s\}\\right\\\} \\pm 2 \\sqrt\{\{\\mathbb V\}\\left\\\{s(\{\\bf x\}^\\ast)\\mid\{\\bf s\}\\right\\\}\}$ (note that the posterior means of $f(\{\\bf x\}^\\ast)$ and $s(\{\\bf x\}^\\ast)$ are the same, so there is no need to plot it again). Notice that $95.45\\%$ of observed data lie now within the newly designated region. These new limits establish a confidence range for our predictions. See how the uncertainty grows as we move away from the interpolation region to the extrapolation areas.\cb1 \
\pard\pardeftab720\sl340\qr\partightenfactor0

\f0 \cf2 \cb4 \strokec2 In\'a0[8]:\cb1 \
\pard\pardeftab720\sl340\partightenfactor0
\cf8 \cb7 \strokec8  \
\pard\pardeftab720\sl340\qr\partightenfactor0
\cf3 \cb1 \strokec3 \
\pard\pardeftab720\sl400\partightenfactor0

\f4\fs24 \cf0 \kerning1\expnd0\expndtw0 \outl0\strokewidth0 {{\NeXTGraphic corrected_figure2_4.png \width17800 \height12280
}¬}\pard\pardeftab720\sl400\partightenfactor0

\f1\fs28 \cf3 \expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec3 \
\pard\pardeftab720\sl340\qr\partightenfactor0

\f0 \cf2 \strokec2 \
\pard\pardeftab720\sl500\partightenfactor0

\f1\b\fs52 \cf3 \cb4 \strokec3 3. Bayesian Inference with real data. The stocks dataset.\cf5 \strokec5 \'b6\cf3 \cb1 \strokec3 \
\pard\pardeftab720\sl400\partightenfactor0

\b0\fs28 \cf3 \cb4 Once our code has been tested on synthetic data, we will use it with real data. Load and properly normalize data corresponding to the evolution of the stocks of 10 airline companies. This data set is an adaptation of the Stock dataset from {\field{\*\fldinst{HYPERLINK "http://www.dcc.fc.up.pt/~ltorgo/Regression/DataSets.html"}}{\fldrslt \cf5 \ul \ulc5 \strokec5 http://www.dcc.fc.up.pt/~ltorgo/Regression/DataSets.html}}, which in turn was taken from the StatLib Repository, {\field{\*\fldinst{HYPERLINK "http://lib.stat.cmu.edu/"}}{\fldrslt \cf5 \ul \ulc5 \strokec5 http://lib.stat.cmu.edu/}}\cb1 \
\pard\pardeftab720\sl340\qr\partightenfactor0

\f0 \cf2 \cb4 \strokec2 In\'a0[9]:\cb1 \
\pard\pardeftab720\sl340\partightenfactor0
\cf8 \cb7 \strokec8 matvar \cf9 \strokec9 =\cf8 \strokec8  scipy\cf9 \strokec9 .\cf8 \strokec8 io\cf9 \strokec9 .\cf8 \strokec8 loadmat(\cf12 \strokec12 'DatosLabReg.mat'\cf8 \strokec8 )\
Xtrain \cf9 \strokec9 =\cf8 \strokec8  matvar[\cf12 \strokec12 'Xtrain'\cf8 \strokec8 ]\
Xtest \cf9 \strokec9 =\cf8 \strokec8  matvar[\cf12 \strokec12 'Xtest'\cf8 \strokec8 ]\
Ytrain \cf9 \strokec9 =\cf8 \strokec8  matvar[\cf12 \strokec12 'Ytrain'\cf8 \strokec8 ]\
Ytest \cf9 \strokec9 =\cf8 \strokec8  matvar[\cf12 \strokec12 'Ytest'\cf8 \strokec8 ]\
\
\pard\pardeftab720\sl340\partightenfactor0

\f2\i \cf6 \strokec6 # Data normalization
\f0\i0 \cf8 \strokec8 \
mean_x \cf9 \strokec9 =\cf8 \strokec8  np\cf9 \strokec9 .\cf8 \strokec8 mean(Xtrain,axis\cf9 \strokec9 =0\cf8 \strokec8 )\
std_x \cf9 \strokec9 =\cf8 \strokec8  np\cf9 \strokec9 .\cf8 \strokec8 std(Xtrain,axis\cf9 \strokec9 =0\cf8 \strokec8 )\
Xtrain \cf9 \strokec9 =\cf8 \strokec8  (Xtrain \cf9 \strokec9 -\cf8 \strokec8  mean_x) \cf9 \strokec9 /\cf8 \strokec8  std_x\
Xtest \cf9 \strokec9 =\cf8 \strokec8  (Xtest \cf9 \strokec9 -\cf8 \strokec8  mean_x) \cf9 \strokec9 /\cf8 \strokec8  std_x\
\

\f2\i \cf6 \strokec6 # Extend input data matrices with a column of 1's
\f0\i0 \cf8 \strokec8 \
col_1 \cf9 \strokec9 =\cf8 \strokec8  np\cf9 \strokec9 .\cf8 \strokec8 ones( (Xtrain\cf9 \strokec9 .\cf8 \strokec8 shape[\cf9 \strokec9 0\cf8 \strokec8 ],\cf9 \strokec9 1\cf8 \strokec8 ) )\
Xtrain_e \cf9 \strokec9 =\cf8 \strokec8  np\cf9 \strokec9 .\cf8 \strokec8 concatenate( (col_1,Xtrain), axis \cf9 \strokec9 =\cf8 \strokec8  \cf9 \strokec9 1\cf8 \strokec8  )\
\
col_1 \cf9 \strokec9 =\cf8 \strokec8  np\cf9 \strokec9 .\cf8 \strokec8 ones( (Xtest\cf9 \strokec9 .\cf8 \strokec8 shape[\cf9 \strokec9 0\cf8 \strokec8 ],\cf9 \strokec9 1\cf8 \strokec8 ) )\
Xtest_e \cf9 \strokec9 =\cf8 \strokec8  np\cf9 \strokec9 .\cf8 \strokec8 concatenate( (col_1,Xtest), axis \cf9 \strokec9 =\cf8 \strokec8  \cf9 \strokec9 1\cf8 \strokec8  )\
\pard\pardeftab720\sl340\qr\partightenfactor0
\cf2 \cb1 \strokec2 \
\pard\pardeftab720\sl400\partightenfactor0

\f1 \cf3 \cb4 \strokec3 After running this code, you will have inside matrix $\\mbox\{Xtrain_e\}$ an initial column of ones and the evolution of (normalized) price for 9 airlines, whereas vector Ytrain will contain a single column with the price evolution of the tenth airline. The objective of the regression task is to estimate the price of the tenth airline from the prices of the other nine.\cb1 \
\pard\pardeftab720\sl340\qr\partightenfactor0

\f0 \cf2 \strokec2 \
\pard\pardeftab720\sl420\partightenfactor0

\f1\b\fs44 \cf3 \cb4 \strokec3 3.1. Hyperparameter selection\cf5 \strokec5 \'b6\cf3 \cb1 \strokec3 \
\pard\pardeftab720\sl400\partightenfactor0

\b0\fs28 \cf3 \cb4 Since the values $\\sigma_0$ and $\\sigma_\\varepsilon$ are no longer known, a first rough estimation is needed (we will soon see how to estimate these values in a principled way).\cb1 \
\cb4 To this end, we will adjust them using the LS solution to the regression problem:\cb1 \
\pard\tx220\tx720\pardeftab720\li720\fi-720\sl400\partightenfactor0
\ls2\ilvl0\cf3 \cb4 \kerning1\expnd0\expndtw0 \outl0\strokewidth0 {\listtext	\'95	}\expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec3 $\\sigma_0^2$ will be taken as the average of the square values of $\{\\hat \{\\bf w\}\}_\{LS\}$\cb1 \
\ls2\ilvl0\cb4 \kerning1\expnd0\expndtw0 \outl0\strokewidth0 {\listtext	\'95	}\expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec3 $\\sigma_\\varepsilon^2$ will be taken as two times the average of the square of the residuals when using $\{\\hat \{\\bf w\}\}_\{LS\}$\cb1 \
\pard\pardeftab720\sl340\qr\partightenfactor0

\f0 \cf2 \cb4 \strokec2 In\'a0[10]:\cb1 \
\pard\pardeftab720\sl340\partightenfactor0
\cf8 \cb7 \strokec8 w_LS, residuals, rank, s \cf9 \strokec9 =\cf8 \strokec8  np\cf9 \strokec9 .\cf8 \strokec8 linalg\cf9 \strokec9 .\cf8 \strokec8 lstsq(Xtrain_e,Ytrain)\
sigma_0 \cf9 \strokec9 =\cf8 \strokec8  np\cf9 \strokec9 .\cf8 \strokec8 sqrt(np\cf9 \strokec9 .\cf8 \strokec8 mean(w_LS\cf9 \strokec9 **2\cf8 \strokec8 ))\
sigma_eps \cf9 \strokec9 =\cf8 \strokec8  np\cf9 \strokec9 .\cf8 \strokec8 sqrt(\cf9 \strokec9 2\cf8 \strokec8  \cf9 \strokec9 *\cf8 \strokec8  np\cf9 \strokec9 .\cf8 \strokec8 mean((Ytrain \cf9 \strokec9 -\cf8 \strokec8  Xtrain_e\cf9 \strokec9 .\cf8 \strokec8 dot(w_LS))\cf9 \strokec9 **2\cf8 \strokec8 ))\
\pard\pardeftab720\sl340\qr\partightenfactor0
\cf2 \cb1 \strokec2 \
\pard\pardeftab720\sl420\partightenfactor0

\f1\b\fs44 \cf3 \cb4 \strokec3 3.2. Posterior pdf of the weight vector\cf5 \strokec5 \'b6\cf3 \cb1 \strokec3 \
\pard\pardeftab720\sl400\partightenfactor0

\b0\fs28 \cf3 \cb4 Using the previous values for the hyperparameters, compute the 
\i a posteriori
\i0  mean and covariance matrix of the weight vector $\{\\bf w\}$. Instead of two weights there will now be 10.\cb1 \
\pard\pardeftab720\sl340\qr\partightenfactor0

\f0 \cf2 \cb4 \strokec2 In\'a0[11]:\cb1 \
\pard\pardeftab720\sl340\partightenfactor0
\cf8 \cb7 \strokec8 Cov_w \cf9 \strokec9 =\cf8 \strokec8  \
mean_w \cf9 \strokec9 =\cf8 \strokec8  \
\pard\pardeftab720\sl340\qr\partightenfactor0
\cf2 \cb1 \strokec2 \
\pard\pardeftab720\sl400\partightenfactor0

\f1 \cf3 \cb4 \strokec3 The resulting posterior is:\cb1 \
\pard\pardeftab720\sl340\qr\partightenfactor0

\f0 \cf2 \cb4 \strokec2 In\'a0[12]:\cb1 \
\pard\pardeftab720\sl340\partightenfactor0

\f3\b \cf10 \cb7 \strokec10 print
\f0\b0 \cf8 \strokec8  \cf12 \strokec12 'mean_w = '\cf8 \strokec8  \cf9 \strokec9 +\cf8 \strokec8  \cf10 \strokec10 str\cf8 \strokec8 (mean_w)\

\f3\b \cf10 \strokec10 print
\f0\b0 \cf8 \strokec8  \cf12 \strokec12 'Cov_w = '\cf8 \strokec8  \cf9 \strokec9 +\cf8 \strokec8  \cf10 \strokec10 str\cf8 \strokec8 (Cov_w)\
\pard\pardeftab720\sl340\qr\partightenfactor0
\cf3 \cb1 \strokec3 \
\pard\pardeftab720\sl340\partightenfactor0
\cf3 \cb4 mean_w = [[ 47.05815827]\
 [  5.00414611]\
 [  2.23805657]\
 [  0.15284809]\
 [ -1.21321506]\
 [  1.35020502]\
 [ -3.1205305 ]\
 [  1.08434917]\
 [  0.85755156]\
 [  2.24208409]]\
Cov_w = [[  1.37708166e-02  -1.36083568e-17   4.54237611e-17  -1.17184358e-17\
   -2.50087935e-17  -1.22866282e-17   3.87650157e-18  -3.28213231e-18\
    4.64251814e-17  -1.00391352e-17]\
 [ -1.36083568e-17   1.61193895e-01   1.19983623e-02   2.52752373e-02\
   -2.74399921e-03   2.91979986e-03   3.46489945e-02   9.27404804e-03\
   -1.16337869e-01  -1.85649689e-02]\
 [  4.54237611e-17   1.19983623e-02   2.11749245e-01  -5.34011223e-02\
   -1.23160182e-02  -7.26971855e-02  -6.72052851e-03  -4.46380127e-02\
    4.83685664e-02  -7.56631814e-02]\
 [ -1.17184358e-17   2.52752373e-02  -5.34011223e-02   6.04494893e-02\
    8.63341517e-03   5.27479724e-03  -1.77929599e-02   8.84585833e-03\
   -3.36597425e-02   1.97630174e-02]\
 [ -2.50087935e-17  -2.74399921e-03  -1.23160182e-02   8.63341517e-03\
    9.65083458e-02   1.94374658e-02  -2.75922622e-02  -6.81626120e-02\
   -2.31222582e-02  -1.47183809e-02]\
 [ -1.22866282e-17   2.91979986e-03  -7.26971855e-02   5.27479724e-03\
    1.94374658e-02   5.96233432e-02   3.01813655e-03  -2.00311615e-03\
   -1.11951357e-02   1.72095411e-02]\
 [  3.87650157e-18   3.46489945e-02  -6.72052851e-03  -1.77929599e-02\
   -2.75922622e-02   3.01813655e-03   5.67439844e-02   9.67316186e-03\
   -6.10559900e-03   4.28213647e-03]\
 [ -3.28213231e-18   9.27404804e-03  -4.46380127e-02   8.84585833e-03\
   -6.81626120e-02  -2.00311615e-03   9.67316186e-03   9.10729919e-02\
   -2.60304735e-02   2.77987562e-02]\
 [  4.64251814e-17  -1.16337869e-01   4.83685664e-02  -3.36597425e-02\
   -2.31222582e-02  -1.11951357e-02  -6.10559900e-03  -2.60304735e-02\
    1.45649950e-01   3.99358078e-03]\
 [ -1.00391352e-17  -1.85649689e-02  -7.56631814e-02   1.97630174e-02\
   -1.47183809e-02   1.72095411e-02   4.28213647e-03   2.77987562e-02\
    3.99358078e-03   4.96488271e-02]]\
\pard\pardeftab720\sl340\qr\partightenfactor0
\cf2 \cb1 \strokec2 \
\pard\pardeftab720\sl420\partightenfactor0

\f1\b\fs44 \cf3 \cb4 \strokec3 3.3. Model assessment\cf5 \strokec5 \'b6\cf3 \cb1 \strokec3 \
\pard\pardeftab720\sl400\partightenfactor0

\b0\fs28 \cf3 \cb4 In order to verify the performance of the resulting model, compute the posterior mean and variance of each of the test outputs from the posterior over $\{\\bf w\}$. I.e, compute $\{\\mathbb E\}\\left\\\{s(\{\\bf x\}^\\ast)\\mid\{\\bf s\}\\right\\\}$ and $\\sqrt\{\{\\mathbb V\}\\left\\\{s(\{\\bf x\}^\\ast)\\mid\{\\bf s\}\\right\\\}\}$ for each test sample $\{\\bf x\}^\\ast$ contained in each row of $\\mbox\{Xtest\}$. Be sure not to use the outputs $\\mbox\{Ytest\}$ at any point during this process.\cb1 \
\cb4 Store the predictive mean and variance of all test samples in two vectors called $\\mbox\{m_y\}$ and $\\mbox\{v_y\}$, respectively.\cb1 \
\pard\pardeftab720\sl340\qr\partightenfactor0

\f0 \cf2 \cb4 \strokec2 In\'a0[13]:\cb1 \
\pard\pardeftab720\sl340\partightenfactor0
\cf8 \cb7 \strokec8 m_y \cf9 \strokec9 =\cf8 \strokec8  \
v_y \cf9 \strokec9 =\cf8 \strokec8  \
\pard\pardeftab720\sl340\qr\partightenfactor0
\cf2 \cb1 \strokec2 \
\pard\pardeftab720\sl400\partightenfactor0

\f1 \cf3 \cb4 \strokec3 Compute now the mean square error (MSE) and the negative log-predictive density (NLPD) with the following code:\cb1 \
\pard\pardeftab720\sl340\qr\partightenfactor0

\f0 \cf2 \cb4 \strokec2 In\'a0[14]:\cb1 \
\pard\pardeftab720\sl340\partightenfactor0

\f3\b \cf10 \cb7 \strokec10 from
\f0\b0 \cf8 \strokec8  
\f3\b \cf11 \strokec11 math
\f0\b0 \cf8 \strokec8  
\f3\b \cf10 \strokec10 import
\f0\b0 \cf8 \strokec8  pi\
\
MSE \cf9 \strokec9 =\cf8 \strokec8  np\cf9 \strokec9 .\cf8 \strokec8 mean((m_y \cf9 \strokec9 -\cf8 \strokec8  Ytest)\cf9 \strokec9 **2\cf8 \strokec8 )\
NLPD \cf9 \strokec9 =\cf8 \strokec8  \cf9 \strokec9 0.5\cf8 \strokec8  \cf9 \strokec9 *\cf8 \strokec8  np\cf9 \strokec9 .\cf8 \strokec8 mean(((Ytest \cf9 \strokec9 -\cf8 \strokec8  m_y)\cf9 \strokec9 **2\cf8 \strokec8 )\cf9 \strokec9 /\cf8 \strokec8 (np\cf9 \strokec9 .\cf8 \strokec8 matrix(v_y)\cf9 \strokec9 .\cf8 \strokec8 T) \cf9 \strokec9 +\cf8 \strokec8  \cf9 \strokec9 0.5*\cf8 \strokec8 np\cf9 \strokec9 .\cf8 \strokec8 log(\cf9 \strokec9 2*\cf8 \strokec8 pi\cf9 \strokec9 *\cf8 \strokec8 np\cf9 \strokec9 .\cf8 \strokec8 matrix(v_y)\cf9 \strokec9 .\cf8 \strokec8 T))\
\pard\pardeftab720\sl340\qr\partightenfactor0
\cf2 \cb1 \strokec2 \
\pard\pardeftab720\sl400\partightenfactor0

\f1 \cf3 \cb4 \strokec3 Results should be:\cb1 \
\pard\pardeftab720\sl340\qr\partightenfactor0

\f0 \cf2 \cb4 \strokec2 In\'a0[15]:\cb1 \
\pard\pardeftab720\sl340\partightenfactor0

\f3\b \cf10 \cb7 \strokec10 print
\f0\b0 \cf8 \strokec8  \cf12 \strokec12 'MSE = '\cf8 \strokec8  \cf9 \strokec9 +\cf8 \strokec8  \cf10 \strokec10 str\cf8 \strokec8 (MSE)\

\f3\b \cf10 \strokec10 print
\f0\b0 \cf8 \strokec8  \cf12 \strokec12 'NLPD = '\cf8 \strokec8  \cf9 \strokec9 +\cf8 \strokec8  \cf10 \strokec10 str\cf8 \strokec8 (NLPD)\
\pard\pardeftab720\sl340\qr\partightenfactor0
\cf3 \cb1 \strokec3 \
\pard\pardeftab720\sl340\partightenfactor0
\cf3 \cb4 MSE = 6.11353618976\
NLPD = 1.33761732312\
\pard\pardeftab720\sl340\qr\partightenfactor0
\cf2 \cb1 \strokec2 \
\pard\pardeftab720\sl400\partightenfactor0

\f1 \cf3 \cb4 \strokec3 These two measures reveal the quality of our predictor (with lower values revealing higher quality). The first measure (MSE) only compares the predictive mean with the actual value and always has a positive value (if zero was reached, it would mean a perfect prediction). It does not take into account predictive variance. The second measure (NLPD) takes into account both the deviation and the predictive variance (uncertainty) to measure the quality of the probabilistic prediction (a high error in a prediction that was already known to have high variance has a smaller penalty, but also, announcing a high variance when the prediction error is small won\'92t award such a good score).\cb1 \
\pard\pardeftab720\sl340\qr\partightenfactor0

\f0 \cf2 \strokec2 \
\pard\pardeftab720\sl500\partightenfactor0

\f1\b\fs52 \cf3 \cb4 \strokec3 4. Non-linear regression with Gaussian Processes\cf5 \strokec5 \'b6\cf3 \cb1 \strokec3 \
\pard\pardeftab720\sl420\partightenfactor0

\fs44 \cf3 \cb4 4.1. Multidimensional regression\cf5 \strokec5 \'b6\cf3 \cb1 \strokec3 \
\pard\pardeftab720\sl400\partightenfactor0

\b0\fs28 \cf3 \cb4 Rather than using a parametric form for $f(\{\\mathbf x\})$, in this section we will use directly the values of the latent function that we will model with a Gaussian process\cb1 \
\pard\pardeftab720\sl400\partightenfactor0
\cf3 \cb4 $$f(\{\\mathbf x\}) \\sim \{\\cal GP\}\\left(0,k_f(\{\\mathbf x\}_i,\{\\mathbf x\}_j)\\right),$$\
\pard\pardeftab720\sl400\partightenfactor0
\cf3 where we are assuming a zero mean, and where we will use the Ornstein-Uhlenbeck covariance function, which is defined as:\cb1 \
\pard\pardeftab720\sl400\partightenfactor0
\cf3 \cb4 $$k_f(\{\\mathbf x\}_i,\{\\mathbf x\}_j) = \\sigma_0^2 \\exp \\left( -\\frac\{1\}\{l\}\\|\{\\mathbf x\}_i-\{\\mathbf x\}_j\\|\\right)$$\
\pard\pardeftab720\sl400\partightenfactor0
\cf3 First, we will use the following gross estimation for the hyperparameters:\cb1 \
\pard\pardeftab720\sl340\qr\partightenfactor0

\f0 \cf2 \cb4 \strokec2 In\'a0[16]:\cb1 \
\pard\pardeftab720\sl340\partightenfactor0
\cf8 \cb7 \strokec8 sigma_0 \cf9 \strokec9 =\cf8 \strokec8  np\cf9 \strokec9 .\cf8 \strokec8 std(Ytrain)\
sigma_eps \cf9 \strokec9 =\cf8 \strokec8  sigma_0 \cf9 \strokec9 /\cf8 \strokec8  np\cf9 \strokec9 .\cf8 \strokec8 sqrt(\cf9 \strokec9 10\cf8 \strokec8 )\
l \cf9 \strokec9 =\cf8 \strokec8  \cf9 \strokec9 8\cf8 \strokec8 \
\pard\pardeftab720\sl340\qr\partightenfactor0
\cf2 \cb1 \strokec2 \
\pard\pardeftab720\sl400\partightenfactor0

\f1 \cf3 \cb4 \strokec3 As we studied in a previous session, the joint distribution of the target values in the training set, $\{\\mathbf s\}$, and the latent values corresponding to the test points, $\{\\mathbf f\}^\\ast$, is given by\cb1 \
\pard\pardeftab720\sl400\partightenfactor0
\cf3 \cb4 $$\\left[\\begin\{array\}\{c\}\{\\bf s\}\\\\\{\\bf f\}^\\ast\\end\{array\}\\right]~\\sim~\{\\cal N\}\\left(\{\\bf 0\},\\left[\\begin\{array\}\{cc\}\{\\bf K\} + \\sigma_\\varepsilon^2 \{\\bf I\}& \{\\bf K\}_\\ast^\\top \\\\ \{\\bf K\}_\\ast & \{\\bf K\}_\{\\ast\\ast\} \\end\{array\}\\right]\\right)$$\
\pard\pardeftab720\sl400\partightenfactor0
\cf3 Using this model, obtain the posterior of $\{\\mathbf s\}^\\ast$ given $\{\\mathbf s\}$. In particular, calculate the 
\i a posteriori
\i0  predictive mean and standard deviations, $\{\\mathbb E\}\\left\\\{s(\{\\bf x\}^\\ast)\\mid\{\\bf s\}\\right\\\}$ and $\\sqrt\{\{\\mathbb V\}\\left\\\{s(\{\\bf x\}^\\ast)\\mid\{\\bf s\}\\right\\\}\}$ for each test sample $\{\\bf x\}^\\ast$.\cb1 \
\cb4 Obtain the MSE and NLPD and compare them with those obtained Subsection 3.3.\cb1 \
\pard\pardeftab720\sl340\qr\partightenfactor0

\f0 \cf2 \cb4 \strokec2 In\'a0[17]:\cb1 \
\pard\pardeftab720\sl340\partightenfactor0
\cf8 \cb7 \strokec8  \
\pard\pardeftab720\sl340\qr\partightenfactor0
\cf2 \cb1 \strokec2 \
\pard\pardeftab720\sl400\partightenfactor0

\f1 \cf3 \cb4 \strokec3 You should obtain the following results:\cb1 \
\pard\pardeftab720\sl340\qr\partightenfactor0

\f0 \cf2 \cb4 \strokec2 In\'a0[18]:\cb1 \
\pard\pardeftab720\sl340\partightenfactor0

\f3\b \cf10 \cb7 \strokec10 print
\f0\b0 \cf8 \strokec8  \cf12 \strokec12 'MSE = '\cf8 \strokec8  \cf9 \strokec9 +\cf8 \strokec8  \cf10 \strokec10 str\cf8 \strokec8 (MSE)\

\f3\b \cf10 \strokec10 print
\f0\b0 \cf8 \strokec8  \cf12 \strokec12 'NLPD = '\cf8 \strokec8  \cf9 \strokec9 +\cf8 \strokec8  \cf10 \strokec10 str\cf8 \strokec8 (NLPD)\
\pard\pardeftab720\sl340\qr\partightenfactor0
\cf3 \cb1 \strokec3 \
\pard\pardeftab720\sl340\partightenfactor0
\cf3 \cb4 MSE = 0.494826685647\
NLPD = 0.967323914258\
\pard\pardeftab720\sl340\qr\partightenfactor0
\cf2 \cb1 \strokec2 \
\pard\pardeftab720\sl420\partightenfactor0

\f1\b\fs44 \cf3 \cb4 \strokec3 4.2. Unidimensional regression\cf5 \strokec5 \'b6\cf3 \cb1 \strokec3 \
\pard\pardeftab720\sl400\partightenfactor0

\b0\fs28 \cf3 \cb4 Use now only the first company to compute the non-linear regression. Obtain the posterior distribution of $f(\{\\mathbf x\}^\\ast)$ evaluated at the test values $\{\\mathbf x\}^\\ast$, i.e, $p(f(\{\\mathbf x\}^\\ast)\\mid \{\\mathbf s\})$.\cb1 \
\cb4 This distribution is Gaussian, with mean $\{\\mathbb E\}\\left\\\{f(\{\\bf x\}^\\ast)\\mid\{\\bf s\}\\right\\\}$ and a covariance matrix $\\text\{Cov\}\\left[f(\{\\bf x\}^\\ast)\\mid\{\\bf s\}\\right]$. Sample 50 random vectors from the distribution and plot them vs. the values $x^\\ast$, together with the test samples.\cb1 \
\cb4 These 50 samples of the function space are analogous to the 50 straight lines that were generated in Subsection 2.3. Again, the Bayesian model does not provide a single function, but a pdf over functions, from which we extracted 50 possible functions.\cb1 \
\pard\pardeftab720\sl340\qr\partightenfactor0

\f0 \cf2 \cb4 \strokec2 In\'a0[19]:\cb1 \
\pard\pardeftab720\sl340\partightenfactor0
\cf8 \cb7 \strokec8  \
\pard\pardeftab720\sl340\qr\partightenfactor0
\cf3 \cb1 \strokec3 \
\pard\pardeftab720\sl400\partightenfactor0

\f1 \cf3 {{\NeXTGraphic 3__#$!@%!#__unknown.png \width17940 \height12280
}¬}\pard\pardeftab720\sl400\partightenfactor0
\cf3 \cb4  \cb1 \
\pard\pardeftab720\sl340\qr\partightenfactor0

\f0 \cf2 \strokec2 \
\pard\pardeftab720\sl400\partightenfactor0

\f1 \cf3 \cb4 \strokec3 Plot again the previous figure, this time including in your plot the confidence interval delimited by two standard deviations of the prediction, similarly to what was done in Subsection 2.4. You can observe how $95.45\\%$ of observed data fall within the designated area.\cb1 \
\pard\pardeftab720\sl340\qr\partightenfactor0

\f0 \cf2 \cb4 \strokec2 In\'a0[20]:\cb1 \
\pard\pardeftab720\sl340\partightenfactor0
\cf8 \cb7 \strokec8  \
\pard\pardeftab720\sl340\qr\partightenfactor0
\cf3 \cb1 \strokec3 \
\pard\pardeftab720\sl400\partightenfactor0

\f1 \cf3 {{\NeXTGraphic 4__#$!@%!#__unknown.png \width17940 \height12280
}¬}\pard\pardeftab720\sl400\partightenfactor0
\cf3 \cb4  \cb1 \
\pard\pardeftab720\sl340\qr\partightenfactor0

\f0 \cf2 \strokec2 \
\pard\pardeftab720\sl400\partightenfactor0

\f1 \cf3 \cb4 \strokec3 Compute now the MSE and NLPD of the model. The correct results are given below:\cb1 \
\pard\pardeftab720\sl340\qr\partightenfactor0

\f0 \cf2 \cb4 \strokec2 In\'a0[21]:\cb1 \
\pard\pardeftab720\sl340\partightenfactor0

\f3\b \cf10 \cb7 \strokec10 print
\f0\b0 \cf8 \strokec8  \cf12 \strokec12 'MSE = '\cf8 \strokec8  \cf9 \strokec9 +\cf8 \strokec8  \cf10 \strokec10 str\cf8 \strokec8 (MSE)\

\f3\b \cf10 \strokec10 print
\f0\b0 \cf8 \strokec8  \cf12 \strokec12 'NLPD = '\cf8 \strokec8  \cf9 \strokec9 +\cf8 \strokec8  \cf10 \strokec10 str\cf8 \strokec8 (NLPD)\
\pard\pardeftab720\sl340\qr\partightenfactor0
\cf3 \cb1 \strokec3 \
\pard\pardeftab720\sl340\partightenfactor0
\cf3 \cb4 MSE = 6.93366416329\
NLPD = 1.61261114765\
}