{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "N3MgtUPKg1qq",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Data preprocessing methods: Normalization\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "    Notebook version:\n",
    "\n",
    "    * 1.0 (Sep 15, 2020) - First version\n",
    "    * 1.1 (Sep 15, 2021) - Exercises\n",
    "\n",
    "    Authors: Jes√∫s Cid Sueiro (jcid@ing.uc3m.es)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-VPh26VrkBRD",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Some libraries that will be used along the notebook.\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data preprocessing\n",
    "\n",
    "## 1.1. The dataset.\n",
    "\n",
    "A key component of any data processing method or any machine learning algorithm is the **dataset**, i.e., the set of data that will be the input to the method or algorithm. \n",
    "\n",
    "The dataset collects information extracted from a population (of objects, entities, individuals,...). For instance, we can measure the weight and height of students from a class and collect this information in a dataset ${\\cal S} = \\{{\\bf x}_k, k=0, \\ldots, K-1\\}$ where $K$ is the number of students, and each sample is a 2 dimensional vector, ${\\bf x}_k= (x_{k0}, x_{k1})$, with the height  and the weight in the first and the second component, respectively. These components are usually called **features**. In other datasets, the number of features can be arbitrarily large."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dBTW556Vjz39",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 1.1. Data preprocessing\n",
    "\n",
    "The aim of [data preprocessing methods](https://scikit-learn.org/stable/modules/preprocessing.html) is to transform the data into a form that is ready to apply machine learning algorithms. This may include:\n",
    "\n",
    "  * [Data normalization](https://scikit-learn.org/stable/modules/preprocessing.html#standardization-or-mean-removal-and-variance-scaling): transform the individual features to ensure a proper range of variation\n",
    "  * [Data imputation](https://scikit-learn.org/stable/modules/impute.html): assign values to features that may be missed for some data samples\n",
    "  * [Feature extraction](https://scikit-learn.org/stable/modules/feature_extraction.html): transform the original data to compute new features that are more appropiate for a specific prediction task\n",
    "  * [Dimensionality reduction](https://en.wikipedia.org/wiki/Dimensionality_reduction): remove features that are not relevant for the prediction task.\n",
    "  * [Outlier removal](https://scikit-learn.org/stable/modules/outlier_detection.html): remove samples that may contain errors and are not reliable for the prediction task.\n",
    "  * [Clustering](https://scikit-learn.org/stable/modules/clustering.html): partition the data into smaller subsets, that could be easier to process.\n",
    "  \n",
    "In this notebook we will focus on data normalization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 2. Data normalization \n",
    "\n",
    "All samples in the dataset can be arranged by rows in a $K \\times m$ **data matrix** ${\\bf X}$, where $m$ is the number of features (i.e. the dimension of the vector space containing the data). Each one of the $m$ data features may represent variables of very different nature (e.g. time, distance, price, volume, pixel intensity,...). Thus, the scale and the range of variation of each feature can be completely different.\n",
    "\n",
    "As an illustration, consider the 2-dimensional dataset in the figure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_blobs\n",
    "X, y = make_blobs(n_samples=300, centers=4, random_state=0, cluster_std=0.60)\n",
    "X = X @ np.array([[30, 4], [-8, 1]]) + np.array([90, 10])\n",
    "\n",
    "plt.figure(figsize=(12, 3))\n",
    "plt.scatter(X[:, 0], X[:, 1], s=50);\n",
    "plt.axis('equal')\n",
    "plt.xlabel('$x_0$')\n",
    "plt.ylabel('$x_1$')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "We can see that the first data feature ($x_0$) has a much large range of variation than the second ($x_1$). In practice, this may be problematic: the convergence properties of some machine learning algorithms may depend critically on the feature distributions and, in general, features sets ranging over similar scales use to offer a better performance.\n",
    "\n",
    "For this reason, transforming the data in order to get similar range of variations for all features is desirable. This can be done in several ways."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 2.1. Standard scaling.\n",
    "\n",
    "A common normalization method consists on applying an affine transformation\n",
    "$$\n",
    "{\\bf t}_k = {\\bf D}({\\bf x}_k - {\\bf m})\n",
    "$$\n",
    "\n",
    "where ${\\bf D}$ is a diagonal matrix, in such a way that the transformed dataset ${\\cal S}' = \\{{\\bf t}_k, k=0, \\ldots, K-1\\}$ has zero sample mean, i.e.,\n",
    "\n",
    "$$\n",
    "\\frac{1}{K} \\sum_{k=0}^{K-1} {\\bf t}_k = 0\n",
    "$$\n",
    "\n",
    "and unit sample variance, i.e., \n",
    "\n",
    "$$\n",
    "\\frac{1}{K} \\sum_{k=0}^{K-1} t_{ki}^2 = 1\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "It is not difficult to verify that this can be done by taking ${\\bf m}$ equal to the sample mean\n",
    "$$\n",
    "{\\bf m} = \\frac{1}{K} \\sum_{k=0}^{K-1} {\\bf x}_k\n",
    "$$\n",
    "\n",
    "and taking the diagonal components of ${\\bf D}$ equal to the inverse of the standard deviation of each feature, i.e.,\n",
    "\n",
    "$$\n",
    "d_{ii} = \\frac{1}{\\sqrt{\\frac{1}{K} \\sum_{k=0}^{K-1} (x_{ki} - m_i)^2}}\n",
    "$$\n",
    "\n",
    "Using the data matrix ${\\bf X}$ and the *broadcasting* property of the basic mathematical operators in Python, the implementation of this normalization is straightforward."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**Exercise 1**: Apply a standard scaling to the data matrix. To do so:\n",
    "\n",
    "  1. Compute the mean, and store it in variable `m` (you can use method `mean` from `numpy`)\n",
    "  2. Compute the standard deviation of each feature, and store the result in variable `s` (you can use method `std` from `numpy`)\n",
    "  3. Take advangate of the broadcasting property to normalize the data matrix in a single line of code. Save the result in variable `T`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# Compute the sample mean\n",
    "# m = <FILL IN>\n",
    "print(f'The sample mean is m = {m}')\n",
    "\n",
    "# Compute the standard deviation of each feature\n",
    "# s = <FILL IN>\n",
    "\n",
    "# Normalize de data matrix\n",
    "# T = <FILL IN>\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "We can test if the transformed features have zero-mean and unit variance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Testing mean\n",
    "print(f\"- The mean of the transformed features are: {np.mean(T, axis=0)}\")\n",
    "print(f\"- The standard deviation of the transformed features are: {np.std(T, axis=0)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "(note that the results can deviate from 0 or 1 due to finite precision errors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# Now you can verify if your solution satisfies\n",
    "plt.figure(figsize=(4, 4))\n",
    "plt.scatter(T[:, 0], T[:, 1], s=50);\n",
    "plt.axis('equal')\n",
    "plt.xlabel('$x_0$')\n",
    "plt.ylabel('$x_1$')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### 2.1.1. Implementation in `sklearn`\n",
    "\n",
    "The `sklearn` package contains a method to perform the standard scaling over a given data matrix.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X)\n",
    "print(f'The sample mean is m = {scaler.mean_}')\n",
    "\n",
    "T2 = scaler.transform(X)\n",
    "plt.figure(figsize=(4, 4))\n",
    "plt.scatter(T2[:, 0], T2[:, 1], s=50);\n",
    "plt.axis('equal')\n",
    "plt.xlabel('$x_0$')\n",
    "plt.ylabel('$x_1$')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Note that, once we have defined the scaler object in Python, you can apply the scaling transformation to other datasets. This will be useful in further topics, when the dataset may be split in several matrices and we may be interested in defining the transformation using some matrix, and apply it to others"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 2.2. Other normalizations.\n",
    "\n",
    "The are some alternatives to the standard scaling that may be interesting for some datasets. Here we show some of them, available at the [preprocessing](https://scikit-learn.org/stable/modules/classes.html#module-sklearn.preprocessing) module in `sklearn`:\n",
    "\n",
    "  * [preprocessing.MaxAbsScaler](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.MaxAbsScaler.html#sklearn.preprocessing.MaxAbsScaler): Scale each feature by its maximum absolute value. As a result, all feature values will lie in the interval [-1, 1].\n",
    "  * [preprocessing.MinMaxScaler](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.MinMaxScaler.html#sklearn.preprocessing.MinMaxScaler): Transform features by scaling each feature to a given range. Also, all feature values will lie in the specified interval.\n",
    "  * [preprocessing.Normalizer](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.Normalizer.html#sklearn.preprocessing.Normalizer): Normalize samples individually to unit norm. That is, it applies the transformation ${\\bf t}_k = \\frac{1}{\\|{\\bf x}_k\\|} {\\bf x}_k$\n",
    "  * [preprocessing.PowerTransformer](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.PowerTransformer.html#sklearn.preprocessing.PowerTransformer): Apply a power transform featurewise to make data more Gaussian-like.\n",
    "  * [preprocessing.QuantileTransformer](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.QuantileTransformer.html#sklearn.preprocessing.QuantileTransformer): Transform features using quantile information. The transformed features follow a specific target distribution (uniform or normal). \n",
    "  * [preprocessing.RobustScaler](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.RobustScaler.html#sklearn.preprocessing.RobustScaler): Scale features using statistics that are robust to outliers. This way, anomalous values in one or very few samples cannot have a strong influence in the normalization.\n",
    "\n",
    "You can find more detailed explanation of these transformations `sklearn` [documentation](https://scikit-learn.org/stable/modules/preprocessing.html#preprocessing).\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**Exercise 2**: Use `sklearn` to transform the data matrix `X` into a matrix `T24`such that the minimum feature value is 2 and the maximum is 4.\n",
    "\n",
    "(Hint: select and import the appropriate preprocessing module from `sklearn` an follow the same steps used in the code cell above for the scandard scaler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# Write your solution here\n",
    "# <SOL>\n",
    "# </SOL>\n",
    "\n",
    "# We can visually check that the transformed data features lie in the selected range.\n",
    "plt.figure(figsize=(4, 4))\n",
    "plt.scatter(T24[:, 0], T24[:, 1], s=50);\n",
    "plt.axis('equal')\n",
    "plt.xlabel('$x_0$')\n",
    "plt.ylabel('$x_1$')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Intro3_Working_with_Data_solution.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
